,id,title,categories,abstract,doi,created,updated,authors
0,1202.0909,a berry-esseen bound for the uniform multinomial occupancy model,math.pr math.st stat.th,"the inductive size bias coupling technique and stein's method yield a berry-esseen theorem for the number of urns having occupancy $d \ge 2$ when $n$ balls are uniformly distributed over $m$ urns. in particular, there exists a constant $c$ depending only on $d$ such that $$ \sup_{z \in \mathbb{r}}|p(w_{n,m} \le z) -p(z \le z)| \le c \left( \frac{1+(\frac{n}{m})^3}{\sigma_{n,m}} \right) \quad \mbox{for all $n \ge d$ and $m \ge 2$,} $$ where $w_{n,m}$ and $\sigma_{n,m}^2$ are the standardized count and variance, respectively, of the number of urns with $d$ balls, and $z$ is a standard normal random variable. asymptotically, the bound is optimal up to constants if $n$ and $m$ tend to infinity together in a way such that $n/m$ stays bounded.",10.1214/ejp.v18-1983,2012-02-04,2019-03-30,"['jay bartroff', 'larry goldstein']"
1,1207.0437,"ordinal and cardinal dendrograms depicting migration-based   regionalization of 3,000 + u. s. counties",physics.soc-ph cs.si stat.ap,"we have obtained a ""hierarchical regionalization"" of 3,107 county-level units of the united states based upon census-recorded 1995-2000 intercounty migration flows. the methodology employed was the two-stage (double-standardization and strong component [directed graph] hierarchical clustering) algorithm described in the 2009 pnas (106 [26], e66) letter (arxiv:0904.4863). various features (e. g., cosmopolitan vs. provincial aspects, and indices of isolation) of the regionalization have been previously discussed in arxiv:0907.2393, arxiv:0903.3623 and arxiv:0809.2768. however, due to the lengthy (38-page) nature of the associated dendrogram, the detailed tree structure itself was not readily available for inspection. here, we do present this (county-searchable) dendrogram--and invite readers to explore it, based on their particular interests/locations. an ordinal scale--rather than the originally-derived cardinal scale of the doubly-standardized values--in which groupings/features were more immediately apparent, was originally presented. now, we append the cardinal-scale dendrogram.",,2012-07-02,2019-04-05,['paul b. slater']
2,1301.2007,spectral clustering based on local pca,stat.ml,"we propose a spectral clustering method based on local principal components analysis (pca). after performing local pca in selected neighborhoods, the algorithm builds a nearest neighbor graph weighted according to a discrepancy between the principal subspaces in the neighborhoods, and then applies spectral clustering. as opposed to standard spectral methods based solely on pairwise distances between points, our algorithm is able to resolve intersections. we establish theoretical guarantees for simpler variants within a prototypical mathematical framework for multi-manifold clustering, and evaluate our algorithm on various simulated data sets.",,2013-01-09,,"['ery arias-castro', 'gilad lerman', 'teng zhang']"
3,1303.6169,co-occurring hiv risk behaviors among males entering jail,stat.ap,"this paper examines the pattern of hiv risk behaviors among male jail detainees. from multivariate analyses of baseline data from an hiv intervention study of ours, we find that: (1) cocaine use, heroin use and multiple sexual partners; and (2) heavy drinking and marijuana are often co-occurring among this population. from pairwise analyses, we find that (1) heroin and idu (2) unprotected sexes with main, with non-main, and in last sexual encounter are mostly co-occurring. idu is found to be associated with middle ages (30-40) and multiple prior incarcerations, and multiple sex partners associated young males with age <30, african american race, and low education. our findings suggest that efficient interventions to reduce hiv infection in this high-risk population may have to target on these behaviors simultaneously and be demographically adapted.",,2013-03-25,2019-04-04,"['tao liu', 'lauri bazerman', 'megan pinkston', 'amy nunn', 'aadia rana', 'curt g. beckwith']"
4,1305.7007,estimation of false discovery proportion with unknown dependence,stat.me,"large-scale multiple testing with highly correlated test statistics arises frequently in many scientific research. incorporating correlation information in estimating false discovery proportion has attracted increasing attention in recent years. when the covariance matrix of test statistics is known, fan, han & gu (2012) provided a consistent estimate of false discovery proportion (fdp) under arbitrary dependence structure. however, the covariance matrix is often unknown in many applications and such dependence information has to be estimated before estimating fdp (efron, 2010). the estimation accuracy can greatly affect the convergence result of fdp or even violate its consistency. in the current paper, we provide methodological modification and theoretical investigations for estimation of fdp with unknown covariance. first we develop requirements for estimates of eigenvalues and eigenvectors such that we can obtain a consistent estimate of fdp. secondly we give conditions on the dependence structures such that the estimate of fdp is consistent. such dependence structures include sparse covariance matrices, which have been popularly considered in the contemporary random matrix theory. when data are sampled from an approximate factor model, which encompasses most practical situations, we provide a consistent estimate of fdp via exploiting this specific dependence structure. the results are further demonstrated by simulation studies and some real data applications.",,2013-05-30,2019-03-26,"['jianqing fan', 'xu han']"
5,1401.2139,distinguishing noise from chaos: objective versus subjective criteria   using horizontal visibility graph,stat.ml cs.it math.it nlin.cd,"a recently proposed methodology called the horizontal visibility graph (hvg) [luque {\it et al.}, phys. rev. e., 80, 046103 (2009)] that constitutes a geometrical simplification of the well known visibility graph algorithm [lacasa {\it et al.\/}, proc. natl. sci. u.s.a. 105, 4972 (2008)], has been used to study the distinction between deterministic and stochastic components in time series [l. lacasa and r. toral, phys. rev. e., 82, 036120 (2010)]. specifically, the authors propose that the node degree distribution of these processes follows an exponential functional of the form $p(\kappa)\sim \exp(-\lambda~\kappa)$, in which $\kappa$ is the node degree and $\lambda$ is a positive parameter able to distinguish between deterministic (chaotic) and stochastic (uncorrelated and correlated) dynamics. in this work, we investigate the characteristics of the node degree distributions constructed by using hvg, for time series corresponding to $28$ chaotic maps and $3$ different stochastic processes. we thoroughly study the methodology proposed by lacasa and toral finding several cases for which their hypothesis is not valid. we propose a methodology that uses the hvg together with information theory quantifiers. an extensive and careful analysis of the node degree distributions obtained by applying hvg allow us to conclude that the fisher-shannon information plane is a remarkable tool able to graphically represent the different nature, deterministic or stochastic, of the systems under study.",10.1371/journal.pone.0108004,2014-01-09,,"['martín gómez ravetti', 'laura c. carpi', 'bruna amin gonçalves', 'alejandro c. frery', 'osvaldo a. rosso']"
6,1407.5232,on coverage and local radial rates of ddm-credible sets,math.st stat.th,"for a general statistical model, we introduce the notion of data dependent measure (ddm) on the model parameter. typical examples of ddm are the posterior distributions. like for posteriors, the quality of a ddm is characterized by the contraction rate which we allow to be local, i.e., depending on the parameter. we construct confidence sets as ddm-credible sets and address the issue of optimality of such sets, via a trade-off between its ""size"" (the local radial rate) and its coverage probability. in the mildly ill-posed inverse signal-in-white-noise model, we construct a ddm as empirical bayes posterior with respect to a certain prior, and define its (default) credible set. then we introduce 'excessive bias restriction' (ebr), more general than 'self-similarity' and 'polished tail condition' recently studied in the literature. under ebr, we establish the confidence optimality of our credible set with some local (oracle) radial rate. we also derive the oracle estimation inequality and the oracle ddm-contraction rate, non-asymptotically and uniformly in $\ell_2$. the obtained local results are more powerful than global: adaptive minimax results for a number of smoothness scales follow as consequence, in particular, the ones considered by szabo, van der vaart and van zanten (2015).",,2014-07-19,2015-10-20,['eduard belitser']
7,1410.6698,testing the maximal rank of the volatility process for continuous   diffusions observed with noise,math.st stat.th,"in this paper, we present a test for the maximal rank of the volatility process in continuous diffusion models observed with noise. such models are typically applied in mathematical finance, where latent price processes are corrupted by microstructure noise at ultra high frequencies. using high frequency observations we construct a test statistic for the maximal rank of the time varying stochastic volatility process. our methodology is based upon a combination of a matrix perturbation approach and pre-averaging. we will show the asymptotic mixed normality of the test statistic and obtain a consistent testing procedure.",10.3150/16-bej836,2014-10-24,,"['tobias fissler', 'mark podolskij']"
8,1503.01161,the bayesian case model: a generative approach for case-based reasoning   and prototype classification,stat.ml cs.lg,"we present the bayesian case model (bcm), a general framework for bayesian case-based reasoning (cbr) and prototype classification and clustering. bcm brings the intuitive power of cbr to a bayesian generative framework. the bcm learns prototypes, the ""quintessential"" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. simultaneously, bcm pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. the prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by bcm, compared to those given by prior art.",,2015-03-03,,"['been kim', 'cynthia rudin', 'julie shah']"
9,1505.03481,relations between adjacency and modularity graph partitioning,stat.ml,"in this paper the exact linear relation between the leading eigenvector of the unnormalized modularity matrix and the eigenvectors of the adjacency matrix is developed. based on this analysis a method to approximate the leading eigenvector of the modularity matrix is given, and the relative error of the approximation is derived. a complete proof of the equivalence between normalized modularity clustering and normalized adjacency clustering is also given. some applications and experiments are given to illustrate and corroborate the points that are made in the theoretical development.",,2015-05-09,2019-03-30,"['hansi jiang', 'carl meyer']"
10,1505.05816,an empirical approach to demographic inference with genomic data,q-bio.pe math.pr stat.ap,"inference with population genetic data usually treats the population pedigree as a nuisance parameter, the unobserved product of a past history of random mating. however, the history of genetic relationships in a given population is a fixed, unobserved object, and so an alternative approach is to treat this network of relationships as a complex object we wish to learn about, by observing how genomes have been noisily passed down through it. this paper explores this point of view, showing how to translate questions about population genetic data into calculations with a poisson process of mutations on all ancestral genomes. this method is applied to give a robust interpretation to the $f_4$ statistic used to identify admixture, and to design a new statistic that measures covariances in mean times to most recent common ancestor between two pairs of sequences. the method more generally interprets population genetic statistics in terms of sums of specific functions over ancestral genomes, thereby providing concrete, broadly interpretable interpretations for these statistics. this provides a method for describing demographic history without simplified demographic models. more generally, it brings into focus the population pedigree, which is averaged over in model-based demographic inference.",,2015-05-21,2019-04-01,['peter l. ralph']
11,1506.08536,a simple yet efficient algorithm for multiple kernel learning under   elastic-net constraints,stat.ml cs.lg,this papers introduces an algorithm for the solution of multiple kernel learning (mkl) problems with elastic-net constraints on the kernel weights. the algorithm compares very favourably in terms of time and space complexity to existing approaches and can be implemented with simple code that does not rely on external libraries (except a conventional svm solver).,,2015-06-29,2019-04-05,['luca citi']
12,1507.07024,a multiscale strategy for bayesian inference using transport maps,stat.co math.pr stat.ap,"in many inverse problems, model parameters cannot be precisely determined from observational data. bayesian inference provides a mechanism for capturing the resulting parameter uncertainty, but typically at a high computational cost. this work introduces a multiscale decomposition that exploits conditional independence across scales, when present in certain classes of inverse problems, to decouple bayesian inference into two stages: (1) a computationally tractable coarse-scale inference problem; and (2) a mapping of the low-dimensional coarse-scale posterior distribution into the original high-dimensional parameter space. this decomposition relies on a characterization of the non-gaussian joint distribution of coarse- and fine-scale quantities via optimal transport maps. we demonstrate our approach on a sequence of inverse problems arising in subsurface flow, using the multiscale finite element method to discretize the steady state pressure equation. we compare the multiscale strategy with full-dimensional markov chain monte carlo on a problem of moderate dimension (100 parameters) and then use it to infer a conductivity field described by over 10,000 parameters.",10.1137/15m1032478,2015-07-24,2016-02-17,"['matthew parno', 'tarek moselhy', 'youssef marzouk']"
13,1509.09286,pinsker bound under measurement budget constrain: optimal allocation,math.st stat.th,"in the classical many normal means with different variances, we consider the situation when the observer is allowed to allocate the available measurement budget over the coordinates of the parameter of interest. the benchmark is the minimax linear risk over a set. we solve the problem of optimal allocation of observations under the measurement budget constrain for two types of sets, ellipsoids and hyperrectangles. by elaborating on the two examples of sobolev ellipsoids and hyperectangles, we demonstrate how re-allocating the measurements in the (sub-)optimal way improves on the standard uniform allocation. in particular, we improve the famous pinsker (1980) bound.",,2015-09-30,,['eduard belitser']
14,1605.08003,tight complexity bounds for optimizing composite objectives,math.oc cs.lg stat.ml,"we provide tight upper and lower bounds on the complexity of minimizing the average of $m$ convex functions using gradient and prox oracles of the component functions. we show a significant gap between the complexity of deterministic vs randomized optimization. for smooth functions, we show that accelerated gradient descent (agd) and an accelerated variant of svrg are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. for non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.",,2016-05-25,2019-04-04,"['blake woodworth', 'nathan srebro']"
15,1607.02633,bayesian inference for stochastic differential equation mixed effects   models of a tumor xenography study,stat.ap stat.co stat.me,"we consider bayesian inference for stochastic differential equation mixed effects models (sdemems) exemplifying tumor response to treatment and regrowth in mice. we produce an extensive study on how a sdemem can be fitted using both exact inference based on pseudo-marginal mcmc and approximate inference via bayesian synthetic likelihoods (bsl). we investigate a two-compartments sdemem, these corresponding to the fractions of tumor cells killed by and survived to a treatment, respectively. case study data considers a tumor xenography study with two treatment groups and one control, each containing 5-8 mice. results from the case study and from simulations indicate that the sdemem is able to reproduce the observed growth patterns and that bsl is a robust tool for inference in sdemems. finally, we compare the fit of the sdemem to a similar ordinary differential equation model. due to small sample sizes, strong prior information is needed to identify all model parameters in the sdemem and it cannot be determined which of the two models is the better in terms of predicting tumor growth curves. in a simulation study we find that with a sample of 17 mice per group bsl is able to identify all model parameters and distinguish treatment groups.",10.1111/rssc.12347,2016-07-09,2019-02-17,"['umberto picchini', 'julie lyng forman']"
16,1608.01532,fixed-effect regressions on network data,stat.me econ.em,"this paper considers inference on fixed effects in a linear regression model estimated from network data. an important special case of our setup is the two-way regression model. this is a workhorse technique in the analysis of matched data sets, such as employer-employee or student-teacher panel data. we formalize how the structure of the network affects the accuracy with which the fixed effects can be estimated. this allows us to derive sufficient conditions on the network for consistent estimation and asymptotically-valid inference to be possible. estimation of moments is also considered. we allow for general networks and our setup covers both the dense and sparse case. we provide numerical results for the estimation of teacher value-added models and regressions with occupational dummies.",,2016-08-04,2019-04-01,"['koen jochmans', 'martin weidner']"
17,1609.01057,inference for conditioned galton-watson trees from their harris path,math.st stat.th,"tree-structured data naturally appear in various fields, particularly in biology where plants and blood vessels may be described by trees, but also in computer science because xml documents form a tree structure. this paper is devoted to the estimation of the relative scale parameter of conditioned galton-watson trees. new estimators are introduced and their consistency is stated. a comparison is made with an existing approach of the literature. a simulation study shows the good behavior of our procedure on finite-sample sizes and from missing or noisy data. an application to the analysis of revisions of wikipedia articles is also considered through real data.",,2016-09-05,2019-04-08,"['romain azaïs', 'alexandre genadot', 'benoît henry']"
18,1609.03779,non-asymptotic upper bounds for the reconstruction error of pca,math.st stat.th,"we analyse the reconstruction error of principal component analysis (pca) and prove non-asymptotic upper bounds for the corresponding excess risk. these bounds unify and improve existing upper bounds from the literature. in particular, they give oracle inequalities under mild eigenvalue conditions. the bounds reveal that the excess risk differs significantly from usually considered subspace distances based on canonical angles. our approach relies on the analysis of empirical spectral projectors combined with concentration inequalities for weighted empirical covariance operators and empirical eigenvalues.",,2016-09-13,2019-03-29,"['markus reiß', 'martin wahl']"
19,1610.01234,"ensemble validation: selectivity has a price, but variety is free",stat.ml cs.lg,"suppose some classifiers are selected from a set of hypothesis classifiers to form an equally-weighted ensemble that selects a member classifier at random for each input example. then the ensemble has an error bound consisting of the average error bound for the member classifiers, a term for selectivity that varies from zero (if all hypothesis classifiers are selected) to a standard uniform error bound (if only a single classifier is selected), and small constants. there is no penalty for using a richer hypothesis set if the same fraction of the hypothesis classifiers are selected for the ensemble.",,2016-10-04,2019-03-28,"['eric bax', 'farshad kooti']"
20,1611.00962,multitask protein function prediction through task dissimilarity,stat.ml cs.lg q-bio.qm,"automated protein function prediction is a challenging problem with distinctive features, such as the hierarchical organization of protein functions and the scarcity of annotated proteins for most biological functions. we propose a multitask learning algorithm addressing both issues. unlike standard multitask algorithms, which use task (protein functions) similarity information as a bias to speed up learning, we show that dissimilarity information enforces separation of rare class labels from frequent class labels, and for this reason is better suited for solving unbalanced protein function prediction problems. we support our claim by showing that a multitask extension of the label propagation algorithm empirically works best when the task relatedness information is represented using a dissimilarity matrix as opposed to a similarity matrix. moreover, the experimental comparison carried out on three model organism shows that our method has a more stable performance in both ""protein-centric"" and ""function-centric"" evaluation settings.",10.1109/tcbb.2017.2684127,2016-11-03,,"['marco frasca', 'nicolò cesa bianchi']"
21,1612.06618,an asymptotic expansion for the normalizing constant of the   conway-maxwell-poisson distribution,math.st stat.th,"the conway-maxwell-poisson distribution is a two-parameter generalisation of the poisson distribution that can be used to model data that is under- or over-dispersed relative to the poisson distribution. the normalizing constant $z(\lambda,\nu)$ is given by an infinite series that in general has no closed form, although several papers have derived approximations for this sum. in this work, we start by using probabilistic argument to obtain the leading term in the asymptotic expansion of $z(\lambda,\nu)$ in the limit $\lambda\rightarrow\infty$ that holds for all $\nu>0$. we then use an integral representation to obtain the entire asymptotic series and give explicit formulas for the first eight coefficients. we apply this asymptotic series to obtain approximations for the mean, variance, cumulants, skweness, excess kurtosis and raw moments of cmp random variables. numerical results confirm that these correction terms yield more accurate estimates than those obtained using just the leading order term.",,2016-12-20,2017-10-16,"['robert e. gaunt', 'satish iyengar', 'adri b. olde daalhuis', 'burcin simsek']"
22,1701.03077,a general and adaptive robust loss function,cs.cv cs.lg stat.ml,"we present a generalization of the cauchy/lorentzian, geman-mcclure, welsch/leclerc, generalized charbonnier, charbonnier/pseudo-huber/l1-l2, and l2 loss functions. by introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and cauchy distributions as special cases. this probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.",,2017-01-11,2019-04-04,['jonathan t. barron']
23,1702.06819,distributed representations of signed networks,stat.ml cs.lg cs.si,"recent successes in word embedding and document embedding have motivated researchers to explore similar representations for networks and to use such representations for tasks such as edge prediction, node label prediction, and community detection. such network embedding methods are largely focused on finding distributed representations for unsigned networks and are unable to discover embeddings that respect polarities inherent in edges. we propose signet, a fast scalable embedding method suitable for signed networks. our proposed objective function aims to carefully model the social structure implicit in signed networks by reinforcing the principles of social balance theory. our method builds upon the traditional word2vec family of embedding approaches and adds a new targeted node sampling strategy to maintain structural balance in higher-order neighborhoods. we demonstrate the superiority of signet over state-of-the-art methods proposed for both signed and unsigned networks on several real world datasets from different domains. in particular, signet offers an approach to generate a richer vocabulary of features of signed networks to support representation and reasoning.",10.1007/978-3-319-93037-4_13,2017-02-22,2019-04-07,"['mohammad raihanul islam', 'b. aditya prakash', 'naren ramakrishnan']"
24,1704.00850,estimating the spectral gap of a trace-class markov operator,math.st stat.me stat.th,"the utility of a markov chain monte carlo algorithm is, in large part, determined by the size of the spectral gap of the corresponding markov operator. however, calculating (and even approximating) the spectral gaps of practical monte carlo markov chains in statistics has proven to be an extremely difficult and often insurmountable task, especially when these chains move on continuous state spaces. in this paper, a method for accurate estimation of the spectral gap is developed for general state space markov chains whose operators are non-negative and trace-class. the method is based on the fact that the second largest eigenvalue (and hence the spectral gap) of such operators can be bounded above and below by simple functions of the power sums of the eigenvalues. these power sums often have nice integral representations. a classical monte carlo method is proposed to estimate these integrals, and a simple sufficient condition for finite variance is provided. this leads to asymptotically valid confidence intervals for the second largest eigenvalue (and the spectral gap) of the markov operator. in contrast with previously existing techniques, our method is not based on a near-stationary version of the markov chain, which, paradoxically, cannot be obtained in a principled manner without bounds on the spectral gap. on the other hand, it can be quite expensive from a computational standpoint. the efficiency of the method is studied both theoretically and empirically.",,2017-04-03,2019-04-04,"['qian qin', 'james p. hobert', 'kshitij khare']"
25,1704.03995,optimal experimental design that minimizes the width of simultaneous   confidence bands,math.st stat.me stat.th,"we propose an optimal experimental design for a curvilinear regression model that minimizes the band-width of simultaneous confidence bands. simultaneous confidence bands for curvilinear regression are constructed by evaluating the volume of a tube about a curve that is defined as a trajectory of a regression basis vector (naiman, 1986). the proposed criterion is constructed based on the volume of a tube, and the corresponding optimal design that minimizes the volume of tube is referred to as the tube-volume optimal (tv-optimal) design. for fourier and weighted polynomial regressions, the problem is formalized as one of minimization over the cone of hankel positive definite matrices, and the criterion to minimize is expressed as an elliptic integral. we show that the m\""obius group keeps our problem invariant, and hence, minimization can be conducted over cross-sections of orbits. we demonstrate that for the weighted polynomial regression and the fourier regression with three bases, the tube-volume optimal design forms an orbit of the m\""obius group containing d-optimal designs as representative elements.",,2017-04-13,2019-03-30,"['satoshi kuriki', 'henry p. wynn']"
26,1705.03799,model-based computed tomography image estimation: partitioning approach,stat.ap,"there is a growing interest to get a fully mr based radiotherapy. the most important development needed is to obtain improved bone tissue estimation. the existing model-based methods perform poorly on bone tissues. this paper was aimed at obtaining improved bone tissue estimation. skew gaussian mixture model and gaussian mixture model were proposed to investigate ct image estimation from mr images by partitioning the data into two major tissue types. the performance of the proposed models was evaluated using leave-one-out cross-validation method on real data. in comparison with the existing model-based approaches, the model-based partitioning approach outperformed in bone tissue estimation, especially in dense bone tissue estimation.",,2017-05-10,2019-03-30,"['fekadu l. bayisa', 'jun yu']"
27,1705.04565,estimating the reach of a manifold,math.st math.dg stat.th,"various problems in manifold estimation make use of a quantity called the reach, denoted by $\tau\_m$, which is a measure of the regularity of the manifold. this paper is the first investigation into the problem of how to estimate the reach. first, we study the geometry of the reach through an approximation perspective. we derive new geometric results on the reach for submanifolds without boundary. an estimator $\hat{\tau}$ of $\tau\_{m}$ is proposed in a framework where tangent spaces are known, and bounds assessing its efficiency are derived. in the case of i.i.d. random point cloud $\mathbb{x}\_{n}$, $\hat{\tau}(\mathbb{x}\_{n})$ is showed to achieve uniform expected loss bounds over a $\mathcal{c}^3$-like model. finally, we obtain upper and lower bounds on the minimax rate for estimating the reach.",,2017-05-12,2019-04-08,"['eddie aamari', 'jisu kim', 'frédéric chazal', 'bertrand michel', 'alessandro rinaldo', 'larry wasserman']"
28,1705.06567,controlling the time discretization bias for the supremum of brownian   motion,math.pr stat.co,"we consider the bias arising from time discretization when estimating the threshold crossing probability $w(b) := \mathbb{p}(\sup_{t\in[0,1]} b_t > b)$, with $(b_t)_{t\in[0,1]}$ a standard brownian motion. we prove that if the discretization is equidistant, then to reach a given target value of the relative bias, the number of grid points has to grow quadratically in $b$, as $b$ grows. when considering non-equidistant discretizations (with threshold-dependent grid points), we can substantially improve on this: we show that for such grids the required number of grid points is independent of $b$, and in addition we point out how they can be used to construct a strongly efficient algorithm for the estimation of $w(b)$. finally, we show how to apply the resulting algorithm for a broad class of stochastic processes; it is empirically shown that the threshold-dependent grid significantly outperforms its equidistant counterpart.",10.1145/3177775,2017-05-18,2017-09-13,"['krzysztof bisewski', 'daan crommelin', 'michel mandjes']"
29,1706.03665,statistical properties of sketching algorithms,stat.me stat.co,"sketching is a probabilistic data compression technique that has been largely developed in the computer science community. numerical operations on big datasets can be intolerably slow; sketching algorithms address this issue by generating a smaller surrogate dataset. typically, inference proceeds on the compressed dataset. sketching algorithms generally use random projections to compress the original dataset and this stochastic generation process makes them amenable to statistical analysis. we argue that the sketched data can be modelled as a random sample, thus placing this family of data compression methods firmly within an inferential framework. in particular, we focus on the gaussian, hadamard and clarkson-woodruff sketches, and their use in single pass sketching algorithms for linear regression with huge $n$. we explore the statistical properties of sketched regression algorithms and derive new distributional results for a large class of sketched estimators. a key result is a conditional central limit theorem for data oblivious sketches. an important finding is that the best choice of sketching algorithm in terms of mean square error is related to the signal to noise ratio in the source dataset. finally, we demonstrate the theory and the limits of its applicability on two real datasets.",,2017-06-12,2019-04-02,"['daniel ahfock', 'william j. astle', 'sylvia richardson']"
30,1707.02294,a case study of empirical bayes in user-movie recommendation system,stat.ml stat.co,in this article we provide a formulation of empirical bayes described by atchade (2011) to tune the hyperparameters of priors used in bayesian set up of collaborative filter. we implement the same in movielens small dataset. we see that it can be used to get a good initial choice for the parameters. it can also be used to guess an initial choice for hyper-parameters in grid search procedure even for the datasets where mcmc oscillates around the true value or takes long time to converge.,10.1080/23737484.2017.1392266,2017-07-07,,"['arabin kumar dey', 'raghav somani', 'sreangsu acharyya']"
31,1707.03815,deep gaussian embedding of graphs: unsupervised inductive learning via   ranking,stat.ml cs.lg cs.si,"methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. we propose graph2gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a gaussian distribution, allowing us to capture uncertainty about the representation. furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. by leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. to learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph.",,2017-07-12,2018-02-27,"['aleksandar bojchevski', 'stephan günnemann']"
32,1707.07657,engineering fast multilevel support vector machines,cs.lg cs.ds stat.co stat.ml,"the computational complexity of solving nonlinear support vector machine (svm) is prohibitive on large-scale data. in particular, this issue becomes very sensitive when the data represents additional difficulties such as highly imbalanced class sizes. typically, nonlinear kernels produce significantly higher classification quality to linear kernels but introduce extra kernel and model parameters which requires computationally expensive fitting. this increases the quality but also reduces the performance dramatically. we introduce a generalized fast multilevel framework for regular and weighted svm and discuss several versions of its algorithmic components that lead to a good trade-off between quality and time. our framework is implemented using petsc which allows an easy integration with scientific computing tasks. the experimental results demonstrate significant speed up compared to the state-of-the-art nonlinear svm libraries.   reproducibility: our source code, documentation and parameters are available at https:// github.com/esadr/mlsvm.",,2017-07-24,2019-04-05,"['e. sadrfaridpour', 't. razzaghi', 'i. safro']"
33,1707.09714,a geometric variational approach to bayesian inference,stat.me,"we propose a novel riemannian geometric framework for variational inference in bayesian models based on the nonparametric fisher-rao metric on the manifold of probability density functions. under the square-root density representation, the manifold can be identified with the positive orthant of the unit hypersphere in l2, and the fisher-rao metric reduces to the standard l2 metric. exploiting such a riemannian structure, we formulate the task of approximating the posterior distribution as a variational problem on the hypersphere based on the alpha-divergence. this provides a tighter lower bound on the marginal distribution when compared to, and a corresponding upper bound unavailable with, approaches based on the kullback-leibler divergence. we propose a novel gradient-based algorithm for the variational problem based on frechet derivative operators motivated by the geometry of the hilbert sphere, and examine its properties. through simulations and real-data applications, we demonstrate the utility of the proposed geometric framework and algorithm on several bayesian models.",,2017-07-30,2019-03-27,"['abhijoy saha', 'karthik bharath', 'sebastian kurtek']"
34,1708.00598,controllable generative adversarial network,cs.lg cs.cv stat.ml,"recently introduced generative adversarial network (gan) has been shown numerous promising results to generate realistic samples. the essential task of gan is to control the features of samples generated from a random distribution. while the current gan structures, such as conditional gan, successfully generate samples with desired major features, they often fail to produce detailed features that bring specific differences among samples. to overcome this limitation, here we propose a controllable gan (controlgan) structure. by separating a feature classifier from a discriminator, the generator of controlgan is designed to learn generating synthetic samples with the specific detailed features. evaluated with multiple image datasets, controlgan shows a power to generate improved samples with well-controlled features. furthermore, we demonstrate that controlgan can generate intermediate features and opposite features for interpolated and extrapolated input labels that are not used in the training process. it implies that controlgan can significantly contribute to the variety of generated samples.",,2017-08-02,2019-03-30,"['minhyeok lee', 'junhee seok']"
35,1708.01799,efficient contextual bandits in non-stationary worlds,cs.lg stat.ml,"most contextual bandit algorithms minimize regret against the best fixed policy, a questionable benchmark for non-stationary environments that are ubiquitous in applications. in this work, we develop several efficient contextual bandit algorithms for non-stationary environments by equipping existing methods for i.i.d. problems with sophisticated statistical tests so as to dynamically adapt to a change in distribution.   we analyze various standard notions of regret suited to non-stationary environments for these algorithms, including interval regret, switching regret, and dynamic regret. when competing with the best policy at each time, one of our algorithms achieves regret $\mathcal{o}(\sqrt{st})$ if there are $t$ rounds with $s$ stationary periods, or more generally $\mathcal{o}(\delta^{1/3}t^{2/3})$ where $\delta$ is some non-stationarity measure. these results almost match the optimal guarantees achieved by an inefficient baseline that is a variant of the classic exp4 algorithm. the dynamic regret result is also the first one for efficient and fully adversarial contextual bandit.   furthermore, while the results above require tuning a parameter based on the unknown quantity $s$ or $\delta$, we also develop a parameter free algorithm achieving regret $\min\{s^{1/4}t^{3/4}, \delta^{1/5}t^{4/5}\}$. this improves and generalizes the best existing result $\delta^{0.18}t^{0.82}$ by karnin and anava (2016) which only holds for the two-armed bandit problem.",,2017-08-05,2019-04-03,"['haipeng luo', 'chen-yu wei', 'alekh agarwal', 'john langford']"
36,1708.01945,a bootstrap method for error estimation in randomized matrix   multiplication,stat.ml cs.lg cs.na,"in recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. in this way, the dimension reduction step encodes a tradeoff between cost and accuracy. however, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know (1) how accurate a given solution is, or (2) how much computation is needed to achieve a given level of accuracy. in the current paper, we study randomized matrix multiplication (sketching) as a prototype setting for addressing these general problems. as a solution, we develop a bootstrap method for \emph{directly estimating} the accuracy as a function of the reduced dimension (as opposed to deriving worst-case bounds on the accuracy in terms of the reduced dimension). from a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an ""extrapolation"" technique. in addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method.",,2017-08-06,2019-04-03,"['miles e. lopes', 'shusen wang', 'michael w. mahoney']"
37,1708.03579,self-exciting point processes with spatial covariates: modeling the   dynamics of crime,stat.ap,"crime has both varying patterns in space, related to features of the environment, economy, and policing, and patterns in time arising from criminal behavior, such as retaliation. serious crimes may also be presaged by minor crimes of disorder. we demonstrate that these spatial and temporal patterns are generally confounded, requiring analyses to take both into account, and propose a spatio-temporal self-exciting point process model which incorporates spatial features, near-repeat and retaliation effects, and triggering. we develop inference methods and diagnostic tools, such as residual maps, for this model, and through extensive simulation and crime data obtained from pittsburgh, pennsylvania, demonstrate its properties and usefulness.",10.1111/rssc.12277,2017-08-11,2019-04-07,"['alex reinhart', 'joel greenhouse']"
38,1708.05963,neural networks compression for language modeling,stat.ml cs.cl cs.lg cs.ne,"in this paper, we consider several compression techniques for the language modeling problem based on recurrent neural networks (rnns). it is known that conventional rnns, e.g, lstm-based networks in language modeling, are characterized with either high space complexity or substantial inference time. this problem is especially crucial for mobile applications, in which the constant interaction with the remote server is inappropriate. by using the penn treebank (ptb) dataset we compare pruning, quantization, low-rank factorization, tensor train decomposition for lstm networks in terms of model size and suitability for fast inference.",10.1007/978-3-319-69900-4_44,2017-08-20,,"['artem m. grachev', 'dmitry i. ignatov', 'andrey v. savchenko']"
39,1708.08911,simultaneous variable and covariance selection with the multivariate   spike-and-slab lasso,stat.me,"we propose a bayesian procedure for simultaneous variable and covariance selection using continuous spike-and-slab priors in multivariate linear regression models where q possibly correlated responses are regressed onto p predictors. rather than relying on a stochastic search through the high-dimensional model space, we develop an ecm algorithm similar to the emvs procedure of rockova & george (2014) targeting modal estimates of the matrix of regression coefficients and residual precision matrix. varying the scale of the continuous spike densities facilitates dynamic posterior exploration and allows us to filter out negligible regression coefficients and partial covariances gradually. our method is seen to substantially outperform regularization competitors on simulated data. we demonstrate our method with a re-examination of data from a recent observational study of the effect of playing high school football on several later-life cognition, psychological, and socio-economic outcomes.",10.1080/10618600.2019.1593179,2017-08-29,2018-07-24,"['sameer k. deshpande', 'veronika rockova', 'edward i. george']"
40,1710.10346,inference for stochastic kinetic models from multiple data sources for   joint estimation of infection dynamics from aggregate reports and virological   data,stat.ap,"influenza and respiratory syncytial virus (rsv) are the leading etiological agents of seasonal acute respiratory infections (ari) around the world. medical doctors typically base the diagnosis of ari on patients' symptoms alone, and do not always conduct virological tests necessary to identify individual viruses, which limits the ability to study the interaction between multiple pathogens and make public health recommendations. we consider a stochastic kinetic model (skm) for two interacting ari pathogens circulating in a large population and an empirically motivated background process for infections with other pathogens causing similar symptoms. an extended marginal sampling approach based on the linear noise approximation to the skm integrates multiple data sources and additional model components. we infer the parameters defining the pathogens' dynamics and interaction within a bayesian hierarchical model and explore the posterior trajectories of infections for each illness based on aggregate infection reports from six epidemic seasons collected by the state health department, and a subset of virological tests from a sentinel program at a general hospital in san luis potos\'i, m\'exico. we interpret the results based on real and simulated data and make recommendations for future data collection strategies. supplementary materials and software are provided online.",,2017-10-27,2019-03-28,"['yury e. garcía', 'oksana a. chkrebtii', 'marcos a. capistrán', 'daniel e. noyola']"
41,1711.01742,model-free nonconvex matrix completion: local minima analysis and   applications in memory-efficient kernel pca,math.oc cs.lg stat.ml,"this work studies low-rank approximation of a positive semidefinite matrix from partial entries via nonconvex optimization. we characterized how well local-minimum based low-rank factorization approximates a fixed positive semidefinite matrix without any assumptions on the rank-matching, the condition number or eigenspace incoherence parameter. furthermore, under certain assumptions on rank-matching and well-boundedness of condition numbers and eigenspace incoherence parameters, a corollary of our main theorem improves the state-of-the-art sampling rate results for nonconvex matrix completion with no spurious local minima in ge et al. [2016, 2017]. in addition, we investigated when the proposed nonconvex optimization results in accurate low-rank approximations even in presence of large condition numbers, large incoherence parameters, or rank mismatching. we also propose to apply the nonconvex optimization to memory-efficient kernel pca. compared to the well-known nystr\""{o}m methods, numerical experiments indicate that the proposed nonconvex optimization approach yields more stable results in both low-rank approximation and clustering.",,2017-11-06,2019-04-04,"['ji chen', 'xiaodong li']"
42,1711.03838,modeling asymmetric relationships from symmetric networks,stat.me,"many relationships requiring mutual agreement between pairs of actors produce observable networks that are symmetric and undirected. nevertheless the unobserved, asymmetric network is often of primary scientific interest. we propose a method that probabilistically reconstructs the unobserved, asymmetric network from the observed, symmetric graph using a regression-based framework that allows for inference on predictors of actors' decisions. we apply this model to the bilateral investment treaty network. our approach extracts politically relevant information about the network structure that is inaccessible to alternative approaches and has superior predictive performance.",10.1017/pan.2018.41,2017-11-10,2018-06-26,"['arturas rozenas', 'shahryar minhas', 'john ahlquist']"
43,1711.06705,principal boundary on riemannian manifolds,stat.ml cs.lg stat.me,"we consider the classification problem and focus on nonlinear methods for classification on manifolds. for multivariate datasets lying on an embedded nonlinear riemannian manifold within the higher-dimensional ambient space, we aim to acquire a classification boundary for the classes with labels, using the intrinsic metric on the manifolds. motivated by finding an optimal boundary between the two classes, we invent a novel approach -- the principal boundary. from the perspective of classification, the principal boundary is defined as an optimal curve that moves in between the principal flows traced out from two classes of data, and at any point on the boundary, it maximizes the margin between the two classes. we estimate the boundary in quality with its direction, supervised by the two principal flows. we show that the principal boundary yields the usual decision boundary found by the support vector machine in the sense that locally, the two boundaries coincide. some optimality and convergence properties of the random principal boundary and its population counterpart are also shown. we illustrate how to find, use and interpret the principal boundary with an application in real data.",,2017-10-21,2019-03-30,"['zhigang yao', 'zhenyue zhang']"
44,1711.07230,optimism-based adaptive regulation of linear-quadratic systems,cs.sy math.oc stat.ap stat.ml,"the main challenge for adaptive regulation of linear-quadratic systems is the trade-off between identification and control. an adaptive policy needs to address both the estimation of unknown dynamics parameters (exploration), as well as the regulation of the underlying system (exploitation). to this end, optimism-based methods which bias the identification in favor of optimistic approximations of the true parameter are employed in the literature. a number of asymptotic results have been established, but their finite time counterparts are few, with important restrictions.   this study establishes results for the worst-case regret of optimism-based adaptive policies. the presented high probability upper bounds are optimal up to logarithmic factors. the non-asymptotic analysis of this work requires very mild assumptions; (i) stabilizability of the system's dynamics, and (ii) limiting the degree of heaviness of the noise distribution. to establish such bounds, certain novel techniques are developed to comprehensively address the probabilistic behavior of dependent random matrices with heavy-tailed distributions.",,2017-11-20,2019-03-28,"['mohamad kazem shirani faradonbeh', 'ambuj tewari', 'george michailidis']"
45,1711.09628,order-sensitivity and equivariance of scoring functions,math.st stat.th,"the relative performance of competing point forecasts is usually measured in terms of loss or scoring functions. it is widely accepted that these scoring function should be strictly consistent in the sense that the expected score is minimized by the correctly specified forecast for a certain statistical functional such as the mean, median, or a certain risk measure. thus, strict consistency opens the way to meaningful forecast comparison, but is also important in regression and m-estimation. usually strictly consistent scoring functions for an elicitable functional are not unique. to give guidance on the choice of a scoring function, this paper introduces two additional quality criteria. order-sensitivity opens the possibility to compare two deliberately misspecified forecasts given that the forecasts are ordered in a certain sense. on the other hand, equivariant scoring functions obey similar equivariance properties as the functional at hand - such as translation invariance or positive homogeneity. in our study, we consider scoring functions for popular functionals, putting special emphasis on vector-valued functionals, e.g. the pair (mean, variance) or (value at risk, expected shortfall).",10.1214/19-ejs1552,2017-11-27,,"['tobias fissler', 'johanna f. ziegel']"
46,1711.11279,interpretability beyond feature attribution: quantitative testing with   concept activation vectors (tcav),stat.ml,"the interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. in addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. to address these challenges, we introduce concept activation vectors (cavs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. the key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. we show how to use cavs as part of a technique, testing with cavs (tcav), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of ""zebra"" is to the presence of stripes. using the domain of image classification as a testing ground, we describe how cavs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.",,2017-11-30,2018-06-07,"['been kim', 'martin wattenberg', 'justin gilmer', 'carrie cai', 'james wexler', 'fernanda viegas', 'rory sayres']"
47,1712.00203,closed-loop field development with multipoint geostatistics and   statistical performance assessment,math.oc cs.ce physics.data-an stat.ap,"closed-loop field development (clfd) optimization is a comprehensive framework for optimal development of subsurface resources. clfd involves three major steps: 1) optimization of full development plan based on current set of models, 2) drilling new wells and collecting new spatial and temporal (production) data, 3) model calibration based on all data. this process is repeated until the optimal number of wells is drilled. this work introduces an efficient clfd implementation for complex systems described by multipoint geostatistics (mps). model calibration is accomplished in two steps: conditioning to spatial data by a geostatistical simulation method, and conditioning to production data by optimization-based pca. a statistical procedure is presented to assess the performance of clfd. methodology is applied to an oil reservoir example for 25 different true-model cases. application of a single-step of clfd, improved the true npv in 64%--80% of cases. the full clfd procedure (with three steps) improved the true npv in 96% of cases, with an average improvement of 37%.",,2017-12-01,2019-04-02,['mehrdad g shirangi']
48,1801.09956,nonparametric bayesian volatility estimation,stat.me math.st q-fin.st stat.th,"given discrete time observations over a fixed time interval, we study a nonparametric bayesian approach to estimation of the volatility coefficient of a stochastic differential equation. we postulate a histogram-type prior on the volatility with piecewise constant realisations on bins forming a partition of the time interval. the values on the bins are assigned an inverse gamma markov chain (igmc) prior. posterior inference is straightforward to implement via gibbs sampling, as the full conditional distributions are available explicitly and turn out to be inverse gamma. we also discuss in detail the hyperparameter selection for our method. our nonparametric bayesian approach leads to good practical results in representative simulation examples. finally, we apply it on a classical data set in change-point analysis: weekly closings of the dow-jones industrial averages.",10.1007/978-3-030-04161-8_19,2018-01-30,2019-03-29,"['shota gugushvili', 'frank van der meulen', 'moritz schauer', 'peter spreij']"
49,1802.00680,a generative model for natural sounds based on latent force modelling,cs.lg cs.sd eess.as stat.ml,"recent advances in analysis of subband amplitude envelopes of natural sounds have resulted in convincing synthesis, showing subband amplitudes to be a crucial component of perception. probabilistic latent variable analysis is particularly revealing, but existing approaches don't incorporate prior knowledge about the physical behaviour of amplitude envelopes, such as exponential decay and feedback. we use latent force modelling, a probabilistic learning paradigm that incorporates physical knowledge into gaussian process regression, to model correlation across spectral subband envelopes. we augment the standard latent force model approach by explicitly modelling correlations over multiple time steps. incorporating this prior knowledge strengthens the interpretation of the latent functions as the source that generated the signal. we examine this interpretation via an experiment which shows that sounds generated by sampling from our probabilistic model are perceived to be more realistic than those generated by similar models based on nonnegative matrix factorisation, even in cases where our model is outperformed from a reconstruction error perspective.",,2018-02-02,2019-03-27,"['william j. wilkinson', 'joshua d. reiss', 'dan stowell']"
50,1802.04364,junction tree variational autoencoder for molecular graph generation,cs.lg cs.ne stat.ml,"we seek to automate the design of molecules based on specific chemical properties. in computational terms, this task involves continuous embedding and generation of molecular graphs. our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear smiles strings instead of graphs. our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. this approach allows us to incrementally expand molecules while maintaining chemical validity at every step. we evaluate our model on multiple tasks ranging from molecular generation to optimization. across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.",,2018-02-12,2019-03-29,"['wengong jin', 'regina barzilay', 'tommi jaakkola']"
51,1802.04888,the false positive risk: a proposal concerning what to do about p values,stat.ap,"it is widely acknowledged that the biomedical literature suffer from a surfeit of false positive results. part of the reason for this is the persistence of the myth that observation of a p value less than 0.05 is sufficient justification to claim that you've made a discovery.   it is hopeless to expect users to change their reliance on p values unless they are offered an alternative way of judging the reliability of their conclusions. if the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. one such proposal is based on calculation of false positive risk.   it is suggested that p values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p value. this number could be the minimum false positive risk (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). alternatively one could specify the prior probability that it would be necessary to believe in order to achieve a false positive risk of, say, 0.05.",10.1080/00031305.2018.1529622,2018-02-13,2019-01-10,['david colquhoun']
52,1802.06179,learning to race through coordinate descent bayesian optimisation,cs.ro cs.lg stat.ml,"in the automation of many kinds of processes, the observable outcome can often be described as the combined effect of an entire sequence of actions, or controls, applied throughout its execution. in these cases, strategies to optimise control policies for individual stages of the process might not be applicable, and instead the whole policy might have to be optimised at once. on the other hand, the cost to evaluate the policy's performance might also be high, being desirable that a solution can be found with as few interactions as possible with the real system. we consider the problem of optimising control policies to allow a robot to complete a given race track within a minimum amount of time. we assume that the robot has no prior information about the track or its own dynamical model, just an initial valid driving example. localisation is only applied to monitor the robot and to provide an indication of its position along the track's centre axis. we propose a method for finding a policy that minimises the time per lap while keeping the vehicle on the track using a bayesian optimisation (bo) approach over a reproducing kernel hilbert space. we apply an algorithm to search more efficiently over high-dimensional policy-parameter spaces with bo, by iterating over each dimension individually, in a sequential coordinate descent-like scheme. experiments demonstrate the performance of the algorithm against other methods in a simulated car racing environment.",10.1109/icra.2018.8460735,2018-02-16,,"['rafael oliveira', 'fernando h. m. rocha', 'lionel ott', 'vitor guizilini', 'fabio ramos', 'valdir grassi']"
53,1802.06476,simultaneous modeling of multiple complications for risk profiling in   diabetes care,cs.lg cs.ai stat.ml,"type 2 diabetes mellitus (t2dm) is a chronic disease that often results in multiple complications. risk prediction and profiling of t2dm complications is critical for healthcare professionals to design personalized treatment plans for patients in diabetes care for improved outcomes. in this paper, we study the risk of developing complications after the initial t2dm diagnosis from longitudinal patient records. we propose a novel multi-task learning approach to simultaneously model multiple complications where each task corresponds to the risk modeling of one complication. specifically, the proposed method strategically captures the relationships (1) between the risks of multiple t2dm complications, (2) between the different risk factors, and (3) between the risk factor selection patterns. the method uses coefficient shrinkage to identify an informative subset of risk factors from high-dimensional data, and uses a hierarchical bayesian framework to allow domain knowledge to be incorporated as priors. the proposed method is favorable for healthcare applications because in additional to improved prediction performance, relationships among the different risks and risk factors are also identified. extensive experimental results on a large electronic medical claims database show that the proposed method outperforms state-of-the-art models by a significant margin. furthermore, we show that the risk associations learned and the risk factors identified lead to meaningful clinical insights.",10.1109/tkde.2019.2904060,2018-02-18,,"['bin liu', 'ying li', 'soumya ghosh', 'zhaonan sun', 'kenney ng', 'jianying hu']"
54,1803.00816,netgan: generating graphs via random walks,stat.ml cs.lg cs.si,"we propose netgan - the first implicit generative model for graphs able to mimic real-world networks. we pose the problem of graph generation as learning the distribution of biased random walks over the input graph. the proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the wasserstein gan objective. netgan is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. at the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. being the first approach to combine both of these desirable properties, netgan opens exciting avenues for further research.",,2018-03-02,2018-06-01,"['aleksandar bojchevski', 'oleksandr shchur', 'daniel zügner', 'stephan günnemann']"
55,1803.01616,tensorial and bipartite block models for link prediction in layered   networks and temporal networks,physics.soc-ph cs.si physics.data-an q-bio.mn stat.ml,"many real-world complex systems are well represented as multilayer networks; predicting interactions in those systems is one of the most pressing problems in predictive network science. to address this challenge, we introduce two stochastic block models for multilayer and temporal networks; one of them uses nodes as its fundamental unit, whereas the other focuses on links. we also develop scalable algorithms for inferring the parameters of these models. because our models describe all layers simultaneously, our approach takes full advantage of the information contained in the whole network when making predictions about any particular layer. we illustrate the potential of our approach by analyzing two empirical datasets---a temporal network of email communications, and a network of drug interactions for treating different cancer types. we find that modeling all layers simultaneously does result, in general, in more accurate link prediction. however, the most predictive model depends on the dataset under consideration; whereas the node-based model is more appropriate for predicting drug interactions, the link-based model is more appropriate for predicting email communication.",10.1103/physreve.99.032307,2018-03-05,,"['marc tarres-deulofeu', 'antonia godoy-lorite', 'roger guimera', 'marta sales-pardo']"
56,1803.03328,a new bandwidth selection criterion for using svdd to analyze   hyperspectral data,stat.ap,"this paper presents a method for hyperspectral image classification that uses support vector data description (svdd) with the gaussian kernel function. svdd has been a popular machine learning technique for single-class classification, but selecting the proper gaussian kernel bandwidth to achieve the best classification performance is always a challenging problem. this paper proposes a new automatic, unsupervised gaussian kernel bandwidth selection approach which is used with a multiclass svdd classification scheme. the performance of the multiclass svdd classification scheme is evaluated on three frequently used hyperspectral data sets, and preliminary results show that the proposed method can achieve better performance than published results on these data sets.",10.1117/12.2314964,2018-03-08,2019-04-05,"['yuwei liao', 'deovrat kakde', 'arin chaudhuri', 'hansi jiang', 'carol sadek', 'seunghyun kong']"
57,1804.00217,fundamental resource trade-offs for encoded distributed optimization,cs.it cs.lg math.it stat.ml,"dealing with the shear size and complexity of today's massive data sets requires computational platforms that can analyze data in a parallelized and distributed fashion. a major bottleneck that arises in such modern distributed computing environments is that some of the worker nodes may run slow. these nodes a.k.a.~stragglers can significantly slow down computation as the slowest node may dictate the overall computational time. a recent computational framework, called encoded optimization, creates redundancy in the data to mitigate the effect of stragglers. in this paper we develop novel mathematical understanding for this framework demonstrating its effectiveness in much broader settings than was previously understood. we also analyze the convergence behavior of iterative encoded optimization algorithms, allowing us to characterize fundamental trade-offs between convergence rate, size of data set, accuracy, computational load (or data redundancy), and straggler toleration in this framework.",,2018-03-31,2019-04-01,"['a. salman avestimehr', 'seyed mohammadreza mousavi kalan', 'mahdi soltanolkotabi']"
58,1804.00355,near-optimal recovery of linear and n-convex functions on unions of   convex sets,math.st stat.th,"in this paper we build provably near-optimal, in the minimax sense, estimates of linear forms and, more generally, ""$n$-convex functionals"" (the simplest example being the maximum of several fractional-linear functions) of unknown ""signal"" known to belong to the union of finitely many convex compact sets from indirect noisy observations of the signal. our main assumption is that the observation scheme in question is good in the sense of a. goldenshluger, a. juditsky, a. nemirovski, electr. j. stat. 9(2) (2015), arxiv:1311.6765, the simplest example being the gaussian scheme where the observation is the sum of linear image of the signal and the standard gaussian noise. the proposed estimates, same as upper bounds on their worst-case risks, stem from solutions to explicit convex optimization problems, making the estimates ""computation-friendly.""",,2018-04-01,2019-03-29,"['anatoli juditsky', 'arkadi nemirovski']"
59,1804.00609,adaptive algorithm for sparse signal recovery,stat.me eess.sp,"spike and slab priors play a key role in inducing sparsity for sparse signal recovery. the use of such priors results in hard non-convex and mixed integer programming problems. most of the existing algorithms to solve the optimization problems involve either simplifying assumptions, relaxations or high computational expenses. we propose a new adaptive alternating direction method of multipliers (aadmm) algorithm to directly solve the presented optimization problem. the algorithm is based on the one-to-one mapping property of the support and non-zero element of the signal. at each step of the algorithm, we update the support by either adding an index to it or removing an index from it and use the alternating direction method of multipliers to recover the signal corresponding to the updated support. experiments on synthetic data and real-world images show that the proposed aadmm algorithm provides superior performance and is computationally cheaper, compared to the recently developed iterative convex refinement (icr) algorithm.",,2018-04-02,2019-03-30,"['fekadu l. bayisa', 'zhiyong zhou', 'ottmar cronie', 'jun yu']"
60,1804.07353,unsupervised representation adversarial learning network: from   reconstruction to generation,cs.cv cs.lg stat.ml,"a good representation for arbitrarily complicated data should have the capability of semantic generation, clustering and reconstruction. previous research has already achieved impressive performance on either one. this paper aims at learning a disentangled representation effective for all of them in an unsupervised way. to achieve all the three tasks together, we learn the forward and inverse mapping between data and representation on the basis of a symmetric adversarial process. in theory, we minimize the upper bound of the two conditional entropy loss between the latent variables and the observations together to achieve the cycle consistency. the newly proposed repgan is tested on mnist, fashionmnist, celeba, and svhn datasets to perform unsupervised classification, generation and reconstruction tasks. the result demonstrates that repgan is able to learn a useful and competitive representation. to the author's knowledge, our work is the first one to achieve both a high unsupervised classification accuracy and low reconstruction error on mnist. codes are available at https://github.com/yzhouas/repgan-tensorflow.",,2018-04-19,2019-04-06,"['yuqian zhou', 'kuangxiao gu', 'thomas huang']"
61,1805.00108,conditional molecular design with deep generative models,cs.lg stat.ml,"although machine learning has been successfully used to propose novel molecules that satisfy desired properties, it is still challenging to explore a large chemical space efficiently. in this paper, we present a conditional molecular design method that facilitates generating new molecules with desired properties. the proposed model, which simultaneously performs both property prediction and molecule generation, is built as a semi-supervised variational autoencoder trained on a set of existing molecules with only a partial annotation. we generate new molecules with desired properties by sampling from the generative distribution estimated by the model. we demonstrate the effectiveness of the proposed model by evaluating it on drug-like molecules. the model improves the performance of property prediction by exploiting unlabeled molecules, and efficiently generates novel molecules fulfilling various target conditions.",10.1021/acs.jcim.8b00263,2018-04-30,2018-07-23,"['seokho kang', 'kyunghyun cho']"
62,1805.01216,disentangling language and knowledge in task-oriented dialogs,cs.lg cs.cl stat.ml,"the knowledge base (kb) used for real-world applications, such as booking a movie or restaurant reservation, keeps changing over time. end-to-end neural networks trained for these task-oriented dialogs are expected to be immune to any changes in the kb. however, existing approaches breakdown when asked to handle such changes. we propose an encoder-decoder architecture (bossnet) with a novel bag-of-sequences (boss) memory, which facilitates the disentangled learning of the response's language model and its knowledge incorporation. consequently, the kb can be modified with new knowledge without a drop in interpretability. we find that bossnet outperforms state-of-the-art models, with considerable improvements (> 10\%) on babi oov test sets and other human-human datasets. we also systematically modify existing datasets to measure disentanglement and show bossnet to be robust to kb modifications.",,2018-05-03,2019-04-05,"['dinesh raghu', 'nikhil gupta', 'n/a mausam']"
63,1805.01862,"lasso, knockoff and gaussian covariates: a comparison",stat.me,"given data $\mathbf{y}$ and $k$ covariates $\mathbf{x}_j$ one problem in linear regression is to decide which if any of the covariates to include when regressing the dependent variable $\mathbf{y}$ on the covariates $\mathbf{x}_j$. in this paper three such methods, lasso, knockoff and gaussian covariates are compared using simulations and real data. the gaussian covariate method is based on exact probabilities which are valid for all $\mathbf{y}$ and $\mathbf{x}_j$ making it model free. moreover the probabilities agree with those based on the f-distribution for the standard linear model with i.i.d. gaussian errors. it is conceptually, mathematically and algorithmically very simple, it is very fast and makes no use of simulations. it outperforms lasso and knockoff in all respects by a considerable margin.",,2018-05-04,2019-03-30,['laurie davies']
64,1805.07405,processing of missing data by neural networks,cs.lg stat.ml,"we propose a general, theoretically justified mechanism for processing missing data by neural networks. our idea is to replace typical neuron's response in the first hidden layer by its expected value. this approach can be applied for various types of networks at minimal cost in their modification. moreover, in contrast to recent approaches, it does not require complete data for training. experimental results performed on different types of architectures show that our method gives better results than typical imputation strategies and other methods dedicated for incomplete data.",,2018-05-18,2019-04-03,"['marek smieja', 'łukasz struski', 'jacek tabor', 'bartosz zieliński', 'przemysław spurek']"
65,1805.07473,progressive ensemble networks for zero-shot recognition,cs.lg cs.ai cs.cv stat.ml,"despite the advancement of supervised image recognition algorithms, their dependence on the availability of labeled data and the rapid expansion of image categories raise the significant challenge of zero-shot learning. zero-shot learning (zsl) aims to transfer knowledge from labeled classes into unlabeled classes to reduce human labeling effort. in this paper, we propose a novel progressive ensemble network model with multiple projected label embeddings to address zero-shot image recognition. the ensemble network is built by learning multiple image classification functions with a shared feature extraction network but different label embedding representations, which enhance the diversity of the classifiers and facilitate information transfer to unlabeled classes. a progressive training framework is then deployed to gradually label the most confident images in each unlabeled class with predicted pseudo-labels and update the ensemble network with the training data augmented by the pseudo-labels. the proposed model performs training on both labeled and unlabeled data. it can naturally bridge the domain shift problem in visual appearances and be extended to the generalized zero-shot learning scenario. we conduct experiments on multiple zsl datasets and the empirical results demonstrate the efficacy of the proposed model.",,2018-05-18,2019-04-06,"['meng ye', 'yuhong guo']"
66,1805.07732,nonlinear distributional gradient temporal-difference learning,cs.lg cs.ai stat.ml,"we devise a distributional variant of gradient temporal-difference (td) learning. distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study \citep{bellemare2017distributional}. in the policy evaluation setting, we design two new algorithms called distributional gtd2 and distributional tdc using the cram{\'e}r distance on the distributional version of the bellman error objective function, which inherits advantages of both the nonlinear gradient td algorithms and the distributional rl approach. in the control setting, we propose the distributional greedy-gq using the similar derivation. we prove the asymptotic almost-sure convergence of distributional gtd2 and tdc to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life rl problems. in each step, the computational complexities of above three algorithms are linear w.r.t.\ the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.",,2018-05-20,2019-04-02,"['chao qu', 'shie mannor', 'huan xu']"
67,1805.07984,adversarial attacks on neural networks for graph data,stat.ml cs.cr cs.lg,"deep learning models for graphs have achieved strong performance for the task of node classification. despite their proliferation, currently there is no study of their robustness to adversarial attacks. yet, in domains where they are likely to be used, e.g. the web, adversaries are common. can deep learning models for graphs be easily fooled? in this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. in addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. we generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. to cope with the underlying discrete domain we propose an efficient algorithm nettack exploiting incremental computations. our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.",10.1145/3219819.3220078,2018-05-21,2018-06-12,"['daniel zügner', 'amir akbarnejad', 'stephan günnemann']"
68,1805.09091,neural networks for post-processing ensemble weather forecasts,stat.ml cs.lg physics.ao-ph stat.ap stat.me,"ensemble weather predictions require statistical post-processing of systematic errors to obtain reliable and accurate probabilistic forecasts. traditionally, this is accomplished with distributional regression models in which the parameters of a predictive distribution are estimated from a training period. we propose a flexible alternative based on neural networks that can incorporate nonlinear relationships between arbitrary predictor variables and forecast distribution parameters that are automatically learned in a data-driven way rather than requiring pre-specified link functions. in a case study of 2-meter temperature forecasts at surface stations in germany, the neural network approach significantly outperforms benchmark post-processing methods while being computationally more affordable. key components to this improvement are the use of auxiliary predictor variables and station-specific information with the help of embeddings. furthermore, the trained neural network can be used to gain insight into the importance of meteorological variables thereby challenging the notion of neural networks as uninterpretable black boxes. our approach can easily be extended to other statistical post-processing and forecasting problems. we anticipate that recent advances in deep learning combined with the ever-increasing amounts of model and observation data will transform the post-processing of numerical weather forecasts in the coming decade.",10.1175/mwr-d-18-0187.1,2018-05-23,,"['stephan rasp', 'sebastian lerch']"
69,1805.10755,dual policy iteration,cs.lg stat.ml,"recently, a novel class of approximate policy iteration (api) algorithms have demonstrated impressive practical performance (e.g., exit from [2], alphago-zero from [27]). this new family of algorithms maintains, and alternately optimizes, two policies: a fast, reactive policy (e.g., a deep neural network) deployed at test time, and a slow, non-reactive policy (e.g., tree search), that can plan multiple steps ahead. the reactive policy is updated under supervision from the non-reactive policy, while the non-reactive policy is improved with guidance from the reactive policy. in this work we study this dual policy iteration (dpi) strategy in an alternating optimization framework and provide a convergence analysis that extends existing api theory. we also develop a special instance of this framework which reduces the update of non-reactive policies to model-based optimal control using learned local models, and provides a theoretically sound way of unifying model-free and model-based rl approaches with unknown dynamics. we demonstrate the efficacy of our approach on various continuous control markov decision processes.",,2018-05-27,2019-04-05,"['wen sun', 'geoffrey j. gordon', 'byron boots', 'j. andrew bagnell']"
70,1805.11126,statistical methods in computed tomography image estimation,stat.me stat.ap,"purpose: there is increasing interest in computed tomography (ct) image estimations from magnetic resonance (mr) images. the estimated ct images can be utilised for attenuation correction, patient positioning, and dose planning in diagnostic and radiotherapy workflows. this study aims to introduce a novel statistical learning approach for improving ct estimation from mr images and to compare the performance of our method with the existing model based ct image estimation methods.   methods: the statistical learning approach proposed here consists of two stages. at the training stage, prior knowledges about tissue-types from ct images were used together with a gaussian mixture model (gmm) to explore ct image estimations from mr images. since the prior knowledges are not available at the prediction stage, a classifier based on rusboost algorithm was trained to estimate the tissue-types from mr images. for a new patient, the trained classifier and gmms were used to predict ct image from mr images. the classifier and gmms were validated by using voxel level 10-fold cross-validation and patient-level leave-one-out cross-validation, respectively.   results: the proposed approach has outperformance in ct estimation quality in comparison with the existing model based methods, especially on bone tissues. our method improved ct image estimation by 5% and 23% on the whole brain and bone tissues, respectively.   conclusions: evaluation of our method shows that it is a promising method to generate ct image substitutes for the implementation of fully mr-based radiotherapy and pet/mri applications.",10.1002/mp.13204,2018-05-28,2019-03-30,"['fekadu l. bayisa', 'xijia liu', 'anders garpebring', 'jun yu']"
71,1805.12279,bayesian pose graph optimization via bingham distributions and tempered   geodesic mcmc,cs.cv cs.ai cs.cg cs.ro stat.ml,"we introduce tempered geodesic markov chain monte carlo (tg-mcmc) algorithm for initializing pose graph optimization problems, arising in various scenarios such as sfm (structure from motion) or slam (simultaneous localization and mapping). tg-mcmc is first of its kind as it unites asymptotically global non-convex optimization on the spherical manifold of quaternions with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of individual solutions. we devise rigorous theoretical convergence guarantees for our method and extensively evaluate it on synthetic and real benchmark datasets. besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.",,2018-05-30,2019-03-30,"['tolga birdal', 'umut şimşekli', 'm. onur eken', 'slobodan ilic']"
72,1806.04655,figurenet: a deep learning model for question-answering on scientific   plots,cs.lg stat.ml,"deep learning has managed to push boundaries in a wide variety of tasks. one area of interest is to tackle problems in reasoning and understanding, with an aim to emulate human intelligence. in this work, we describe a deep learning model that addresses the reasoning task of question-answering on categorical plots. we introduce a novel architecture figurenet, that learns to identify various plot elements, quantify the represented values and determine a relative ordering of these statistical values. we test our model on the figureqa dataset which provides images and accompanying questions for scientific plots like bar graphs and pie charts, augmented with rich annotations. our approach outperforms the state-of-the-art relation networks baseline by approximately $7\%$ on this dataset, with a training time that is over an order of magnitude lesser.",,2018-06-12,2019-04-01,"['revanth reddy', 'rahul ramesh', 'ameet deshpande', 'mitesh m. khapra']"
73,1806.05049,map inference via block-coordinate frank-wolfe algorithm,cs.lg cs.ai stat.ml,"we present a new proximal bundle method for maximum-a-posteriori (map) inference in structured energy minimization problems. the method optimizes a lagrangean relaxation of the original energy minimization problem using a multi plane block-coordinate frank-wolfe method that takes advantage of the specific structure of the lagrangean decomposition. we show empirically that our method outperforms state-of-the-art lagrangean decomposition based algorithms on some challenging markov random field, multi-label discrete tomography and graph matching problems.",,2018-06-13,2019-04-05,"['paul swoboda', 'vladimir kolmogorov']"
74,1806.05077,"mixed-normal limit theorems for multiple skorohod integrals in   high-dimensions, with application to realized covariance",math.st math.pr stat.th,"this paper develops mixed-normal approximations for probabilities that vectors of multiple skorohod integrals belong to random convex polytopes when the dimensions of the vectors possibly diverge to infinity. we apply the developed theory to establish the asymptotic mixed normality of the realized covariance matrix of a high-dimensional continuous semimartingale observed at a high-frequency, where the dimension can be much larger than the sample size. we also present an application of this result to testing the residual sparsity of a high-dimensional continuous-time factor model.",,2018-06-13,2019-03-31,['yuta koike']
75,1806.05357,deep multi-output forecasting: learning to accurately predict blood   glucose trajectories,cs.lg stat.ml,"in many forecasting applications, it is valuable to predict not only the value of a signal at a certain time point in the future, but also the values leading up to that point. this is especially true in clinical applications, where the future state of the patient can be less important than the patient's overall trajectory. this requires multi-step forecasting, a forecasting variant where one aims to predict multiple values in the future simultaneously. standard methods to accomplish this can propagate error from prediction to prediction, reducing quality over the long term. in light of these challenges, we propose multi-output deep architectures for multi-step forecasting in which we explicitly model the distribution of future values of the signal over a prediction horizon. we apply these techniques to the challenging and clinically relevant task of blood glucose forecasting. through a series of experiments on a real-world dataset consisting of 550k blood glucose measurements, we demonstrate the effectiveness of our proposed approaches in capturing the underlying signal dynamics. compared to existing shallow and deep methods, we find that our proposed approaches improve performance individually and capture complementary information, leading to a large improvement over the baseline when combined (4.87 vs. 5.31 absolute percentage error (ape)). overall, the results suggest the efficacy of our proposed approach in predicting blood glucose level and multi-step forecasting more generally.",10.1145/3219819.3220102,2018-06-14,,"['ian fox', 'lynn ang', 'mamta jaiswal', 'rodica pop-busui', 'jenna wiens']"
76,1806.06403,geometric mean extension for data sets with zeros,stat.ap,"there are numerous examples in different research fields where the use of the geometric mean is more appropriate than the arithmetic mean. however, the geometric mean has a serious limitation in comparison with the arithmetic mean. means are used to summarize the information in a large set of values in a single number; yet, the geometric mean of a data set with at least one zero is always zero. as a result, the geometric mean does not capture any information about the non-zero values. the purpose of this short contribution is to review solutions proposed in the literature that enable the computation of the geometric mean of data sets containing zeros and to show that they do not fulfil the `recovery' or `monotonicity' conditions that we define. the standard geometric mean should be recovered from the modified geometric mean if the data set does not contain any zeros (recovery condition). also, if the values of an ordered data set are greater one by one than the values of another data set then the modified geometric mean of the first data set must be greater than the modified geometric mean of the second data set (monotonicity condition). we then formulate a modified version of the geometric mean that can handle zeros while satisfying both desired conditions.",,2018-06-17,2019-04-04,"['roberto de la cruz', 'jan-ulrich kreft']"
77,1806.06908,designing optimal binary rating systems,cs.lg stat.ml,"modern online platforms rely on effective rating systems to learn about items. we consider the optimal design of rating systems that collect binary feedback after transactions. we make three contributions. first, we formalize the performance of a rating system as the speed with which it recovers the true underlying ranking on items (in a large deviations sense), accounting for both items' underlying match rates and the platform's preferences. second, we provide an efficient algorithm to compute the binary feedback system that yields the highest such performance. finally, we show how this theoretical perspective can be used to empirically design an implementable, approximately optimal rating system, and validate our approach using real-world experimental data collected on amazon mechanical turk.",,2018-06-18,2019-04-06,"['nikhil garg', 'ramesh johari']"
78,1806.07307,estimation from non-linear observations via convex programming with   application to bilinear regression,stat.ml cs.lg math.oc stat.co,"we propose a computationally efficient estimator, formulated as a convex program, for a broad class of non-linear regression problems that involve difference of convex (dc) non-linearities. the proposed method can be viewed as a significant extension of the ""anchored regression"" method formulated and analyzed in [10] for regression with convex non-linearities. our main assumption, in addition to other mild statistical and computational assumptions, is availability of a certain approximation oracle for the average of the gradients of the observation functions at a ground truth. under this assumption and using a pac-bayesian analysis we show that the proposed estimator produces an accurate estimate with high probability. as a concrete example, we study the proposed framework in the bilinear regression problem with gaussian factors and quantify a sufficient sample complexity for exact recovery. furthermore, we describe a computationally tractable scheme that provably produces the required approximation oracle in the considered bilinear regression problem.",,2018-06-19,2019-03-28,['sohail bahmani']
79,1806.07934,a function emulation approach for doubly intractable distributions,stat.co,"doubly intractable distributions arise in many settings, for example in markov models for point processes and exponential random graph models for networks. bayesian inference for these models is challenging because they involve intractable normalising ""constants"" that are actually functions of the parameters of interest. although several clever computational methods have been developed for these models, each method suffers from computational issues that makes it computationally burdensome or even infeasible for many problems. we propose a novel algorithm that provides computational gains over existing methods by replacing monte carlo approximations to the normalising function with a gaussian process-based approximation. we provide theoretical justification for this method. we also develop a closely related algorithm that is applicable more broadly to any likelihood function that is expensive to evaluate. we illustrate the application of our methods to a variety of challenging simulated and real data examples, including an exponential random graph model, a markov point process, and a model for infectious disease dynamics. the algorithm shows significant gains in computational efficiency over existing methods, and has the potential for greater gains for more challenging problems. for a random graph model example, we show how this gain in efficiency allows us to carry out accurate bayesian inference when other algorithms are computationally impractical.",,2018-06-20,2019-04-02,"['jaewoo park', 'murali haran']"
80,1806.09386,treatment effects beyond the mean using gamlss,stat.ap,"this paper introduces distributional regression, also known as generalized additive models for location, scale and shape (gamlss), as a modeling framework for analyzing treatment effects beyond the mean. by relating each parameter of the response distribution to explanatory variables, gamlss model the treatment effect on the whole conditional distribution. additionally, any nonnormal outcome and nonlinear effects of explanatory variables can be incorporated. we elaborate on the combination of gamlss with program evaluation methods in economics and provide practical guidance on the usage of gamlss by reanalyzing data from the mexican \textit{progresa} program. contrary to expectations, no significant effects of a cash transfer on the conditional inequality level between treatment and control group are found.",,2018-06-25,2019-03-28,"['maike hohberg', 'peter pütz', 'thomas kneib']"
81,1806.09401,quasi-likelihood analysis of an ergodic diffusion plus noise,math.st stat.me stat.th,we consider adaptive maximum-likelihood-type estimators and adaptive bayes-type ones for discretely observed ergodic diffusion processes with observation noise whose variance is constant. the quasi-likelihood functions for the diffusion and drift parameters are introduced and the polynomial-type large deviation inequalities for those quasi-likelihoods are shown to see the convergence of moments for those estimators.,,2018-06-25,2019-04-01,"['shogo h. nakakita', 'masayuki uchida']"
82,1806.10008,the conditionality principle in high-dimensional regression,math.st stat.th,"consider a high-dimensional linear regression problem, where the number of covariates is larger than the number of observations and the interest is in estimating the conditional variance of the response variable given the covariates. a conditional and unconditioned framework are considered, where conditioning is with respect to the covariates, which are ancillary to the parameter of interest. in recent papers, a consistent estimator was developed in the unconditional framework when the marginal distribution of the covariates is normal with known mean and variance. in the present work, a certain bayesian hypothesis test is formulated under the conditional framework, and it is shown that the bayes risk is a constant. this implies that no consistent estimator exists in the conditional framework. however, when the marginal distribution of the covariates is normal, the conditional error of the above consistent estimator converges to zero, with probability converging to one. it follows that even in the conditional setting, information about the marginal distribution of an ancillary statistic may have a significant impact on statistical inference. the practical implication in the context of high-dimensional regression models is that additional observations, where only the covariates are given, are potentially very useful and should not be ignored. this finding is most relevant to semi-supervised learning problems where covariate information is easy to obtain.",,2018-06-26,2019-03-27,['david azriel']
83,1806.10234,scalable gaussian process inference with finite-data mean and variance   guarantees,stat.ml cs.lg stat.co stat.me,"gaussian processes (gps) offer a flexible class of priors for nonparametric bayesian regression, but popular gp posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. we develop an approach to scalable approximate gp regression with finite-data guarantees on the accuracy of pointwise posterior mean and variance estimates. our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned fisher (pf) divergence. we show that unlike the kullback--leibler divergence (used in variational inference), the pf divergence bounds the 2-wasserstein distance, which in turn provides tight bounds the pointwise difference of the mean and variance functions. we demonstrate that, for sparse gp likelihood approximations, we can minimize the pf divergence efficiently. our experiments show that optimizing the pf divergence has the same computational requirements as variational sparse gps while providing comparable empirical performance--in addition to our novel finite-data quality guarantees.",,2018-06-26,2019-03-27,"['jonathan h. huggins', 'trevor campbell', 'mikołaj kasprzak', 'tamara broderick']"
84,1807.01406,connecting weighted automata and recurrent neural networks through   spectral learning,cs.lg cs.fl stat.ml,"in this paper, we unravel a fundamental connection between weighted finite automata~(wfas) and second-order recurrent neural networks~(2-rnns): in the case of sequences of discrete symbols, wfas and 2-rnns with linear activation functions are expressively equivalent. motivated by this result, we build upon a recent extension of the spectral learning algorithm to vector-valued wfas and propose the first provable learning algorithm for linear 2-rnns defined over sequences of continuous input vectors. this algorithm relies on estimating low rank sub-blocks of the so-called hankel tensor, from which the parameters of a linear 2-rnn can be provably recovered. the performances of the proposed method are assessed in a simulation study.",,2018-07-03,2019-04-07,"['guillaume rabusseau', 'tianyu li', 'doina precup']"
85,1807.02701,deepsource: point source detection using deep learning,astro-ph.im cs.cv cs.lg hep-ph stat.ml,"point source detection at low signal-to-noise is challenging for astronomical surveys, particularly in radio interferometry images where the noise is correlated. machine learning is a promising solution, allowing the development of algorithms tailored to specific telescope arrays and science cases. we present deepsource - a deep learning solution - that uses convolutional neural networks to achieve these goals. deepsource enhances the signal-to-noise ratio (snr) of the original map and then uses dynamic blob detection to detect sources. trained and tested on two sets of 500 simulated 1 deg x 1 deg meerkat images with a total of 300,000 sources, deepsource is essentially perfect in both purity and completeness down to snr = 4 and outperforms pybdsf in all metrics. for uniformly-weighted images it achieves a purity x completeness (pc) score at snr = 3 of 0.73, compared to 0.31 for the best pybdsf model. for natural-weighting we find a smaller improvement of ~40% in the pc score at snr = 3. if instead we ask where either of the purity or completeness first drop to 90%, we find that deepsource reaches this value at snr = 3.6 compared to the 4.3 of pybdsf (natural-weighting). a key advantage of deepsource is that it can learn to optimally trade off purity and completeness for any science case under consideration. our results show that deep learning is a promising approach to point source detection in astronomical images.",10.1093/mnras/stz131,2018-07-07,,"['a. vafaei sadr', 'etienne. e. vos', 'bruce a. bassett', 'zafiirah hosenie', 'n. oozeer', 'michelle lochner']"
86,1807.03142,faster bounding box annotation for object detection in indoor scenes,cs.cv cs.lg stat.ml,"this paper proposes an approach for rapid bounding box annotation for object detection datasets. the procedure consists of two stages: the first step is to annotate a part of the dataset manually, and the second step proposes annotations for the remaining samples using a model trained with the first stage annotations. we experimentally study which first/second stage split minimizes to total workload. in addition, we introduce a new fully labeled object detection dataset collected from indoor scenes. compared to other indoor datasets, our collection has more class categories, different backgrounds, lighting conditions, occlusion and high intra-class differences. we train deep learning based object detectors with a number of state-of-the-art models and compare them in terms of speed and accuracy. the fully annotated dataset is released freely available for the research community.",10.1109/euvip.2018.8611732,2018-07-03,,"['bishwo adhikari', 'jukka peltomäki', 'jussi puura', 'heikki huttunen']"
87,1807.03929,quantification under prior probability shift: the ratio estimator and   its extensions,stat.ml cs.lg,"the quantification problem consists of determining the prevalence of a given label in a target population. however, one often has access to the labels in a sample from the training population but not in the target population. a common assumption in this situation is that of prior probability shift, that is, once the labels are known, the distribution of the features is the same in the training and target populations. in this paper, we derive a new lower bound for the risk of the quantification problem under the prior shift assumption. complementing this lower bound, we present a new approximately minimax class of estimators, ratio estimators, which generalize several previous proposals in the literature. using a weaker version of the prior shift assumption, which can be tested, we show that ratio estimators can be used to build confidence intervals for the quantification problem. we also extend the ratio estimator so that it can: (i) incorporate labels from the target population, when they are available and (ii) estimate how the prevalence of positive labels varies according to a function of certain covariates.",,2018-07-10,2019-04-05,"['afonso fernandes vaz', 'rafael izbicki', 'rafael bassi stern']"
88,1807.03981,a note on the distribution of the product of zero mean correlated normal   random variables,math.st stat.th,"the problem of finding an explicit formula for the probability density function of two zero mean correlated normal random variables dates back to 1936. perhaps surprisingly, this problem was not resolved until 2016. this is all the more surprising given that a very simple proof is available, which is the subject of this note; we identify the product of two zero mean correlated normal random variables as a variance-gamma random variable, from which an explicit formula for probability density function is immediate.",,2018-07-11,,['robert e. gaunt']
89,1807.04193,distributed variational representation learning,stat.ml cs.lg,"the problem of distributed representation learning is one in which multiple sources of information $x_1,\ldots,x_k$ are processed separately so as to learn as much information as possible about some ground truth $y$. we investigate this problem from information-theoretic grounds, through a generalization of tishby's centralized information bottleneck (ib) method to the distributed setting. specifically, $k$ encoders, $k \geq 2$, compress their observations $x_1,\ldots,x_k$ separately in a manner such that, collectively, the produced representations preserve as much information as possible about $y$. we study both discrete memoryless (dm) and memoryless vector gaussian data models. for the discrete model, we establish a single-letter characterization of the optimal tradeoff between complexity (or rate) and relevance (or information) for a class of memoryless sources (the observations $x_1,\ldots,x_k$ being conditionally independent given $y$). for the vector gaussian model, we provide an explicit characterization of the optimal complexity-relevance tradeoff. furthermore, we develop a variational bound on the complexity-relevance tradeoff which generalizes the evidence lower bound (elbo) to the distributed setting. we also provide two algorithms that allow to compute this bound: i) a blahut-arimoto type iterative algorithm which enables to compute optimal complexity-relevance encoding mappings by iterating over a set of self-consistent equations, and ii) a variational inference type algorithm in which the encoding mappings are parametrized by neural networks and the bound approximated by markov sampling and optimized with stochastic gradient descent. numerical results on synthetic and real datasets are provided to support the efficiency of the approaches and algorithms developed in this paper.",,2018-07-11,2019-03-31,"['inaki estella aguerri', 'abdellatif zaidi']"
90,1807.05819,bayes factor testing of equality and order constraints on measures of   association in social research,stat.me,"measures of association play a central role in the social sciences to quantify the strength of a linear relationship between the variables of interest. in many applications researchers can translate scientific expectations to hypotheses with equality and/or order constraints on these measures of association. in this paper a bayes factor test is proposed for testing multiple hypotheses with constraints on the measures of association between ordinal and/or continuous variables, possibly after correcting for certain covariates. this test can be used to obtain a direct answer to the research question how much evidence there is in the data for a social science theory relative to competing theories. the accompanying software package `bct' allows users to apply the methodology in an easy manner. an empirical application from leisure studies about the associations between life, leisure and relationship satisfaction and an application about the differences about egalitarian justice beliefs across countries are used to illustrate the methodology.",,2018-07-16,2019-04-03,"['joris mulder', 'john p. t. m. gelissen']"
91,1807.07560,compositional gan: learning image-conditional binary composition,cs.cv cs.ai cs.lg stat.ml,"generative adversarial networks (gans) can produce images of remarkable complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. in this work, we propose a novel self-consistent composition-by-decomposition (code) network to compose a pair of objects. given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. we evaluate our approach through qualitative experiments and user evaluations. our results indicate that the learned model captures potential interactions between the two object domains, and generates realistic composed scenes at test time.",,2018-07-19,2019-03-28,"['samaneh azadi', 'deepak pathak', 'sayna ebrahimi', 'trevor darrell']"
92,1807.07978,prior convictions: black-box adversarial attacks with bandits and priors,stat.ml cs.cr cs.lg,"we study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. we introduce a framework that conceptually unifies much of the existing work on black-box attacks, and we demonstrate that the current state-of-the-art methods are optimal in a natural sense. despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. we give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. the resulting methods use two to four times fewer queries and fail two to five times less often than the current state-of-the-art.",,2018-07-20,2019-03-27,"['andrew ilyas', 'logan engstrom', 'aleksander madry']"
93,1807.08934,saags: biased stochastic variance reduction methods for large-scale   learning,cs.lg cs.ai stat.ml,"stochastic approximation is one of the effective approach to deal with the large-scale machine learning problems and the recent research has focused on reduction of variance, caused by the noisy approximations of the gradients. in this paper, we have proposed novel variants of saag-i and ii (stochastic average adjusted gradient) (chauhan et al. 2017), called saag-iii and iv, respectively. unlike saag-i, starting point is set to average of previous epoch in saag-iii, and unlike saag-ii, the snap point and starting point are set to average and last iterate of previous epoch in saag-iv, respectively. to determine the step size, we have used stochastic backtracking-armijo line search (sbas) which performs line search only on selected mini-batch of data points. since backtracking line search is not suitable for large-scale problems and the constants used to find the step size, like lipschitz constant, are not always available so sbas could be very effective in such cases. we have extended saags (i, ii, iii and iv) to solve non-smooth problems and designed two update rules for smooth and non-smooth problems. moreover, our theoretical results have proved linear convergence of saag-iv for all the four combinations of smoothness and strong-convexity, in expectation. finally, our experimental studies have proved the efficacy of proposed methods against the state-of-art techniques.",10.1007/s10489-019-01450-3,2018-07-24,2019-04-06,"['vinod kumar chauhan', 'anuj sharma', 'kalpana dahiya']"
94,1807.10869,residual balancing: a method of constructing weights for marginal   structural models,stat.ap,"when making causal inferences, post-treatment confounders complicate analyses of time-varying treatment effects. conditioning on these variables naively to estimate marginal effects may inappropriately block causal pathways and may induce spurious associations between treatment and the outcome, leading to bias. to avoid such bias, researchers often use marginal structural models (msms) with inverse probability weighting (ipw). however, ipw requires models for the conditional distributions of treatment and is highly sensitive to their misspecification. moreover, ipw is relatively inefficient, susceptible to finite-sample bias, and difficult to use with continuous treatments. we introduce an alternative method of constructing weights for msms, which we call ""residual balancing."" in contrast to ipw, it requires modeling the conditional means of the post-treatment confounders rather than the conditional distributions of treatment, and it is therefore easier to use with continuous exposures. numeric simulations suggest that residual balancing is both more efficient and more robust to model misspecification than ipw and its variants. we illustrate the method by estimating (a) the cumulative effect of negative advertising on election outcomes and (b) the controlled direct effect of shared democracy on public support for war. open source software is available for implementing the proposed method.",,2018-07-27,2019-03-29,"['xiang zhou', 'geoffrey t. wodtke']"
95,1808.01174,generalization error in deep learning,cs.lg cs.ai stat.ml,"deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. however, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. in this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.",,2018-08-03,2019-04-06,"['daniel jakubovitz', 'raja giryes', 'miguel r. d. rodrigues']"
96,1808.01630,a review of learning with deep generative models from perspective of   graphical modeling,cs.lg stat.ml,"this document aims to provide a review on learning with deep generative models (dgms), which is an highly-active area in machine learning and more generally, artificial intelligence. this review is not meant to be a tutorial, but when necessary, we provide self-contained derivations for completeness. this review has two features. first, though there are different perspectives to classify dgms, we choose to organize this review from the perspective of graphical modeling, because the learning methods for directed dgms and undirected dgms are fundamentally different. second, we differentiate model definitions from model learning algorithms, since different learning algorithms can be applied to solve the learning problem on the same model, and an algorithm can be applied to learn different models. we thus separate model definition and model learning, with more emphasis on reviewing, differentiating and connecting different learning algorithms. we also discuss promising future research directions.",,2018-08-05,2019-03-26,['zhijian ou']
97,1808.03216,data-driven polynomial chaos expansion for machine learning regression,stat.ml cs.lg stat.co,"we present a regression technique for data-driven problems based on polynomial chaos expansion (pce). pce is a popular technique in the field of uncertainty quantification (uq), where it is typically used to replace a runnable but expensive computational model subject to random inputs with an inexpensive-to-evaluate polynomial function. the metamodel obtained enables a reliable estimation of the statistics of the output, provided that a suitable probabilistic model of the input is available. machine learning (ml) regression is a research field that focuses on providing purely data-driven input-output maps, with the focus on pointwise prediction accuracy. we show that a pce metamodel purely trained on data can yield pointwise predictions whose accuracy is comparable to that of other ml regression models, such as neural networks and support vector machines. the comparisons are performed on benchmark datasets available from the literature. the methodology also enables the quantification of the output uncertainties, and is robust to noise. furthermore, it enjoys additional desirable properties, such as good performance for small training sets and simplicity of construction, with only little parameter tuning required.",10.1016/j.jcp.2019.03.039,2018-08-09,2019-04-01,"['e. torre', 's. marelli', 'p. embrechts', 'b. sudret']"
98,1808.05298,monitoring through many eyes: integrating disparate datasets to improve   monitoring of the great barrier reef,stat.ap,"numerous organisations collect data in the great barrier reef (gbr), but they are rarely analysed together due to different program objectives, methods, and data quality. we developed a weighted spatiotemporal bayesian model and used it to integrate image based hard coral data collected by professional and citizen scientists, who captured and or classified underwater images. we used the model to predict coral cover across the gbr with estimates of uncertainty; thus filling gaps in space and time where no data exist. additional data increased the models predictive ability by 43 percent, but did not affect model inferences about pressures (e.g. bleaching and cyclone damage). thus, effective integration of professional and high-volume citizen data could enhance the capacity and cost efficiency of monitoring programs. this general approach is equally viable for other variables collected in the marine environment or other ecosystems; opening up new opportunities to integrate data and provide pathways for community engagement and stewardship.",,2018-08-15,2019-03-27,"['erin e peterson', 'edgar santos-fernández', 'carla chen', 'sam clifford', 'julie vercelloni', 'alan pearse', 'ross brown', 'bryce christensen', 'allan james', 'ken anthony', 'jennifer loder', 'manuel gonzález-rivero', 'chris roelfsema', 'm. julian caley', 'tomasz bednarz', 'kerrie mengersen']"
99,1808.05464,transfer learning for brain-computer interfaces: a euclidean space data   alignment approach,cs.lg cs.hc q-bio.nc stat.ml,"objective: this paper targets a major challenge in developing practical eeg-based brain-computer interfaces (bcis): how to cope with individual differences so that better learning performance can be obtained for a new subject, with minimum or even no subject-specific data? methods: we propose a novel approach to align eeg trials from different subjects in the euclidean space to make them more similar, and hence improve the learning performance for a new subject. our approach has three desirable properties: 1) it aligns the eeg trials directly in the euclidean space, and any signal processing, feature extraction and machine learning algorithms can then be applied to the aligned trials; 2) its computational cost is very low; and, 3) it is unsupervised and does not need any label information from the new subject. results: both offline and simulated online experiments on motor imagery classification and event-related potential classification verified that our proposed approach outperformed a state-of-the-art riemannian space data alignment approach, and several approaches without data alignment. conclusion: the proposed euclidean space eeg data alignment approach can greatly facilitate transfer learning in bcis. significance: our proposed approach is effective, efficient, and easy to implement. it could be an essential pre-processing step for eeg-based bcis.",,2018-08-08,2019-04-02,"['he he', 'dongrui wu']"
100,1808.07801,on a 'two truths' phenomenon in spectral graph clustering,stat.ml cs.lg,"clustering is concerned with coherently grouping observations without any explicit concept of true groupings. spectral graph clustering - clustering the vertices of a graph based on their spectral embedding - is commonly approached via k-means (or, more generally, gaussian mixture model) clustering composed with either laplacian or adjacency spectral embedding (lse or ase). recent theoretical results provide new understanding of the problem and solutions, and lead us to a 'two truths' lse vs. ase spectral graph clustering phenomenon convincingly illustrated here via a diffusion mri connectome data set: the different embedding methods yield different clustering results, with lse capturing left hemisphere/right hemisphere affinity structure and ase capturing gray matter/white matter core-periphery structure.",10.1073/pnas.1814462116,2018-08-23,2019-02-11,"['carey e. priebe', 'youngser park', 'joshua t. vogelstein', 'john m. conroy', 'vince lyzinski', 'minh tang', 'avanti athreya', 'joshua cape', 'eric bridgeford']"
101,1808.09920,question answering by reasoning across documents with graph   convolutional networks,cs.cl stat.ml,"most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. we introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. we frame it as an inference problem on a graph. mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document co-reference). graph convolutional networks (gcns) are applied to these graphs and trained to perform multi-step reasoning. our entity-gcn method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, wikihop (welbl et al., 2018).",,2018-08-29,2019-04-07,"['nicola de cao', 'wilker aziz', 'ivan titov']"
102,1809.01322,preferential sampling for presence/absence data and for fusion of   presence/absence data with presence-only data,stat.me,"presence/absence data and presence-only data are the two customary sources for learning about species distributions over a region. we illuminate the fundamental modeling differences between the two types of data. most simply, locations are considered as fixed under presence/absence data; locations are random under presence-only data. the definition of ""probability of presence"" is incompatible between the two. so, we take issue with modeling strategies in the literature which ignore this incompatibility, which assume that presence/absence modeling can be induced from presence-only specifications and therefore, that fusion of presence-only and presence/absence data sources is routine. we argue that presence/absence data should be modeled at point level. that is, we need to specify a surface which provides the probability of presence at any location in the region. a realization from this surface is a binary map yielding the results of bernoulli trials across all locations. presence-only data should be modeled as a point pattern driven by specification of an intensity function. we further argue that, with just presence/absence data, preferential sampling, using a shared process perspective, can improve our estimated presence/absence surface and prediction of presence. we also argue that preferential sampling can enable a probabilistically coherent fusion of the two data types. we illustrate with two real datasets, one presence/absence, one presence-only for invasive species presence in new england in the united states. we demonstrate that potential bias in sampling locations can affect inference with regard to presence/absence and show that inference can be improved with preferential sampling ideas. we also provide a probabilistically coherent fusion of the two datasets to again improve inference with regard to presence/absence.",,2018-09-05,2019-04-02,"['alan. e. gelfand', 'shinichiro shirota']"
103,1809.01712,coverage-based designs improve sample mining and hyper-parameter   optimization,cs.lg stat.ml,"sampling one or more effective solutions from large search spaces is a recurring idea in machine learning, and sequential optimization has become a popular solution. typical examples include data summarization, sample mining for predictive modeling and hyper-parameter optimization. existing solutions attempt to adaptively trade-off between global exploration and local exploitation, wherein the initial exploratory sample is critical to their success. while discrepancy-based samples have become the de facto approach for exploration, results from computer graphics suggest that coverage-based designs, e.g. poisson disk sampling, can be a superior alternative. in order to successfully adopt coverage-based sample designs to ml applications, which were originally developed for 2-d image analysis, we propose fundamental advances by constructing a parameterized family of designs with provably improved coverage characteristics, and by developing algorithms for effective sample synthesis. using experiments in sample mining and hyper-parameter optimization for supervised learning, we show that our approach consistently outperforms existing exploratory sampling methods in both blind exploration, and sequential search with bayesian optimization.",,2018-09-05,2019-04-04,"['gowtham muniraju', 'bhavya kailkhura', 'jayaraman j. thiagarajan', 'peer-timo bremer', 'cihan tepedelenlioglu', 'andreas spanias']"
104,1809.01804,discovering influential factors in variational autoencoder,cs.lg stat.ml,"in the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. in this work, we focus on supervising the influential factors extracted by the variational autoencoder(vae). the vae is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. we argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. we find the vae objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and results in some non-influential factors whose function on data reconstruction could be ignored. we show mutual information also influences the lower bound of vae's reconstruction error and downstream classification task. to make such indicator applicable, we design an algorithm for calculating the mutual information for vae and prove its consistency. experimental results on mnist, celeba and deap datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on deap dataset.",,2018-09-05,2019-04-05,"['shiqi liu', 'jingxin liu', 'qian zhao', 'xiangyong cao', 'huibin li', 'hongying meng', 'sheng liu', 'deyu meng']"
105,1809.05052,a general theory for preferential sampling in environmental networks,stat.me,"this paper presents a general model framework for detecting the preferential sampling of environmental monitors recording an environmental process across space and/or time. this is achieved by considering the joint distribution of an environmental process with a site--selection process that considers where and when sites are placed to measure the process. the environmental process may be spatial, temporal or spatio--temporal in nature. by sharing random effects between the two processes, the joint model is able to establish whether site placement was stochastically dependent of the environmental process under study. the embedding into a spatio--temporal framework also allows for the modelling of the dynamic site---selection process itself. real--world factors affecting both the size and location of the network can be easily modelled and quantified. depending upon the choice of population of locations to consider for selection across space and time under the site--selection process, different insights about the precise nature of preferential sampling can be obtained. the general framework developed in the paper is designed to be easily and quickly fit using the r-inla package. we apply this framework to a case study involving particulate air pollution over the uk where a major reduction in the size of a monitoring network through time occurred. it is demonstrated that a significant response--biased reduction in the air quality monitoring network occurred. we also show that the network was consistently unrepresentative of the levels of particulate matter seen across much of gb throughout the operating life of the network. finally we show that this may have led to a severe over-reporting of the population--average exposure levels experienced across gb. this could have great impacts on estimates of the health effects of black smoke levels.",,2018-09-13,2019-04-06,"['joe watson', 'james v. zidek', 'gavin shaddick']"
106,1809.06581,a probabilistic framework for approximating functions in active   subspaces,math.pr math.na math.st stat.me stat.th,"this paper develops a comprehensive probabilistic setup to compute approximating functions in active subspaces. constantine et al. proposed the active subspace method in (constantine et al., 2014) to reduce the dimension of computational problems. it can be seen as an attempt to approximate a high-dimensional function of interest $f$ by a low-dimensional one. to do this, a common approach is to integrate $f$ over the inactive, i.e. non-dominant, directions with a suitable conditional density function. in practice, this can be done with a finite monte carlo sum, making not only the resulting approximation random in the inactive variable for each fixed input from the active subspace, but also its expectation, i.e. the integral of the low-dimensional function weighted with a probability measure on the active variable. in this regard we develop a fully probabilistic framework extending results from (constantine et al., 2014, 2016). the results are supported by a simple numerical example.",,2018-09-18,2019-04-07,['mario teixeira parente']
107,1809.07861,machine learning for forecasting mid price movement using limit order   book data,cs.ce cs.lg stat.ml,"forecasting the movements of stock prices is one the most challenging problems in financial markets analysis. in this paper, we use machine learning (ml) algorithms for the prediction of future price movements using limit order book data. two different sets of features are combined and evaluated: handcrafted features based on the raw order book data and features extracted by ml algorithms, resulting in feature vectors with highly variant dimensionalities. three classifiers are evaluated using combinations of these sets of features on two different evaluation setups and three prediction scenarios. even though the large scale and high frequency nature of the limit order book poses several challenges, the scope of the conducted experiments and the significance of the experimental results indicate that machine learning highly befits this task carving the path towards future research in this field.",,2018-09-19,2019-04-08,"['paraskevi nousi', 'avraam tsantekidis', 'nikolaos passalis', 'adamantios ntakaris', 'juho kanniainen', 'anastasios tefas', 'moncef gabbouj', 'alexandros iosifidis']"
108,1809.08922,context-aware systems for sequential item recommendation,cs.ir cs.lg stat.ml,"quizlet is the most popular online learning tool in the united states, and is used by over 2/3 of high school students, and 1/2 of college students. with more than 95% of quizlet users reporting improved grades as a result, the platform has become the de-facto tool used in millions of classrooms. in this paper, we explore the task of recommending suitable content for a student to study, given their prior interests, as well as what their peers are studying. we propose a novel approach, i.e. neural educational recommendation engine (nere), to recommend educational content by leveraging student behaviors rather than ratings. we have found that this approach better captures social factors that are more aligned with learning. nere is based on a recurrent neural network that includes collaborative and content-based approaches for recommendation, and takes into account any particular student's speed, mastery, and experience to recommend the appropriate task. we train nere by jointly learning the user embeddings and content embeddings, and attempt to predict the content embedding for the final timestamp. we also develop a confidence estimator for our neural network, which is a crucial requirement for productionizing this model. we apply nere to quizlet's proprietary dataset, and present our results. we achieved an r^2 score of 0.81 in the content embedding space, and a recall score of 54% on our 100 nearest neighbors. this vastly exceeds the recall@100 score of 12% that a standard matrix-factorization approach provides. we conclude with a discussion on how nere will be deployed, and position our work as one of the first educational recommender systems for the k-12 space.",10.1109/icdmw.2018.00056,2018-09-20,2019-04-04,"['moin nadeem', 'dustin stansbury', 'shane mooney']"
109,1809.10243,segmentation of skin lesions and their attributes using multi-scale   convolutional neural networks and domain specific augmentations,cs.cv cs.lg stat.ml,"computer-aided diagnosis systems for classification of different type of skin lesions have been an active field of research in recent decades. it has been shown that introducing lesions and their attributes masks into lesion classification pipeline can greatly improve the performance. in this paper, we propose a framework by incorporating transfer learning for segmenting lesions and their attributes based on the convolutional neural networks. the proposed framework is based on the encoder-decoder architecture which utilizes a variety of pre-trained networks in the encoding path and generates the prediction map by combining multi-scale information in decoding path using a pyramid pooling manner. to address the lack of training data and increase the proposed model generalization, an extensive set of novel domain-specific augmentation routines have been applied to simulate the real variations in dermoscopy images. finally, by performing broad experiments on three different data sets obtained from international skin imaging collaboration archive (isic2016, isic2017, and isic2018 challenges data sets), we show that the proposed method outperforms other state-of-the-art approaches for isic2016 and isic2017 segmentation task and achieved the first rank on the leader-board of isic2018 attribute detection task.",,2018-09-23,2019-03-29,"['mostafa jahanifar', 'neda zamani tajeddin', 'navid alemi koohbanani', 'ali gooya', 'nasir rajpoot']"
110,1809.10462,robust covariance estimation under $l_4-l_2$ norm equivalence,math.st stat.th,"let $x$ be a centered random vector taking values in $\mathbb{r}^d$ and let $\sigma= \mathbb{e}(x\otimes x)$ be its covariance matrix. we show that if $x$ satisfies an $l_4-l_2$ norm equivalence, there is a covariance estimator $\hat{\sigma}$ that exhibits the optimal performance one would expect had $x$ been a gaussian vector. the procedure also improves the current state-of-the-art regarding high probability bounds in the subgaussian case (sharp results were only known in expectation or with constant probability). in both scenarios the new bound does not depend explicitly on the dimension $d$, but rather on the effective rank of the covariance matrix $\sigma$.",,2018-09-27,2019-03-26,"['shahar mendelson', 'nikita zhivotovskiy']"
111,1810.00551,generative adversarial network for medical images (mi-gan),cs.lg cs.cv eess.iv stat.ml,"deep learning algorithms produces state-of-the-art results for different machine learning and computer vision tasks. to perform well on a given task, these algorithms require large dataset for training. however, deep learning algorithms lack generalization and suffer from over-fitting whenever trained on small dataset, especially when one is dealing with medical images. for supervised image analysis in medical imaging, having image data along with their corresponding annotated ground-truths is costly as well as time consuming since annotations of the data is done by medical experts manually. in this paper, we propose a new generative adversarial network for medical imaging (mi-gan). the mi-gan generates synthetic medical images and their segmented masks, which can then be used for the application of supervised analysis of medical images. particularly, we present mi-gan for synthesis of retinal images. the proposed method generates precise segmented images better than the existing techniques. the proposed model achieves a dice coefficient of 0.837 on stare dataset and 0.832 on drive dataset which is state-of-the-art performance on both the datasets.",10.1007/s10916-018-1072-9,2018-10-01,,"['talha iqbal', 'hazrat ali']"
112,1810.01072,a flexible sequential monte carlo algorithm for parametric constrained   regression,stat.me stat.co,"an algorithm is proposed that enables the imposition of shape constraints on regression curves, without requiring the constraints to be written as closed-form expressions, nor assuming the functional form of the loss function. this algorithm is based on sequential monte carlo-simulated annealing and only relies on an indicator function that assesses whether or not the constraints are fulfilled, thus allowing the enforcement of various complex constraints by specifying an appropriate indicator function without altering other parts of the algorithm. the algorithm is illustrated by fitting rational function and b-spline regression models subject to a monotonicity constraint. an implementation of the algorithm using r is freely available on github.",10.1016/j.csda.2019.03.011,2018-10-02,2019-04-01,"['kenyon ng', 'berwin a. turlach', 'kevin murray']"
113,1810.01405,gramme: semi-supervised learning using multi-layered graph attention   models,stat.ml cs.lg,"modern data analysis pipelines are becoming increasingly complex due to the presence of multi-view information sources. while graphs are effective in modeling complex relationships, in many scenarios a single graph is rarely sufficient to succinctly represent all interactions, and hence multi-layered graphs have become popular. though this leads to richer representations, extending solutions from the single-graph case is not straightforward. consequently, there is a strong need for novel solutions to solve classical problems, such as node classification, in the multi-layered case. in this paper, we consider the problem of semi-supervised learning with multi-layered graphs. though deep network embeddings, e.g. deepwalk, are widely adopted for community discovery, we argue that feature learning with random node attributes, using graph neural networks, can be more effective. to this end, we propose to use attention models for effective feature learning, and develop two novel architectures, gramme-sg and gramme-fusion, that exploit the inter-layer dependencies for building multi-layered graph embeddings. using empirical studies on several benchmark datasets, we evaluate the proposed approaches and demonstrate significant performance improvements in comparison to state-of-the-art network embedding strategies. the results also show that using simple random features is an effective choice, even in cases where explicit node attributes are not available.",,2018-10-02,2019-03-27,"['uday shankar shanthamallu', 'jayaraman j. thiagarajan', 'huan song', 'andreas spanias']"
114,1810.01861,inhibited softmax for uncertainty estimation in neural networks,cs.lg stat.ml,we present a new method for uncertainty estimation and out-of-distribution detection in neural networks with softmax output. we extend softmax layer with an additional constant input. the corresponding additional output is able to represent the uncertainty of the network. the proposed method requires neither additional parameters nor multiple forward passes nor input preprocessing nor out-of-distribution datasets. we show that our method performs comparably to more computationally expensive methods and outperforms baselines on our experiments from image recognition and sentiment analysis domains.,,2018-10-03,2019-04-07,"['marcin możejko', 'mateusz susik', 'rafał karczewski']"
115,1810.02358,transfer learning via unsupervised task discovery for visual question   answering,cs.lg cs.cl cs.cv stat.ml,"we study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering task. existing large-scale visual datasets with annotations such as image class labels, bounding boxes and region descriptions are good sources for learning rich and diverse visual concepts. however, it is not straightforward how the visual concepts can be captured and transferred to visual question answering models due to missing link between question dependent answering models and visual data without question. we tackle this problem in two steps: 1) learning a task conditional visual classifier, which is capable of solving diverse question-specific visual recognition tasks, based on unsupervised task discovery and 2) transferring the task conditional visual classifier to visual question answering models. specifically, we employ linguistic knowledge sources such as structured lexical database (e.g. wordnet) and visual descriptions for unsupervised task discovery, and transfer a learned task conditional visual classifier as an answering unit in a visual question answering model. we empirically show that the proposed algorithm generalizes to out-of-vocabulary answers successfully using the knowledge transferred from the visual dataset.",,2018-10-03,2019-04-07,"['hyeonwoo noh', 'taehoon kim', 'jonghwan mun', 'bohyung han']"
116,1810.02998,total variation distance for discretely observed l\'evy processes: a   gaussian approximation of the small jumps,math.st math.pr stat.th,"it is common practice to treat small jumps of l\'evy processes as wiener noise and thus to approximate its marginals by a gaussian distribution. however, results that allow to quantify the goodness of this approximation according to a given metric are rare. in this paper, we clarify what happens when the chosen metric is the total variation distance. such a choice is motivated by its statistical interpretation. if the total variation distance between two statistical models converges to zero, then no tests can be constructed to distinguish the two models which are therefore equivalent, statistically speaking. we elaborate a fine analysis of a gaussian approximation for the small jumps of l\'evy processes with infinite l\'evy measure in total variation distance. non asymptotic bounds for the total variation distance between $n$ discrete observations of small jumps of a l\'evy process and the corresponding gaussian distribution is presented and extensively discussed. as a byproduct, new upper bounds for the total variation distance between discrete observations of l\'evy processes are provided. the theory is illustrated by concrete examples.",,2018-10-06,2019-04-02,"['alexandra carpentier', 'céline duval', 'ester mariucci']"
117,1810.03477,compatible matrices of spearman's rank correlation,math.st stat.ap stat.th,"in this paper, we provide a negative answer to a long-standing open problem on the compatibility of spearman's rho matrices. following an equivalence of spearman's rho matrices and linear correlation matrices for dimensions up to 9 in the literature, we show non-equivalence for dimensions 12 or higher. in particular, we connect this problem with the existence of a random vector under some linear projection restrictions in two characterization results.",,2018-10-08,2019-04-03,"['bin wang', 'ruodu wang', 'yuming wang']"
118,1810.03819,sufficient and necessary conditions for the identifiability of the   $q$-matrix,math.st stat.th,"restricted latent class models (rlcms) have recently gained prominence in educational assessment, psychiatric evaluation, and medical diagnosis. different from conventional latent class models, restrictions on the rlcm model parameters are imposed by a design matrix to respect practitioners' scientific assumptions. the design matrix, called $q$-matrix in the cognitive diagnosis literature, is usually constructed by practitioners and domain experts, yet it is subjective and could be misspecified. to address this problem, researchers have proposed to estimate the $q$-matrix from data. on the other hand, the fundamental learnability issue of the $q$-matrix and model parameters remains underexplored and existing studies often impose stronger than needed or even impractical conditions. this paper proposes sufficient and necessary conditions for joint identifiability of the $q$-matrix and the rlcm model parameters under different types of rlcms. the developed identifiability conditions only depend on the design matrix and are easy to verify in practice.",,2018-10-09,2019-04-08,"['yuqi gu', 'gongjun xu']"
119,1810.04114,discovering general-purpose active learning strategies,cs.lg stat.ml,"we propose a general-purpose approach to discovering active learning (al) strategies from data. these strategies are transferable from one domain to another and can be used in conjunction with many machine learning models. to this end, we formalize the annotation process as a markov decision process, design universal state and action spaces and introduce a new reward function that precisely model the al objective of minimizing the annotation cost. we seek to find an optimal (non-myopic) al strategy using reinforcement learning. we evaluate the learned strategies on multiple unrelated domains and show that they consistently outperform state-of-the-art baselines.",,2018-10-09,2019-04-02,"['ksenia konyushkova', 'raphael sznitman', 'pascal fua']"
120,1810.04617,testing community structures for hypergraphs,math.st stat.th,"many complex networks in real world can be formulated as hypergraphs where community detection has been widely used. however, the fundamental question of whether communities exist or not in an observed hypergraph still remains unresolved. the aim of the present paper is to tackle this important problem. specifically, we study when a hypergraph with community structure can be successfully distinguished from its erd\""{o}s-renyi counterpart, and propose concrete test statistics based on hypergraph cycles when the models are distinguishable. our contributions are summarized as follows. for uniform hypergraphs, we show that successful testing is always impossible when average degree tends to zero, might be possible when average degree is bounded, and is possible when average degree is growing. we obtain asymptotic distributions of the proposed test statistics and analyze their power. our results for growing degree case are further extended to nonuniform hypergraphs in which a new test involving both edge and hyperedge information is proposed. the novel aspect of our new test is that it is provably more powerful than the classic test involving only edge information. simulation and real data analysis support our theoretical findings. the proofs rely on janson's contiguity theory (\cite{j95}) and a high-moments driven asymptotic normality result by gao and wormald (\cite{gwald}).",,2018-10-10,2019-04-04,"['mingao yuan', 'ruiqi liu', 'yang feng', 'zuofeng shang']"
121,1810.05079,efficient estimation of autocorrelation spectra,physics.comp-ph stat.co,"the performance of markov chain monte carlo calculations is determined by both ensemble variance of the monte carlo estimator and autocorrelation of the markov process. in order to study autocorrelation, binning analysis is commonly used, where the autocorrelation is estimated from results grouped into bins of logarithmically increasing sizes.   in this paper, we show that binning analysis comes with a bias that can be eliminated by combining bin sizes. we then show binning analysis can be performed on-the-fly with linear overhead in time and logarithmic overhead in memory with respect to the sample size. we then show that binning analysis contains information not only about the integrated effect of autocorrelation, but can be used to estimate the spectrum of autocorrelation lengths, yielding the height of phase space barriers in the system. finally, we revisit the ising model and apply the proposed method to recover its autocorrelation spectra.",,2018-10-11,2019-04-04,['markus wallerberger']
122,1810.05997,predict then propagate: graph neural networks meet personalized pagerank,cs.lg stat.ml,"neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. however, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. in this paper, we use the relationship between graph convolutional networks (gcn) and pagerank to derive an improved propagation scheme based on personalized pagerank. we utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (ppnp), and its fast approximation, appnp. our model's training time is on par or faster and its number of parameters on par or lower than previous models. it leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. we show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for gcn-like models. our implementation is available online.",,2018-10-14,2019-02-27,"['johannes klicpera', 'aleksandar bojchevski', 'stephan günnemann']"
123,1810.06433,calibration procedures for approximate bayesian credible sets,stat.co stat.me,"we develop and apply two calibration procedures for checking the coverage of approximate bayesian credible sets including intervals estimated using monte carlo methods. the user has an ideal prior and likelihood, but generates a credible set for an approximate posterior which is not proportional to the product of ideal likelihood and prior. we estimate the realised posterior coverage achieved by the approximate credible set. this is the coverage of the unknown ``true'' parameter if the data are a realisation of the user's ideal observation model conditioned on the parameter, and the parameter is a draw from the user's ideal prior. in one approach we estimate the posterior coverage at the data by making a semi-parametric logistic regression of binary coverage outcomes on simulated data against summary statistics evaluated on simulated data. in another we use importance sampling from the approximate posterior, windowing simulated data to fall close to the observed data. we illustrate our methods on four examples.",,2018-10-15,2019-04-08,"['jeong eun lee', 'geoff k. nicholls', 'robin j. ryder']"
124,1810.07288,fast and faster convergence of sgd for over-parameterized models and an   accelerated perceptron,cs.lg stat.ml,"modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely, resulting in zero training loss. for such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. under this condition, we prove that constant step-size stochastic gradient descent (sgd) with nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. we also show that this condition implies that sgd can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. given this weaker condition, we prove that sgd with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. under additional assumptions, the above results enable us to prove an o(1/k^2) mistake bound for k iterations of a stochastic perceptron algorithm using the squared-hinge loss. finally, we validate our theoretical findings with experiments on synthetic and real datasets.",,2018-10-16,2019-04-05,"['sharan vaswani', 'francis bach', 'mark schmidt']"
125,1810.08061,autograph: imperative-style coding with graph-based performance,cs.pl cs.lg stat.ml,"there is a perceived trade-off between machine learning code that is easy to write, and machine learning code that is scalable or fast to execute. in machine learning, imperative style libraries like autograd and pytorch are easy to write, but suffer from high interpretive overhead and are not easily deployable in production or mobile settings. graph-based libraries like tensorflow and theano benefit from whole-program optimization and can be deployed broadly, but make expressing complex models more cumbersome. we describe how the use of staged programming in python, via source code transformation, offers a midpoint between these two library design patterns, capturing the benefits of both. a key insight is to delay all type-dependent decisions until runtime, via dynamic dispatch. we instantiate these principles in autograph, a software system that improves the programming experience of the tensorflow library, and demonstrate usability improvements with no loss in performance compared to native tensorflow graphs. we also show that our system is backend agnostic, and demonstrate targeting an alternate ir with characteristics not found in tensorflow graphs.",,2018-10-16,2019-03-26,"['dan moldovan', 'james m decker', 'fei wang', 'andrew a johnson', 'brian k lee', 'zachary nado', 'd sculley', 'tiark rompf', 'alexander b wiltschko']"
126,1810.08926,teaching inverse reinforcement learners via features and demonstrations,cs.lg stat.ml,"learning near-optimal behaviour from an expert's demonstrations typically relies on the assumption that the learner knows the features that the true reward function depends on. in this paper, we study the problem of learning from demonstrations in the setting where this is not the case, i.e., where there is a mismatch between the worldviews of the learner and the expert. we introduce a natural quantity, the teaching risk, which measures the potential suboptimality of policies that look optimal to the learner in this setting. we show that bounds on the teaching risk guarantee that the learner is able to find a near-optimal policy using standard algorithms based on inverse reinforcement learning. based on these findings, we suggest a teaching scheme in which the expert can decrease the teaching risk by updating the learner's worldview, and thus ultimately enable her to find a near-optimal policy.",,2018-10-21,2019-03-27,"['luis haug', 'sebastian tschiatschek', 'adish singla']"
127,1810.09945,analyzing neuroimaging data through recurrent deep learning models,cs.lg cs.cv cs.ne q-bio.nc stat.ml,"the application of deep learning (dl) models to neuroimaging data poses several challenges, due to the high dimensionality, low sample size and complex temporo-spatial dependency structure of these datasets. even further, dl models act as as black-box models, impeding insight into the association of cognitive state and brain activity. to approach these challenges, we introduce the deeplight framework, which utilizes long short-term memory (lstm) based dl models to analyze whole-brain functional magnetic resonance imaging (fmri) data. to decode a cognitive state (e.g., seeing the image of a house), deeplight separates the fmri volume into a sequence of axial brain slices, which is then sequentially processed by an lstm. to maintain interpretability, deeplight adapts the layer-wise relevance propagation (lrp) technique. thereby, decomposing its decoding decision into the contributions of the single input voxels to this decision. importantly, the decomposition is performed on the level of single fmri volumes, enabling deeplight to study the associations between cognitive state and brain activity on several levels of data granularity, from the level of the group down to the level of single time points. to demonstrate the versatility of deeplight, we apply it to a large fmri dataset of the human connectome project. we show that deeplight outperforms conventional approaches of uni- and multivariate fmri analysis in decoding the cognitive states and in identifying the physiologically appropriate brain regions associated with these states. we further demonstrate deeplight's ability to study the fine-grained temporo-spatial variability of brain activity over sequences of single fmri samples.",,2018-10-23,2019-04-05,"['armin w. thomas', 'hauke r. heekeren', 'klaus-robert müller', 'wojciech samek']"
128,1810.10132,smoothed online optimization for regression and control,cs.lg cs.ds math.oc stat.ml,"we consider online convex optimization (oco) in the setting where the costs are $m$-strongly convex and the online learner pays a switching cost for changing decisions between rounds. we show that the recently proposed online balanced descent (obd) algorithm is constant competitive in this setting, with competitive ratio $3 + o(1/m)$, irrespective of the ambient dimension. additionally, we show that when the sequence of cost functions is $\epsilon$-smooth, obd has near-optimal dynamic regret and maintains strong per-round accuracy. we demonstrate the generality of our approach by showing that the obd framework can be used to construct competitive algorithms for a variety of online problems across learning and control, including online variants of ridge regression, logistic regression, maximum likelihood estimation, and lqr control.",,2018-10-23,2019-04-04,"['gautam goel', 'adam wierman']"
129,1810.10581,visual rendering of shapes on 2d display devices guided by hand gestures,cs.hc cs.cv cs.lg stat.ml,"designing of touchless user interface is gaining popularity in various contexts. using such interfaces, users can interact with electronic devices even when the hands are dirty or non-conductive. also, user with partial physical disability can interact with electronic devices using such systems. research in this direction has got major boost because of the emergence of low-cost sensors such as leap motion, kinect or realsense devices. in this paper, we propose a leap motion controller-based methodology to facilitate rendering of 2d and 3d shapes on display devices. the proposed method tracks finger movements while users perform natural gestures within the field of view of the sensor. in the next phase, trajectories are analyzed to extract extended npen++ features in 3d. these features represent finger movements during the gestures and they are fed to unidirectional left-to-right hidden markov model (hmm) for training. a one-to-one mapping between gestures and shapes is proposed. finally, shapes corresponding to these gestures are rendered over the display using mupad interface. we have created a dataset of 5400 samples recorded by 10 volunteers. our dataset contains 18 geometric and 18 non-geometric shapes such as ""circle"", ""rectangle"", ""flower"", ""cone"", ""sphere"" etc. the proposed methodology achieves an accuracy of 92.87% when evaluated using 5-fold cross validation method. our experiments revel that the extended 3d features perform better than existing 3d features in the context of shape representation and classification. the method can be used for developing useful hci applications for smart display devices.",10.1016/j.displa.2019.03.001,2018-10-22,,"['abhik singla', 'partha pratim roy', 'debi prosad dogra']"
130,1810.10667,truncated back-propagation for bilevel optimization,cs.lg stat.ml,"bilevel optimization has been recently revisited for designing and analyzing algorithms in hyperparameter tuning and meta learning tasks. however, due to its nested structure, evaluating exact gradients for high-dimensional problems is computationally challenging. one heuristic to circumvent this difficulty is to use the approximate gradient given by performing truncated back-propagation through the iterative optimization procedure that solves the lower-level problem. although promising empirical performance has been reported, its theoretical properties are still unclear. in this paper, we analyze the properties of this family of approximate gradients and establish sufficient conditions for convergence. we validate this on several hyperparameter tuning and meta learning tasks. we find that optimization with the approximate gradient computed using few-step back-propagation often performs comparably to optimization with the exact gradient, while requiring far less memory and half the computation time.",,2018-10-24,2019-04-05,"['amirreza shaban', 'ching-an cheng', 'nathan hatch', 'byron boots']"
131,1810.10987,nuclear norm regularized estimation of panel regression models,econ.em stat.ml,"in this paper we investigate panel regression models with interactive fixed effects. we propose two new estimation methods that are based on minimizing convex objective functions. the first method minimizes the sum of squared residuals with a nuclear (trace) norm regularization. the second method minimizes the nuclear norm of the residuals. we establish the consistency of the two resulting estimators. those estimators have a very important computational advantage compared to the existing least squares (ls) estimator, in that they are defined as minimizers of a convex objective function. in addition, the nuclear norm penalization helps to resolve a potential identification problem for interactive fixed effect models, in particular when the regressors are low-rank and the number of the factors is unknown. we also show how to construct estimators that are asymptotically equivalent to the least squares (ls) estimator in bai (2009) and moon and weidner (2017) by using our nuclear norm regularized or minimized estimators as initial values for a finite number of ls minimizing iteration steps. this iteration avoids any non-convex minimization, while the original ls estimation problem is generally non-convex, and can have multiple local minima.",,2018-10-25,2019-03-28,"['hyungsik roger moon', 'martin weidner']"
132,1810.11829,on preserving non-discrimination when combining expert advice,cs.lg cs.ds stat.ml,"we study the interplay between sequential decision making and avoiding discrimination against protected groups, when examples arrive online and do not follow distributional assumptions. we consider the most basic extension of classical online learning: ""given a class of predictors that are individually non-discriminatory with respect to a particular metric, how can we combine them to perform as well as the best predictor, while preserving non-discrimination?"" surprisingly we show that this task is unachievable for the prevalent notion of ""equalized odds"" that requires equal false negative rates and equal false positive rates across groups. on the positive side, for another notion of non-discrimination, ""equalized error rates"", we show that running separate instances of the classical multiplicative weights algorithm for each group achieves this guarantee. interestingly, even for this notion, we show that algorithms with stronger performance guarantees than multiplicative weights cannot preserve non-discrimination.",,2018-10-28,2019-03-29,"['avrim blum', 'suriya gunasekar', 'thodoris lykouris', 'nathan srebro']"
133,1810.11859,consistency of elbo maximization for model selection,math.st stat.th,"the evidence lower bound (elbo) is a quantity that plays a key role in variational inference. it can also be used as a criterion in model selection. however, though extremely popular in practice in the variational bayes community, there has never been a general theoretic justification for selecting based on the elbo. in this paper, we show that the elbo maximization strategy has strong theoretical guarantees, and is robust to model misspecification while most works rely on the assumption that one model is correctly specified. we illustrate our theoretical results by an application to the selection of the number of principal components in probabilistic pca.",,2018-10-28,2019-04-08,['badr-eddine chérief-abdellatif']
134,1810.12683,pseudo-bayesian learning with kernel fourier transform as prior,stat.ml cs.lg,"we revisit rahimi and recht (2007)'s kernel random fourier features (rff) method through the lens of the pac-bayesian theory. while the primary goal of rff is to approximate a kernel, we look at the fourier transform as a prior distribution over trigonometric hypotheses. it naturally suggests learning a posterior on these hypotheses. we derive generalization bounds that are optimized by learning a pseudo-posterior obtained from a closed-form expression. based on this study, we consider two learning strategies: the first one finds a compact landmarks-based representation of the data where each landmark is given by a distribution-tailored similarity measure, while the second one provides a pac-bayesian justification to the kernel alignment method of sinha and duchi (2016).",,2018-10-30,2019-03-27,"['gaël letarte', 'emilie morvant', 'pascal germain']"
135,1811.00159,clustered monotone transforms for rating factorization,cs.ir cs.lg stat.ml,"exploiting low-rank structure of the user-item rating matrix has been the crux of many recommendation engines. however, existing recommendation engines force raters with heterogeneous behavior profiles to map their intrinsic rating scales to a common rating scale (e.g. 1-5). this non-linear transformation of the rating scale shatters the low-rank structure of the rating matrix, therefore resulting in a poor fit and consequentially, poor recommendations. in this paper, we propose clustered monotone transforms for rating factorization (cmtrf), a novel approach to perform regression up to unknown monotonic transforms over unknown population segments. essentially, for recommendation systems, the technique searches for monotonic transformations of the rating scales resulting in a better fit. this is combined with an underlying matrix factorization regression model that couples the user-wise ratings to exploit shared low dimensional structure. the rating scale transformations can be generated for each user, for a cluster of users, or for all the users at once, forming the basis of three simple and efficient algorithms proposed in this paper, all of which alternate between transformation of the rating scales and matrix factorization regression. despite the non-convexity, cmtrf is theoretically shown to recover a unique solution under mild conditions. experimental results on two synthetic and seven real-world datasets show that cmtrf outperforms other state-of-the-art baselines.",10.1145/3289600.3291005,2018-10-31,,"['gaurush hiranandani', 'raghav somani', 'oluwasanmi koyejo', 'sreangsu acharyya']"
136,1811.00890,"probabilistic programming with densities in slicstan: efficient,   flexible and deterministic",cs.pl stat.co stat.ml,"stan is a probabilistic programming language that has been increasingly used for real-world scalable projects. however, to make practical inference possible, the language sacrifices some of its usability by adopting a block syntax, which lacks compositionality and flexible user-defined functions. moreover, the semantics of the language has been mainly given in terms of intuition about implementation, and has not been formalised.   this paper provides a formal treatment of the stan language, and introduces the probabilistic programming language slicstan --- a compositional, self-optimising version of stan. our main contributions are: (1) the formalisation of a core subset of stan through an operational density-based semantics; (2) the design and semantics of the stan-like language slicstan, which facilities better code reuse and abstraction through its compositional syntax, more flexible functions, and information-flow type system; and (3) a formal, semantic-preserving procedure for translating slicstan to stan.",10.1145/3290348,2018-11-02,,"['maria i. gorinova', 'andrew d. gordon', 'charles sutton']"
137,1811.01315,modeling stated preference for mobility-on-demand transit: a comparison   of machine learning and logit models,cs.lg cs.ai stat.ap stat.ml,"logit models are usually applied when studying individual travel behavior, i.e., to predict travel mode choice and to gain behavioral insights on traveler preferences. recently, some studies have applied machine learning to model travel mode choice and reported higher out-of-sample predictive accuracy than traditional logit models (e.g., multinomial logit). however, little research focuses on comparing the interpretability of machine learning with logit models. in other words, how to draw behavioral insights from the high-performance ""black-box"" machine-learning models remains largely unsolved in the field of travel behavior modeling.   this paper aims at providing a comprehensive comparison between the two approaches by examining the key similarities and differences in model development, evaluation, and behavioral interpretation between logit and machine-learning models for travel mode choice modeling. to complement the theoretical discussions, the paper also empirically evaluates the two approaches on the stated-preference survey data for a new type of transit system integrating high-frequency fixed-route services and ridesourcing. the results show that machine learning can produce significantly higher predictive accuracy than logit models. moreover, machine learning and logit models largely agree on many aspects of behavioral interpretations. in addition, machine learning can automatically capture the nonlinear relationship between the input features and choice outcomes. the paper concludes that there is great potential in merging ideas from machine learning and conventional statistical methods to develop refined models for travel behavior research and suggests some new research directions.",,2018-11-03,2019-04-01,"['xilei zhao', 'xiang yan', 'alan yu', 'pascal van hentenryck']"
138,1811.01463,security for machine learning-based systems: attacks and challenges   during training and inference,cs.lg cs.cr stat.ml,"the exponential increase in dependencies between the cyber and physical world leads to an enormous amount of data which must be efficiently processed and stored. therefore, computing paradigms are evolving towards machine learning (ml)-based systems because of their ability to efficiently and accurately process the enormous amount of data. although ml-based solutions address the efficient computing requirements of big data, they introduce (new) security vulnerabilities into the systems, which cannot be addressed by traditional monitoring-based security measures. therefore, this paper first presents a brief overview of various security threats in machine learning, their respective threat models and associated research challenges to develop robust security measures. to illustrate the security vulnerabilities of ml during training, inferencing and hardware implementation, we demonstrate some key security threats on ml using lenet and vggnet for mnist and german traffic sign recognition benchmarks (gtsrb), respectively. moreover, based on the security analysis of ml-training, we also propose an attack that has a very less impact on the inference accuracy. towards the end, we highlight the associated research challenges in developing security measures and provide a brief overview of the techniques used to mitigate such security threats.",10.1109/fit.2018.00064,2018-11-04,,"['faiq khalid', 'muhammad abdullah hanif', 'semeen rehman', 'muhammad shafique']"
139,1811.02833,causaltoolbox---estimator stability for heterogeneous treatment effects,stat.me,"estimating heterogeneous treatment effects has become increasingly important in many fields and life and death decisions are now based on these estimates: for example, selecting a personalized course of medical treatment. recently, a variety of procedures relying on different assumptions have been suggested for estimating heterogeneous treatment effects. unfortunately, there are no compelling approaches that allow identification of the procedure that has assumptions that hew closest to the process generating the data set under study and researchers often select one arbitrarily. this approach risks making inferences that rely on incorrect assumptions and gives the experimenter too much scope for $p$-hacking. a single estimator will also tend to overlook patterns other estimators could have picked up. we believe that the conclusion of many published papers might change had a different estimator been chosen and we suggest that practitioners should evaluate many estimators and assess their similarity when investigating heterogeneous treatment effects. we demonstrate this by applying 28 different estimation procedures to an emulated observational data set; this analysis shows that different estimation procedures may give starkly different estimates. we also provide an extensible \texttt{r} package which makes it straightforward for practitioners to follow our recommendations.",,2018-11-07,2019-03-28,"['sören r. künzel', 'simon j. s. walter', 'jasjeet s. sekhon']"
140,1811.03433,explainable cardiac pathology classification on cine mri with motion   characterization by semi-supervised learning of apparent flow,cs.cv cs.ai cs.lg stat.ml,"we propose a method to classify cardiac pathology based on a novel approach to extract image derived features to characterize the shape and motion of the heart. an original semi-supervised learning procedure, which makes efficient use of a large amount of non-segmented images and a small amount of images segmented manually by experts, is developed to generate pixel-wise apparent flow between two time points of a 2d+t cine mri image sequence. combining the apparent flow maps and cardiac segmentation masks, we obtain a local apparent flow corresponding to the 2d motion of myocardium and ventricular cavities. this leads to the generation of time series of the radius and thickness of myocardial segments to represent cardiac motion. these time series of motion features are reliable and explainable characteristics of pathological cardiac motion. furthermore, they are combined with shape-related features to classify cardiac pathologies. using only nine feature values as input, we propose an explainable, simple and flexible model for pathology classification. on acdc training set and testing set, the model achieves 95% and 94% respectively as classification accuracy. its performance is hence comparable to that of the state-of-the-art. comparison with various other models is performed to outline some advantages of our model.",,2018-11-08,2019-03-27,"['qiao zheng', 'hervé delingette', 'nicholas ayache']"
141,1811.03568,a geometric approach of gradient descent algorithms in neural networks,cs.lg stat.ml,"in this paper, we present an original geometric framework to analyze the convergence properties of gradient descent trajectories in the context of linear neural networks. built upon a key invariance property induced by the network structure, we propose a conjecture called \emph{overfitting conjecture} stating that, for almost every training data, the corresponding gradient descent trajectory converges to a global minimum, for almost every initial condition. this would imply that, for linear neural networks of an arbitrary number of hidden layers, the solution achieved by simple gradient descent algorithm is equivalent to that of least square estimation. our first result consists in establishing, in the case of linear networks of arbitrary depth, convergence of gradient descent trajectories to critical points of the loss function. our second result is the proof of the \emph{overfitting conjecture} in the case of single-hidden-layer linear networks with an argument based on the notion of normal hyperbolicity and under a generic property on the training data (i.e., holding for almost every training data).",,2018-11-08,2019-04-02,"['yacine chitour', 'zhenyu liao', 'romain couillet']"
142,1811.04319,playing by the book: an interactive game approach for action graph   extraction from text,cs.lg cs.cl stat.ml,"understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. we focus on the challenging real-world problem of action-graph extraction from material science papers, where language is highly specialized and data annotation is expensive and scarce. we propose a novel approach, text2quest, where procedural text is interpreted as instructions for an interactive game. a learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. the framework can complement existing approaches and enables richer forms of learning compared to static texts. we discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction.",,2018-11-10,2019-04-06,"['ronen tamari', 'hiroyuki shindo', 'dafna shahaf', 'yuji matsumoto']"
143,1811.05560,a fresh look at ignorability for likelihood inference,stat.me,"when data are incomplete, a random vector y for the data process together with a binary random vector r for the process that causes missing data, are modelled jointly. we review conditions under which r can be ignored for drawing likelihood inferences about the distribution for y. the standard approach of rubin (1976) and seaman et. al. (2013) statist. sci., 28:2 pp. 257--268 emulates complete-data methods exactly, and directs an investigator to choose a full model in which missing at random (mar) and distinct of parameters holds if the goal is not to use a full model. another interpretation of ignorability lurking in the literature considers ignorable likelihood estimation independently of any model for the conditional distribution r given y. we discuss shortcomings of the standard approach, and argue that the alternative gives the `right' conditions for ignorability because it treats the problem on its merits, rather than emulating methodology developed for when the investigator is in possession of all of the data.",,2018-11-13,2019-03-28,['john c galati']
144,1811.06773,a novel approach to sparse inverse covariance estimation using transform   domain updates and exponentially adaptive thresholding,cs.lg cs.ir stat.ml,"sparse inverse covariance estimation (sice) is useful in many practical data analyses. recovering the connectivity, non-connectivity graph of covariates is classified amongst the most important data mining and learning problems. in this paper, we introduce a novel sice approach using adaptive thresholding. our method is based on updates in a transformed domain of the desired matrix and exponentially decaying adaptive thresholding in the main domain (inverse covariance matrix domain). in addition to the proposed algorithm, the convergence analysis is also provided. in the numerical experiments section, we show that the proposed method outperforms state-of-the-art methods in terms of accuracy.",,2018-11-16,2019-04-03,"['ashkan esmaeili', 'farokh marvasti']"
145,1811.06980,batch self organizing maps for distributional data using adaptive   distances,stat.ot,"the paper deals with a batch self organizing map algorithm (dbsom) for data described by distributional-valued variables. this kind of variables is characterized to take as values one-dimensional probability or frequency distributions on a numeric support. the objective function optimized in the algorithm depends on the choice of the distance measure. according to the nature of the date, the $l_2$ wasserstein distance is proposed as one of the most suitable metrics to compare distributions. it is widely used in several contexts of analysis of distributional data. conventional batch som algorithms consider that all variables are equally important for the training of the som. however, it is well known that some variables are less relevant than others for this task. in order to take into account the different contribution of the variables we propose an adaptive version of the dbsom algorithm that tackles this problem with an additional step: a relevance weight is automatically learned for each distributional-valued variable. moreover, since the $l_2$ wasserstein distance allows a decomposition into two components: one related to the means and one related to the size and shape of the distributions, also relevance weights are automatically learned for each of the measurement components to emphasize the importance of the different estimated parameters of the distributions. examples of real and synthetic datasets of distributional data illustrate the usefulness of the proposed dbsom algorithms.",,2018-11-17,2019-03-29,"['antonio irpino', 'francisco de carvalho', 'rosanna verde', 'antonio balzanella']"
146,1811.07533,variational bayesian dropout with a hierarchical prior,cs.lg stat.ml,"variational dropout (vd) is a generalization of gaussian dropout, which aims at inferring the posterior of network weights based on a log-uniform prior on them to learn these weights as well as dropout rate simultaneously. the log-uniform prior not only interprets the regularization capacity of gaussian dropout in network training, but also underpins the inference of such posterior. however, the log-uniform prior is an improper prior (i.e., its integral is infinite) which causes the inference of posterior to be ill-posed, thus restricting the regularization performance of vd. to address this problem, we present a new generalization of gaussian dropout, termed variational bayesian dropout (vbd), which turns to exploit a hierarchical prior on the network weights and infer a new joint posterior. specifically, we implement the hierarchical prior as a zero-mean gaussian distribution with variance sampled from a uniform hyper-prior. then, we incorporate such a prior into inferring the joint posterior over network weights and the variance in the hierarchical prior, with which both the network training and the dropout rate estimation can be cast into a joint optimization problem. more importantly, the hierarchical prior is a proper prior which enables the inference of posterior to be well-posed. in addition, we further show that the proposed vbd can be seamlessly applied to network compression. experiments on both classification and network compression tasks demonstrate the superior performance of the proposed vbd in terms of regularizing network training.",,2018-11-19,2019-04-04,"['yuhang liu', 'wenyong dong', 'lei zhang', 'dong gong', 'qinfeng shi']"
147,1811.11148,the structure of optimal private tests for simple hypotheses,cs.ds cs.cr cs.it cs.lg math.it stat.ml,"hypothesis testing plays a central role in statistical inference, and is used in many settings where privacy concerns are paramount. this work answers a basic question about privately testing simple hypotheses: given two distributions $p$ and $q$, and a privacy level $\varepsilon$, how many i.i.d. samples are needed to distinguish $p$ from $q$ subject to $\varepsilon$-differential privacy, and what sort of tests have optimal sample complexity? specifically, we characterize this sample complexity up to constant factors in terms of the structure of $p$ and $q$ and the privacy level $\varepsilon$, and show that this sample complexity is achieved by a certain randomized and clamped variant of the log-likelihood ratio test. our result is an analogue of the classical neyman-pearson lemma in the setting of private hypothesis testing. we also give an application of our result to the private change-point detection. our characterization applies more generally to hypothesis tests satisfying essentially any notion of algorithmic stability, which is known to imply strong generalization bounds in adaptive data analysis, and thus our results have applications even when privacy is not a primary concern.",,2018-11-27,2019-04-02,"['clément l. canonne', 'gautam kamath', 'audra mcmillan', 'adam smith', 'jonathan ullman']"
148,1811.11788,formulating camera-adaptive color constancy as a few-shot meta-learning   problem,cs.cv stat.ml,"digital camera pipelines employ color constancy methods to estimate an unknown scene illuminant, in order to re-illuminate images as if they were acquired under an achromatic light source. fully-supervised learning approaches exhibit state-of-the-art estimation accuracy with camera-specific labelled training imagery. resulting models typically suffer from domain gaps and fail to generalise across imaging devices. in this work, we propose a new approach that affords fast adaptation to previously unseen cameras, and robustness to changes in capture device by leveraging annotated samples across different cameras and datasets. we present a general approach that utilizes the concept of color temperature to frame color constancy as a set of distinct, homogeneous few-shot regression tasks, each associated with an intuitive physical meaning. we integrate this novel formulation within a meta-learning framework, enabling fast generalisation to previously unseen cameras using only handfuls of camera specific training samples. consequently, the time spent for data collection and annotation substantially diminishes in practice whenever a new sensor is used. to quantify this gain, we evaluate our pipeline on three publicly available datasets comprising 12 different cameras and diverse scene content. our approach delivers competitive results both qualitatively and quantitatively while requiring a small fraction of the camera-specific samples compared to standard approaches.",,2018-11-28,2019-04-03,"['steven mcdonagh', 'sarah parisot', 'fengwei zhou', 'xing zhang', 'ales leonardis', 'zhenguo li', 'gregory slabaugh']"
149,1811.12019,large-scale distributed second-order optimization using   kronecker-factored approximate curvature for deep convolutional neural   networks,cs.lg cs.cv stat.ml,"large-scale distributed training of deep neural networks suffer from the generalization gap caused by the increase in the effective mini-batch size. previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. we propose an alternative approach using a second-order optimization method that shows similar generalization capability to first-order methods, but converges faster and can handle larger mini-batches. to test our method on a benchmark where highly optimized first-order methods are available as references, we train resnet-50 on imagenet. we converged to 75% top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75% even with a mini-batch size of 131,072, which took only 978 iterations.",,2018-11-29,2019-03-30,"['kazuki osawa', 'yohei tsuji', 'yuichiro ueno', 'akira naruse', 'rio yokota', 'satoshi matsuoka']"
150,1811.12495,on implicit filter level sparsity in convolutional neural networks,cs.lg cs.cv eess.sp stat.ml,"we investigate filter level sparsity that emerges in convolutional neural networks (cnns) which employ batch normalization and relu activation, and are trained with adaptive gradient descent techniques and l2 regularization or weight decay. we conduct an extensive experimental study casting our initial findings into hypotheses and conclusions about the mechanisms underlying the emergent filter level sparsity. this study allows new insight into the performance gap obeserved between adapative and non-adaptive gradient descent methods in practice. further, analysis of the effect of training strategies and hyperparameters on the sparsity leads to practical suggestions in designing cnn training strategies enabling us to explore the tradeoffs between feature selectivity, network capacity, and generalization performance. lastly, we show that the implicit sparsity can be harnessed for neural network speedup at par or better than explicit sparsification / pruning approaches, with no modifications to the typical training pipeline required.",,2018-11-29,2019-04-05,"['dushyant mehta', 'kwang in kim', 'christian theobalt']"
151,1811.12929,online abstraction with mdp homomorphisms for deep learning,cs.lg stat.ml,"abstraction of markov decision processes is a useful tool for solving complex problems, as it can ignore unimportant aspects of an environment, simplifying the process of learning an optimal policy. in this paper, we propose a new algorithm for finding abstract mdps in environments with continuous state spaces. it is based on mdp homomorphisms, a structure-preserving mapping between mdps. we demonstrate our algorithm's ability to learn abstractions from collected experience and show how to reuse the abstractions to guide exploration in new tasks the agent encounters. our novel task transfer method outperforms baselines based on a deep q-network in the majority of our experiments. the source code is at https://github.com/ondrejba/aamas_19.",,2018-11-30,2019-04-03,"['ondrej biza', 'robert platt']"
152,1812.00151,discrete adversarial attacks and submodular optimization with   applications to text classification,cs.lg cs.cr math.oc stat.ml,"adversarial examples are carefully constructed modifications to an input that completely change the output of a classifier but are imperceptible to humans. despite these successful attacks for continuous data (such as image and audio samples), generating adversarial examples for discrete structures such as text has proven significantly more challenging. in this paper we formulate the attacks with discrete input on a set function as an optimization task. we prove that this set function is submodular for some popular neural network text classifiers under simplifying assumption. this finding guarantees a $1-1/e$ approximation factor for attacks that use the greedy algorithm. meanwhile, we show how to use the gradient of the attacked classifier to guide the greedy search. empirical studies with our proposed optimization scheme show significantly improved attack ability and efficiency, on three different text classification tasks over various baselines. we also use a joint sentence and word paraphrasing technique to maintain the original semantics and syntax of the text. this is validated by a human subject evaluation in subjective metrics on the quality and semantic coherence of our generated adversarial text.",,2018-12-01,2019-04-04,"['qi lei', 'lingfei wu', 'pin-yu chen', 'alexandros g. dimakis', 'inderjit s. dhillon', 'michael witbrock']"
153,1812.00353,accelerate cnn via recursive bayesian pruning,cs.lg stat.ml,"channel pruning, widely used for accelerating convolutional neural networks, is an np-hard problem due to the inter-layer dependency of channel redundancy. existing methods generally ignored the above dependency for computation simplicity. to solve the problem, under the bayesian framework, we here propose a layer-wise recursive bayesian pruning method (rbp). a new dropout-based measurement of redundancy, which facilitate the computation of posterior assuming inter-layer dependency, is introduced. specifically, we model the noise across layers as a markov chain and target its posterior to reflect the inter-layer dependency. considering the closed form solution for posterior is intractable, we derive a sparsity-inducing dirac-like prior which regularizes the distribution of the designed noise to automatically approximate the posterior. compared with the existing methods, no additional overhead is required when the inter-layer dependency assumed. the redundant channels can be simply identified by tiny dropout noise and directly pruned layer by layer. experiments on popular cnn architectures have shown that the proposed method outperforms several state-of-the-arts. particularly, we achieve up to $\bf{5.0\times}$ and $\bf{2.2\times}$ flops reduction with little accuracy loss on the large scale dataset ilsvrc2012 for vgg16 and resnet50, respectively.",,2018-12-02,2019-03-27,"['yuefu zhou', 'ya zhang', 'yanfeng wang', 'qi tian']"
154,1812.01717,towards accurate generative models of video: a new metric & challenges,cs.cv cs.ai cs.lg cs.ne stat.ml,"recent advances in deep generative models have lead to remarkable progress in synthesizing high quality images. following their successful application in image processing and representation learning, an important next step is to consider videos. learning generative models of video is a much harder task, requiring a model to capture the temporal dynamics of a scene, in addition to the visual presentation of objects. while recent attempts at formulating generative models of video have had some success, current progress is hampered by (1) the lack of qualitative metrics that consider visual quality, temporal coherence, and diversity of samples, and (2) the wide gap between purely synthetic video data sets and challenging real-world data sets in terms of complexity. to this extent we propose fr\'{e}chet video distance (fvd), a new metric for generative models of video, and starcraft 2 videos (scv), a benchmark of game play from custom starcraft 2 scenarios that challenge the current capabilities of generative models of video. we contribute a large-scale human study, which confirms that fvd correlates well with qualitative human judgment of generated videos, and provide initial benchmark results on scv.",,2018-12-02,2019-03-27,"['thomas unterthiner', 'sjoerd van steenkiste', 'karol kurach', 'raphael marinier', 'marcin michalski', 'sylvain gelly']"
155,1812.01736,machine learning of coarse-grained molecular dynamics force fields,physics.comp-ph cs.lg stat.ml,"atomistic or ab-initio molecular dynamics simulations are widely used to predict thermodynamics and kinetics and relate them to molecular structure. a common approach to go beyond the time- and length-scales accessible with such computationally expensive simulations is the definition of coarse-grained molecular models. existing coarse-graining approaches define an effective interaction potential to match defined properties of high-resolution models or experimental data. in this paper, we reformulate coarse-graining as a supervised machine learning problem. we use statistical learning theory to decompose the coarse-graining error and cross-validation to select and compare the performance of different models. we introduce cgnets, a deep learning approach, that learns coarse-grained free energy functions and can be trained by a force matching scheme. cgnets maintain all physically relevant invariances and allow one to incorporate prior physics knowledge to avoid sampling of unphysical structures. we show that cgnets can capture all-atom explicit-solvent free energy surfaces with models using only a few coarse-grained beads and no solvent, while classical coarse-graining methods fail to capture crucial features of the free energy surface. thus, cgnets are able to capture multi-body terms that emerge from the dimensionality reduction.",,2018-12-04,2019-04-03,"['jiang wang', 'simon olsson', 'christoph wehmeyer', 'adria perez', 'nicholas e. charron', 'gianni de fabritiis', 'frank noe', 'cecilia clementi']"
156,1812.01739,benchmarking keyword spotting efficiency on neuromorphic hardware,cs.lg stat.ml,"using intel's loihi neuromorphic research chip and abr's nengo deep learning toolkit, we analyze the inference speed, dynamic power consumption, and energy cost per inference of a two-layer neural network keyword spotter trained to recognize a single phrase. we perform comparative analyses of this keyword spotter running on more conventional hardware devices including a cpu, a gpu, nvidia's jetson tx1, and the movidius neural compute stick. our results indicate that for this inference application, loihi outperforms all of these alternatives on an energy cost per inference basis while maintaining equivalent inference accuracy. furthermore, an analysis of tradeoffs between network size, inference speed, and energy cost indicates that loihi's comparative advantage over other low-power computing devices improves for larger networks.",,2018-12-04,2019-04-02,"['peter blouw', 'xuan choo', 'eric hunsberger', 'chris eliasmith']"
157,1812.01803,ecc: platform-independent energy-constrained deep neural network   compression via a bilinear regression model,cs.lg stat.ml,"many dnn-enabled vision applications constantly operate under severe energy constraints such as unmanned aerial vehicles, augmented reality headsets, and smartphones. designing dnns that can meet a stringent energy budget is becoming increasingly important. this paper proposes ecc, a framework that compresses dnns to meet a given energy constraint while minimizing accuracy loss. the key idea of ecc is to model the dnn energy consumption via a novel bilinear regression function. the energy estimate model allows us to formulate dnn compression as a constrained optimization that minimizes the dnn loss function over the energy constraint. the optimization problem, however, has nontrivial constraints. therefore, existing deep learning solvers do not apply directly. we propose an optimization algorithm that combines the essence of the alternating direction method of multipliers (admm) framework with gradient-based learning algorithms. the algorithm decomposes the original constrained optimization into several subproblems that are solved iteratively and efficiently. ecc is also portable across different hardware platforms without requiring hardware knowledge. experiments show that ecc achieves higher accuracy under the same or lower energy budget compared to state-of-the-art resource-constrained dnn compression techniques.",,2018-12-04,2019-04-06,"['haichuan yang', 'yuhao zhu', 'ji liu']"
158,1812.02222,predicting pregnancy using large-scale data from a women's health   tracking mobile application,stat.ap cs.cy cs.lg,"predicting pregnancy has been a fundamental problem in women's health for more than 50 years. previous datasets have been collected via carefully curated medical studies, but the recent growth of women's health tracking mobile apps offers potential for reaching a much broader population. however, the feasibility of predicting pregnancy from mobile health tracking data is unclear. here we develop four models -- a logistic regression model, and 3 lstm models -- to predict a woman's probability of becoming pregnant using data from a women's health tracking app, clue by biowink gmbh. evaluating our models on a dataset of 79 million logs from 65,276 women with ground truth pregnancy test data, we show that our predicted pregnancy probabilities meaningfully stratify women: women in the top 10% of predicted probabilities have a 89% chance of becoming pregnant over 6 menstrual cycles, as compared to a 27% chance for women in the bottom 10%. we develop a technique for extracting interpretable time trends from our deep learning models, and show these trends are consistent with previous fertility research. our findings illustrate the potential that women's health tracking data offers for predicting pregnancy on a broader population; we conclude by discussing the steps needed to fulfill this potential.",,2018-12-05,2019-03-27,"['bo liu', 'shuyang shi', 'yongshang wu', 'daniel thomas', 'laura symul', 'emma pierson', 'jure leskovec']"
159,1812.04155,vision-based navigation with language-based assistance via imitation   learning with indirect intervention,cs.lg cs.cl cs.cv cs.ro stat.ml,"we present vision-based navigation with language-based assistance (vnla), a grounded vision-language task where an agent with visual perception is guided via language to find objects in photorealistic indoor environments. the task emulates a real-world scenario in that (a) the requester may not know how to navigate to the target objects and thus makes requests by only specifying high-level end-goals, and (b) the agent is capable of sensing when it is lost and querying an advisor, who is more qualified at the task, to obtain language subgoals to make progress. to model language-based assistance, we develop a general framework termed imitation learning with indirect intervention (i3l), and propose a solution that is effective on the vnla task. empirical results show that this approach significantly improves the success rate of the learning agent over other baselines in both seen and unseen environments. our code and data are publicly available at https://github.com/debadeepta/vnla .",,2018-12-10,2019-04-05,"['khanh nguyen', 'debadeepta dey', 'chris brockett', 'bill dolan']"
160,1812.04795,divergence measures estimation and its asymptotic normality theory in   the discrete case,math.st stat.th,"in this paper we provide the asymptotic theory of the general of $\phi$-divergences measures, which include the most common divergence measures : renyi and tsallis families and the kullback-leibler measure. we are interested in divergence measures in the discrete case. one sided and two-sided statistical tests are derived as well as symmetrized estimators. almost sure rates of convergence and asymptotic normality theorem are obtained in the general case, and next particularized for the renyi and tsallis families and for the kullback-leibler measure as well. our theorical results are validated by simulations.",,2018-12-11,2019-03-28,"['ba amadou diadie', 'gane samb lo']"
161,1812.04928,multiple model-free knockoffs,stat.me,"model-free knockoffs is a recently proposed technique for identifying covariates that is likely to have an effect on a response variable. the method is an efficient method to control the false discovery rate in hypothesis tests for separate covariates. this paper presents a generalisation of the technique using multiple sets of model-free knockoffs. this is formulated as an open question in candes et al. [4]. with multiple knockoffs, we are able to reduce the randomness in the knockoffs, making the result stronger. since we use the same structure for generating all the knockoffs, the computational resources is far smaller than proportional with the number of knockoffs. we prove a bound on the asymptotic false discovery rate when the number of sets increases that is better then the published bounds for one set.",,2018-12-12,2019-03-28,"['lars holden', 'kristoffer hellton']"
162,1812.04948,a style-based generator architecture for generative adversarial networks,cs.ne cs.lg stat.ml,"we propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. the new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. the new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. to quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. finally, we introduce a new, highly varied and high-quality dataset of human faces.",,2018-12-12,2019-03-29,"['tero karras', 'samuli laine', 'timo aila']"
163,1812.06535,deep clustering based on a mixture of autoencoders,cs.lg cs.ai stat.ml,"in this paper we propose a deep autoencoder mixture clustering (damic) algorithm based on a mixture of deep autoencoders where each cluster is represented by an autoencoder. a clustering network transforms the data into another space and then selects one of the clusters. next, the autoencoder associated with this cluster is used to reconstruct the data-point. the clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders. the optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network. unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point. our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.",,2018-12-16,2019-03-27,"['shlomo e. chazan', 'sharon gannot', 'jacob goldberger']"
164,1812.09658,learning finite-dimensional coding schemes with nonlinear reconstruction   maps,stat.ml cs.lg,"this paper generalizes the maurer--pontil framework of finite-dimensional lossy coding schemes to the setting where a high-dimensional random vector is mapped to an element of a compact set of latent representations in a lower-dimensional euclidean space, and the reconstruction map belongs to a given class of nonlinear maps. under this setup, which encompasses a broad class of unsupervised representation learning problems, we establish a connection to approximate generative modeling under structural constraints using the tools from the theory of optimal transportation. next, we consider problem of learning a coding scheme on the basis of a finite collection of training samples and present generalization bounds that hold with high probability. we then illustrate the general theory in the setting where the reconstruction maps are implemented by deep neural nets.",,2018-12-23,2019-03-31,"['jaeho lee', 'maxim raginsky']"
165,1812.10366,a poisson-gaussian denoising dataset with real fluorescence microscopy   images,cs.cv cs.lg eess.iv stat.ml,"fluorescence microscopy has enabled a dramatic development in modern biology. due to its inherently weak signal, fluorescence microscopy is not only much noisier than photography, but also presented with poisson-gaussian noise where poisson noise, or shot noise, is the dominating noise source. to get clean fluorescence microscopy images, it is highly desirable to have effective denoising algorithms and datasets that are specifically designed to denoise fluorescence microscopy images. while such algorithms exist, no such datasets are available. in this paper, we fill this gap by constructing a dataset - the fluorescence microscopy denoising (fmd) dataset - that is dedicated to poisson-gaussian denoising. the dataset consists of 12,000 real fluorescence microscopy images obtained with commercial confocal, two-photon, and wide-field microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissues. we use image averaging to effectively obtain ground truth images and 60,000 noisy images with different noise levels. we use this dataset to benchmark 10 representative denoising algorithms and find that deep learning methods have the best performance. to our knowledge, this is the first real microscopy image dataset for poisson-gaussian denoising purposes and it could be an important tool for high-quality, real-time denoising applications in biomedical research.",,2018-12-26,2019-04-05,"['yide zhang', 'yinhao zhu', 'evan nichols', 'qingfei wang', 'siyuan zhang', 'cody smith', 'scott howard']"
166,1812.11731,batch size influence on performance of graphic and tensor processing   units during training and inference phases,cs.lg cs.pf stat.ml,"the impact of the maximally possible batch size (for the better runtime) on performance of graphic processing units (gpu) and tensor processing units (tpu) during training and inference phases is investigated. the numerous runs of the selected deep neural network (dnn) were performed on the standard mnist and fashion-mnist datasets. the significant speedup was obtained even for extremely low-scale usage of google tpuv2 units (8 cores only) in comparison to the quite powerful gpu nvidia tesla k80 card with the speedup up to 10x for training stage (without taking into account the overheads) and speedup up to 2x for prediction stage (with and without taking into account overheads). the precise speedup values depend on the utilization level of tpuv2 units and increase with the increase of the data volume under processing, but for the datasets used in this work (mnist and fashion-mnist with images of sizes 28x28) the speedup was observed for batch sizes >512 images for training phase and >40 000 images for prediction phase. it should be noted that these results were obtained without detriment to the prediction accuracy and loss that were equal for both gpu and tpu runs up to the 3rd significant digit for mnist dataset, and up to the 2nd significant digit for fashion-mnist dataset.",10.1007/978-3-030-16621-2_61,2018-12-31,,"['yuriy kochura', 'yuri gordienko', 'vlad taran', 'nikita gordienko', 'alexandr rokovyi', 'oleg alienin', 'sergii stirenko']"
167,1901.00001,impact of ground truth annotation quality on performance of semantic   image segmentation of traffic conditions,cs.cv cs.lg stat.ml,"preparation of high-quality datasets for the urban scene understanding is a labor-intensive task, especially, for datasets designed for the autonomous driving applications. the application of the coarse ground truth (gt) annotations of these datasets without detriment to the accuracy of semantic image segmentation (by the mean intersection over union - miou) could simplify and speedup the dataset preparation and model fine tuning before its practical application. here the results of the comparative analysis for semantic segmentation accuracy obtained by pspnet deep learning architecture are presented for fine and coarse annotated images from cityscapes dataset. two scenarios were investigated: scenario 1 - the fine gt images for training and prediction, and scenario 2 - the fine gt images for training and the coarse gt images for prediction. the obtained results demonstrated that for the most important classes the mean accuracy values of semantic image segmentation for coarse gt annotations are higher than for the fine gt ones, and the standard deviation values are vice versa. it means that for some applications some unimportant classes can be excluded and the model can be tuned further for some classes and specific regions on the coarse gt dataset without loss of the accuracy even. moreover, this opens the perspectives to use deep neural networks for the preparation of such coarse gt datasets.",10.1007/978-3-030-16621-2_17,2018-12-30,,"['vlad taran', 'yuri gordienko', 'alexandr rokovyi', 'oleg alienin', 'sergii stirenko']"
168,1901.00434,the capacity of feedforward neural networks,cs.lg cs.ne math.co stat.ml,"a long standing open problem in the theory of neural networks is the development of quantitative methods to estimate and compare the capabilities of different architectures. here we define the capacity of an architecture by the binary logarithm of the number of functions it can compute, as the synaptic weights are varied. the capacity provides an upper bound on the number of bits that can be extracted from the training data and stored in the architecture during learning. we study the capacity of layered, fully-connected, architectures of linear threshold neurons with $l$ layers of size $n_1,n_2, \ldots, n_l$ and show that in essence the capacity is given by a cubic polynomial in the layer sizes: $c(n_1,\ldots, n_l)=\sum_{k=1}^{l-1} \min(n_1,\ldots,n_k)n_kn_{k+1}$, where layers that are smaller than all previous layers act as bottlenecks. in proving the main result, we also develop new techniques (multiplexing, enrichment, and stacking) as well as new bounds on the capacity of finite sets. we use the main result to identify architectures with maximal or minimal capacity under a number of natural constraints. this leads to the notion of structural regularization for deep architectures. while in general, everything else being equal, shallow networks compute more functions than deep networks, the functions computed by deep networks are more regular and ""interesting"".",,2019-01-02,2019-03-27,"['pierre baldi', 'roman vershynin']"
169,1901.01751,generative adversarial networks for financial trading strategies   fine-tuning and combination,cs.lg q-fin.pm stat.ml,"systematic trading strategies are algorithmic procedures that allocate assets aiming to optimize a certain performance criterion. to obtain an edge in a highly competitive environment, the analyst needs to proper fine-tune its strategy, or discover how to combine weak signals in novel alpha creating manners. both aspects, namely fine-tuning and combination, have been extensively researched using several methods, but emerging techniques such as generative adversarial networks can have an impact into such aspects. therefore, our work proposes the use of conditional generative adversarial networks (cgans) for trading strategies calibration and aggregation. to this purpose, we provide a full methodology on: (i) the training and selection of a cgan for time series data; (ii) how each sample is used for strategies calibration; and (iii) how all generated samples can be used for ensemble modelling. to provide evidence that our approach is well grounded, we have designed an experiment with multiple trading strategies, encompassing 579 assets. we compared cgan with an ensemble scheme and model validation methods, both suited for time series. our results suggest that cgans are a suitable alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha.",,2019-01-07,2019-03-30,"['adriano koshiyama', 'nick firoozye', 'philip treleaven']"
170,1901.02182,"comments on ""deep neural networks with random gaussian weights: a   universal classification strategy?""",stat.ml cs.lg,"in a recently published paper [1], it is shown that deep neural networks (dnns) with random gaussian weights preserve the metric structure of the data, with the property that the distance shrinks more when the angle between the two data points is smaller. we agree that the random projection setup considered in [1] preserves distances with a high probability. but as far as we are concerned, the relation between the angle of the data points and the output distances is quite the opposite, i.e., smaller angles result in a weaker distance shrinkage. this leads us to conclude that theorem 3 and figure 5 in [1] are not accurate. hence the usage of random gaussian weights in dnns cannot provide an ability of universal classification or treating in-class and out-of-class data separately. consequently, the behavior of networks consisting of random gaussian weights only is not useful to explain how dnns achieve state-of-art results in a large variety of problems.",,2019-01-08,2019-04-01,"['talha cihad gulcu', 'alper gungor']"
171,1901.02369,learning the optimal state-feedback via supervised imitation learning,cs.lg cs.ne cs.sy stat.ml,"imitation learning is a control design paradigm that seeks to learn a control policy reproducing demonstrations from expert agents. by substituting expert demonstrations for optimal behaviours, the same paradigm leads to the design of control policies closely approximating the optimal state-feedback. this approach requires training a machine learning algorithm (in our case deep neural networks) directly on state-control pairs originating from optimal trajectories. we have shown in previous work that, when restricted to low-dimensional state and control spaces, this approach is very successful in several deterministic, non-linear problems in continuous-time. in this work, we refine our previous studies using as a test case a simple quadcopter model with quadratic and time-optimal objective functions. we describe in detail the best learning pipeline we have developed, that is able to approximate via deep neural networks the state-feedback map to a very high accuracy. we introduce the use of the softplus activation function in the hidden units of neural networks showing that it results in a smoother control profile whilst retaining the benefits of rectifiers. we show how to evaluate the optimality of the trained state-feedback, and find that already with two layers the objective function reached and its optimal value differ by less than one percent. we later consider also an additional metric linked to the system asymptotic behaviour - time taken to converge to the policy's fixed point. with respect to these metrics, we show that improvements in the mean absolute error do not necessarily correspond to better policies.",,2019-01-07,2019-04-04,"['dharmesh tailor', 'dario izzo']"
172,1901.02976,the square root rule for adaptive importance sampling,math.st stat.co stat.th,"in adaptive importance sampling, and other contexts, we have $k>1$ unbiased and uncorrelated estimates $\hat\mu_k$ of a common quantity $\mu$. the optimal unbiased linear combination weights them inversely to their variances but those weights are unknown and hard to estimate. a simple deterministic square root rule based on a working model that $\mathrm{var}(\hat\mu_k)\propto k^{-1/2}$ gives an unbisaed estimate of $\mu$ that is nearly optimal under a wide range of alternative variance patterns. we show that if $\mathrm{var}(\hat\mu_k)\propto k^{-y}$ for an unknown rate parameter $y\in [0,1]$ then the square root rule yields the optimal variance rate with a constant that is too large by at most $9/8$ for any $0\le y\le 1$ and any number $k$ of estimates. numerical work shows that rule is similarly robust to some other patterns with mildly decreasing variance as $k$ increases.",,2019-01-09,2019-03-28,"['art b. owen', 'yi zhou']"
173,1901.03357,no-regret bayesian optimization with unknown hyperparameters,stat.ml cs.lg,"bayesian optimization (bo) based on gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. while several bo algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. this is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. in this paper, we present the first bo algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. during optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the bo algorithm considers more complex function candidates. based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of bo with online hyperparameter estimation, but retain theoretical convergence guarantees. we evaluate our method on several benchmark problems.",,2019-01-10,2019-04-01,"['felix berkenkamp', 'angela p. schoellig', 'andreas krause']"
174,1901.03553,dive: a spatiotemporal progression model of brain pathology in   neurodegenerative disorders,cs.cv cs.lg q-bio.nc q-bio.qm stat.ml,"here we present dive: data-driven inference of vertexwise evolution. dive is an image-based disease progression model with single-vertex resolution, designed to reconstruct long-term patterns of brain pathology from short-term longitudinal data sets. dive clusters vertex-wise biomarker measurements on the cortical surface that have similar temporal dynamics across a patient population, and concurrently estimates an average trajectory of vertex measurements in each cluster. dive uniquely outputs a parcellation of the cortex into areas with common progression patterns, leading to a new signature for individual diseases. dive further estimates the disease stage and progression speed for every visit of every subject, potentially enhancing stratification for clinical trials or management. on simulated data, dive can recover ground truth clusters and their underlying trajectory, provided the average trajectories are sufficiently different between clusters. we demonstrate dive on data from two cohorts: the alzheimer's disease neuroimaging initiative (adni) and the dementia research centre (drc), uk, containing patients with posterior cortical atrophy (pca) as well as typical alzheimer's disease (tad). dive finds similar spatial patterns of atrophy for tad subjects in the two independent datasets (adni and drc), and further reveals distinct patterns of pathology in different diseases (tad vs pca) and for distinct types of biomarker data: cortical thickness from magnetic resonance imaging (mri) vs amyloid load from positron emission tomography (pet). finally, dive can be used to estimate a fine-grained spatial distribution of pathology in the brain using any kind of voxelwise or vertexwise measures including jacobian compression maps, fractional anisotropy (fa) maps from diffusion imaging or other pet measures. dive source code is available online: https://github.com/mrazvan22/dive",10.1016/j.neuroimage.2019.02.053,2019-01-11,,"['razvan v. marinescu', 'arman eshaghi', 'marco lorenzi', 'alexandra l. young', 'neil p. oxtoby', 'sara garbarino', 'sebastian j. crutch', 'daniel c. alexander']"
175,1901.04134,bayesian graph selection consistency under model misspecification,math.st stat.th,"gaussian graphical models are a popular tool to learn the dependence structure in the form of a graph among variables of interest. bayesian methods have gained in popularity in the last two decades due to their ability to simultaneously learn the covariance and the graph and characterize uncertainty in the selection. for scalability of the markov chain monte carlo algorithms, decomposability is commonly imposed on the graph space. a wide variety of graphical conjugate priors are proposed jointly on the covariance matrix and the graph with improved algorithms to search along the space of decomposable graphs, rendering the methods extremely popular in the context of multivariate dependence modeling. {\it an open problem} in bayesian decomposable structure learning is whether the posterior distribution is able to select a meaningful decomposable graph that it is ``close'' in an appropriate sense to the true non-decomposable graph, when the dimension of the variables increases with the sample size. in this article, we explore specific conditions on the true precision matrix and the graph which results in an affirmative answer to this question using a commonly used hyper-inverse wishart prior on the covariance matrix and a suitable complexity prior on the graph space, both in the well-specified and misspecified settings. in absence of structural sparsity assumptions, our strong selection consistency holds in a high dimensional setting where $p = o(n^{\alpha})$ for $\alpha < 1/3$. we show when the true graph is non-decomposable, the posterior distribution on the graph concentrates on a set of graphs that are {\it minimal triangulations} of the true graph.",,2019-01-14,2019-03-31,"['yabo niu', 'debdeep pati', 'bani mallick']"
176,1901.04869,optimal acceptance sampling for modules f and f1 of the european   measuring instruments directive,stat.ap,"acceptance sampling plans offered by iso 2859-1 are far from optimal under the conditions for statistical verification in modules f and f1 as prescribed by annex ii of the measuring instruments directive (mid) 2014/32/eu, resulting in sample sizes that are larger than necessary. an optimised single-sampling scheme is derived, both for large lots using the binomial distribution and for finite-sized lots using the exact hypergeometric distribution, resulting in smaller sample sizes that are economically more efficient while offering the full statistical protection required by the mid.",10.1080/02664763.2019.1588235,2019-01-15,2019-03-28,['cord a. müller']
177,1901.05194,on the estimation of population size from a dependent triple record   system,stat.me stat.co,"population size estimation based on capture-recapture experiment under triple record system is an interesting problem in various fields including epidemiology, population studies, etc. in many real life scenarios, there exists inherent dependency between capture and recapture attempts. we propose a novel model that successfully incorporates the possible dependency and the associated parameters possess nice interpretations. we provide estimation methodology for the population size and the associated model parameters based on maximum likelihood method. the proposed model is applied to analyze real data sets from public health and census coverage evaluation study. the performance of the proposed estimate is evaluated through extensive simulation study and the results are compared with the existing competitors. the results exhibit superiority of the proposed model over the existing competitors both in real data analysis and simulation study.",,2019-01-16,2019-04-01,"['kiranmoy chatterjee', 'prajamitra bhuyan']"
178,1901.06587,fitting relus via sgd and quantized sgd,cs.lg cs.dc cs.it math.it stat.ml,"in this paper we focus on the problem of finding the optimal weights of the shallowest of neural networks consisting of a single rectified linear unit (relu). these functions are of the form $\mathbf{x}\rightarrow \max(0,\langle\mathbf{w},\mathbf{x}\rangle)$ with $\mathbf{w}\in\mathbb{r}^d$ denoting the weight vector. we focus on a planted model where the inputs are chosen i.i.d. from a gaussian distribution and the labels are generated according to a planted weight vector. we first show that mini-batch stochastic gradient descent when suitably initialized, converges at a geometric rate to the planted model with a number of samples that is optimal up to numerical constants. next we focus on a parallel implementation where in each iteration the mini-batch gradient is calculated in a distributed manner across multiple processors and then broadcast to a master or all other processors. to reduce the communication cost in this setting we utilize a quanitzed stochastic gradient scheme (qsgd) where the partial gradients are quantized. perhaps unexpectedly, we show that qsgd maintains the fast convergence of sgd to a globally optimal model while significantly reducing the communication cost. we further corroborate our numerical findings via various experiments including distributed implementations over amazon ec2.",,2019-01-19,2019-04-01,"['seyed mohammadreza mousavi kalan', 'mahdi soltanolkotabi', 'a. salman avestimehr']"
179,1901.07132,universal rules for fooling deep neural networks based text   classification,cs.lg cs.cl cs.cr stat.ml,"recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. however, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. in fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. in other words, the universal rules are sample and method agnostic. by proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). a comparison with a random search algorithm further justifies the strength of the method. thus, universal rules for fooling networks are here shown to exist. hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses, culminating in perhaps a new age for artificial intelligence.",,2019-01-21,2019-04-03,"['di li', 'danilo vasconcellos vargas', 'sakurai kouichi']"
180,1901.07298,online estimation of multiple dynamic graphs in pattern sequences,stat.ml cond-mat.dis-nn cs.lg cs.ne q-bio.nc,"sequences of correlated binary patterns can represent many time-series data including text, movies, and biological signals. these patterns may be described by weighted combinations of a few dominant structures that underpin specific interactions among the binary elements. to extract the dominant correlation structures and their contributions to generating data in a time-dependent manner, we model the dynamics of binary patterns using the state-space model of an ising-type network that is composed of multiple undirected graphs. we provide a sequential bayes algorithm to estimate the dynamics of weights on the graphs while gaining the graph structures online. this model can uncover overlapping graphs underlying the data better than a traditional orthogonal decomposition method, and outperforms an original time-dependent ising model. we assess the performance of the method by simulated data, and demonstrate that spontaneous activity of cultured hippocampal neurons is represented by dynamics of multiple graphs.",,2019-01-22,2019-03-27,"['jimmy gaudreault', 'arunabh saxena', 'hideaki shimazaki']"
181,1901.07675,a new cgan technique for constrained topology design optimization,cs.lg stat.ml,"this paper presents a new conditional gan (named convex relaxing cgan or crcgan) to replicate the conventional constrained topology optimization algorithms in an extremely effective and efficient process. the proposed crcgan consists of a generator and a discriminator, both of which are deep convolutional neural networks (cnn) and the topology design constraint can be conditionally set to both the generator and discriminator. in order to improve the training efficiency and accuracy due to the dependency between the training images and the condition, a variety of crcgan formulation are introduced to relax the non-convex design space. these new formulations were evaluated and validated via a series of comprehensive experiments. moreover, a minibatch discrimination technique was introduced in the crcgan training process to stabilize the convergence and avoid the mode collapse problems. additional verifications were conducted using the state-of-the-art mnist digits and cifar-10 images conditioned by class labels. the experimental evaluations clearly reveal that the new objective formulation with the minibatch discrimination training provides not only the accuracy but also the consistency of the designs.",,2019-01-22,2019-04-01,"['m. -h. herman shen', 'liang chen']"
182,1901.08991,diffusion variational autoencoders,cs.lg stat.ml,"a standard variational autoencoder, with a euclidean latent space, is structurally incapable of capturing topological properties of certain datasets. to remove topological obstructions, we introduce diffusion variational autoencoders with arbitrary manifolds as a latent space. a diffusion variational autoencoder uses transition kernels of brownian motion on the manifold. in particular, it uses properties of the brownian motion to implement the reparametrization trick and fast approximations to the kl divergence. we show that the diffusion variational autoencoder is capable of capturing topological properties of synthetic datasets. additionally, we train mnist on spheres, tori, projective spaces, so(3), and a torus embedded in r3. although a natural dataset like mnist does not have latent variables with a clear-cut topological structure, training it on a manifold can still highlight topological and geometrical properties.",,2019-01-25,2019-03-28,"['luis a. pérez rey', 'vlado menkovski', 'jacobus w. portegies']"
183,1901.10024,cross-domain image manipulation by demonstration,cs.lg cs.gr stat.ml,"in this work we propose a model that can manipulate individual visual attributes of objects in a real scene using examples of how respective attribute manipulations affect the output of a simulation. as an example, we train our model to manipulate the expression of a human face using nonphotorealistic 3d renders of a face with varied expression. our model manages to preserve all other visual attributes of a real face, such as head orientation, even though this and other attributes are not labeled in either real or synthetic domain. since our model learns to manipulate a specific property in isolation using only ""synthetic demonstrations"" of such manipulations without explicitly provided labels, it can be applied to shape, texture, lighting, and other properties that are difficult to measure or represent as real-valued vectors. we measure the degree to which our model preserves other attributes of a real image when a single specific attribute is manipulated. we use digit datasets to analyze how discrepancy in attribute distributions affects the performance of our model, and demonstrate results in a far more difficult setting: learning to manipulate real human faces using nonphotorealistic 3d renders.",,2019-01-28,2019-04-03,"['ben usman', 'nick dufour', 'kate saenko', 'chris bregler']"
184,1901.10516,the ffbs estimation of high dimensional panel data factor stochastic   volatility models,stat.me,"in this paper, we propose a new style panel data factor stochastic volatility model with observable factors and unobservable factors based on the multivariate stochastic volatility model, which is mainly composed of three parts, such as the mean equation, volatility equation and factor volatility evolution. the stochastic volatility equation is a 1-step forward prediction process with high dimensional parameters to be estimated. using the markov chain monte carlo simulation (mcmc) method, the forward filtering backward sampling (ffbs) algorithm of the stochastic volatility equation is mainly used to estimate the new model by kalman filter recursive algorithm (kfra). the results of numeric simulation and latent factor estimation show that the algorithm possesses robustness and consistency for parameter estimation. this paper makes a comparative analysis of the observable and unobservable factors of internet finance and traditional financial listed companies in the chinese stock market using the new model and its estimation method. the results show that the influence of observable factors is similar to the two types of listed companies, but the influence of unobservable factors is obviously different.",,2019-01-29,2019-04-08,"['guobin fang', 'huimin ma', 'michelle xia', 'bo zhang']"
185,1901.10902,infobot: transfer and exploration via the information bottleneck,stat.ml cs.lg,"a central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. we postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}. these states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. we propose to learn about decision states from prior experience. by training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. we find that this simple mechanism effectively identifies decision states, even in partially observed settings. in effect, the model learns the sensory cues that correlate with potential subgoals. in new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.",,2019-01-30,2019-04-04,"['anirudh goyal', 'riashat islam', 'daniel strouse', 'zafarali ahmed', 'matthew botvinick', 'hugo larochelle', 'yoshua bengio', 'sergey levine']"
186,1901.11512,minimizing negative transfer of knowledge in multivariate gaussian   processes: a scalable and regularized approach,stat.ml cs.lg,"recently there has been an increasing interest in the multivariate gaussian process (mgp) which extends the gaussian process (gp) to deal with multiple outputs. one approach to construct the mgp and account for non-trivial commonalities amongst outputs employs a convolution process (cp). the cp is based on the idea of sharing latent functions across several convolutions. despite the elegance of the cp construction, it provides new challenges that need yet to be tackled. first, even with a moderate number of outputs, model building is extremely prohibitive due to the huge increase in computational demands and number of parameters to be estimated. second, the negative transfer of knowledge may occur when some outputs do not share commonalities. in this paper we address these issues. we propose a regularized pairwise modeling approach for the mgp established using cp. the key feature of our approach is to distribute the estimation of the full multivariate model into a group of bivariate gps which are individually built. interestingly pairwise modeling turns out to possess unique characteristics, which allows us to tackle the challenge of negative transfer through penalizing the latent function that facilitates information sharing in each bivariate model. predictions are then made through combining predictions from the bivariate models within a bayesian framework. the proposed method has excellent scalability when the number of outputs is large and minimizes the negative transfer of knowledge between uncorrelated outputs. statistical guarantees for the proposed method are studied and its advantageous features are demonstrated through numerical studies.",,2019-01-31,2019-03-31,"['raed kontar', 'garvesh raskutti', 'shiyu zhou']"
187,1902.00715,when collaborative filtering meets reinforcement learning,cs.lg cs.ir stat.ml,"in this paper, we study a multi-step interactive recommendation problem, where the item recommended at current step may affect the quality of future recommendations. to address the problem, we develop a novel and effective approach, named cfrl, which seamlessly integrates the ideas of both collaborative filtering (cf) and reinforcement learning (rl). more specifically, we first model the recommender-user interactive recommendation problem as an agent-environment rl task, which is mathematically described by a markov decision process (mdp). further, to achieve collaborative recommendations for the entire user community, we propose a novel cf-based mdp by encoding the states of all users into a shared latent vector space. finally, we propose an effective q-network learning method to learn the agent's optimal policy based on the cf-based mdp. the capability of cfrl is demonstrated by comparing its performance against a variety of existing methods on real-world datasets.",,2019-02-02,2019-04-02,"['yu lei', 'wenjie li']"
188,1902.00744,asymmetric valleys: beyond sharp and flat local minima,cs.lg stat.ml,"despite the non-convex nature of their loss functions, deep neural networks are known to generalize well when optimized with stochastic gradient descent (sgd). recent work conjectures that sgd with proper configuration is able to find wide and flat local minima, which have been proposed to be associated with good generalization performance. in this paper, we observe that local minima of modern deep networks are more than being flat or sharp. specifically, at a local minimum there exist many asymmetric directions such that the loss increases abruptly along one side, and slowly along the opposite side--we formally define such minima as asymmetric valleys. under mild assumptions, we prove that for asymmetric valleys, a solution biased towards the flat side generalizes better than the exact minimizer. further, we show that simply averaging the weights along the sgd trajectory gives rise to such biased solutions implicitly. this provides a theoretical explanation for the intriguing phenomenon observed by izmailov et al. (2018). in addition, we empirically find that batch normalization (bn) appears to be a major cause for asymmetric valleys.",,2019-02-02,2019-04-06,"['haowei he', 'gao huang', 'yang yuan']"
189,1902.01052,restoration and extrapolation of structural transformation by dynamical   general equilibrium feedbacks,stat.ap,"we model sectoral production by serially nesting (cascading) binary compounding processes. the sequence of processes is discovered in a self-similar hierarchical structure stylized in macroscopic input-output transactions. the feedback system of unit cost functions, with recursively estimated nest-wise ces parameters, is calibrated for sectoral productivities to replicate two temporally distant cost share structures, observed in a set of linked input--output tables. we model representative households by multifactor ces, with parameters estimated by fixed effects regressions. by the integrated dynamic general equilibrium model, we extrapolate potential structural transformations, and measure the associated welfare changes, caused by exogenous sectoral productivity shocks.",,2019-02-04,2019-03-27,"['satoshi nakano', 'kazuhiko nishimura']"
190,1902.01132,optimal upper bounds on expected kth record values from igfr   distributions,math.st stat.th,"the paper concerns the optimal upper bounds on the expectations of the kth record values (k >= 1) centered about the sample mean. we consider the case, when the records are based on the infinite sequence of the independent identically distributed random variables, which distribution function is restricted to the family of distributions with the increasing generalized failure rate (igfr). such a class can be defined in terms of the convex orders of some distribution functions. particularly important examples of igfr class are the distributions with the increasing density (id) and increasing failure rate (ifr). presented bounds were obtained with use of the projection method, and are expressed in the scale units based on the standard deviation of the underlying distribution function.",10.1080/02331888.2019.1580282,2019-02-04,,['agnieszka goroncy']
191,1902.01639,exploiting locality in high-dimensional factorial hidden markov models,stat.ml cs.lg,"we propose algorithms for approximate filtering and smoothing in high-dimensional factorial hidden markov models. the approximation involves discarding, in a principled way, likelihood factors according a notion of locality in a factor graph associated with the emission distribution. this allows the exponential-in-dimension cost of exact filtering and smoothing to be avoided. we prove that the approximation accuracy, measured in a local total variation norm, is `dimension-free' in the sense that as the overall dimension of the model increases the error bounds we derive do not necessarily degrade. a key step in the analysis is to quantify the error introduced by localizing the likelihood function in a bayes' rule update. the factorial structure of the likelihood function which we exploit arises naturally when data have known spatial or network structure. we demonstrate the new algorithms on synthetic examples and a london underground passenger flow problem, where the factor graph is effectively given by the train network.",,2019-02-05,2019-04-04,"['lorenzo rimella', 'nick whiteley']"
192,1902.01843,global convergence of neuron birth-death dynamics,stat.ml cs.lg,"neural networks with a large number of parameters admit a mean-field description, which has recently served as a theoretical explanation for the favorable training properties of ""overparameterized"" models. in this regime, gradient descent obeys a deterministic partial differential equation (pde) that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. in this work, we propose a non-local mass transport dynamics that leads to a modified pde with the same minimizer. we implement this non-local dynamics as a stochastic neuronal birth-death process and we prove that it accelerates the rate of convergence in the mean-field limit. we subsequently realize this pde with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be implemented for neural networks with finite numbers of parameters. we illustrate our algorithms with two models to provide intuition for the mechanism through which convergence is accelerated.",,2019-02-05,2019-03-27,"['grant rotskoff', 'samy jelassi', 'joan bruna', 'eric vanden-eijnden']"
193,1902.03081,size independent neural transfer for rddl planning,cs.lg stat.ml,"neural planners for rddl mdps produce deep reactive policies in an offline fashion. these scale well with large domains, but are sample inefficient and time-consuming to train from scratch for each new problem. to mitigate this, recent work has studied neural transfer learning, so that a generic planner trained on other problems of the same domain can rapidly transfer to a new problem. however, this approach only transfers across problems of the same size. we present the first method for neural transfer of rddl mdps that can transfer across problems of different sizes. our architecture has two key innovations to achieve size independence: (1) a state encoder, which outputs a fixed length state embedding by max pooling over varying number of object embeddings, (2) a single parameter-tied action decoder that projects object embeddings into action probabilities for the final policy. on the two challenging rddl domains of sysadmin and game of life, our approach powerfully transfers across problem sizes and has superior learning curves over training from scratch.",,2019-02-08,2019-04-04,"['sankalp garg', 'aniket bajpai', 'n/a mausam']"
194,1902.03223,non-stationary streaming pca,stat.ml cs.lg,"we consider the problem of streaming principal component analysis (pca) when the observations are noisy and generated in a non-stationary environment. given $t$, $p$-dimensional noisy observations sampled from a non-stationary variant of the spiked covariance model, our goal is to construct the best linear $k$-dimensional subspace of the terminal observations. we study the effect of non-stationarity by establishing a lower bound on the number of samples and the corresponding recovery error obtained by any algorithm. we establish the convergence behaviour of the noisy power method using a novel proof technique which maybe of independent interest. we conclude that the recovery guarantee of the noisy power method matches the fundamental limit, thereby generalizing existing results on streaming pca to a non-stationary setting.",,2019-02-08,2019-03-30,"['daniel bienstock', 'apurv shukla', 'seyoung yun']"
195,1902.04205,improving learnability of neural networks: adding supplementary axes to   disentangle data representation,cs.lg stat.ml,"over-parameterized deep neural networks have proven to be able to learn an arbitrary dataset with 100$\%$ training accuracy. because of a risk of overfitting and computational cost issues, we cannot afford to increase the number of network nodes if we want achieve better training results for medical images. previous deep learning research shows that the training ability of a neural network improves dramatically (for the same epoch of training) when a few nodes with supplementary information are added to the network. these few informative nodes allow the network to learn features that are otherwise difficult to learn by generating a disentangled data representation. this paper analyzes how concatenation of additional information as supplementary axes affects the training of the neural networks. this analysis was conducted for a simple multilayer perceptron (mlp) classification model with a rectified linear unit (relu) on two-dimensional training data. we compared the networks with and without concatenation of supplementary information to support our analysis. the model with concatenation showed more robust and accurate training results compared to the model without concatenation. we also confirmed that our findings are valid for deeper convolutional neural networks (cnn) using ultrasound images and for a conditional generative adversarial network (cgan) using the mnist data.",,2019-02-11,,"['bukweon kim', 'sung min lee', 'jin keun seo']"
196,1902.05379,improving dense crowd counting convolutional neural networks using   inverse k-nearest neighbor maps and multiscale upsampling,cs.cv cs.lg stat.ml,"gatherings of thousands to millions of people frequently occur for an enormous variety of events, and automated counting of these high-density crowds is useful for safety, management, and measuring significance of an event. in this work, we show that the regularly accepted labeling scheme of crowd density maps for training deep neural networks is less effective than our alternative inverse k-nearest neighbor (i$k$nn) maps, even when used directly in existing state-of-the-art network structures. we also provide a new network architecture mud-i$k$nn, which uses multi-scale upsampling via transposed convolutions to take full advantage of the provided i$k$nn labeling. this upsampling combined with the i$k$nn maps further improves crowd counting accuracy. our new network architecture performs favorably in comparison with the state-of-the-art. however, our labeling and upsampling techniques are generally applicable to existing crowd counting architectures.",,2019-01-31,2019-03-29,"['greg olmschenk', 'hao tang', 'zhigang zhu']"
197,1902.05679,proxsarah: an efficient algorithmic framework for stochastic composite   nonconvex optimization,math.oc cs.lg stat.ml,"we propose a new stochastic first-order algorithmic framework to solve stochastic composite nonconvex optimization problems that covers both finite-sum and expectation settings. our algorithms rely on the sarah estimator introduced in (nguyen et al, 2017) and consist of two steps: a proximal gradient and an averaging step making them different from existing nonconvex proximal-type algorithms. the algorithms only require an average smoothness assumption of the nonconvex objective term and additional bounded variance assumption if applied to expectation problems. they work with both constant and adaptive step-sizes, while allowing single sample and mini-batches. in all these cases, we prove that our algorithms can achieve the best-known complexity bounds. one key step of our methods is new constant and adaptive step-sizes that help to achieve desired complexity bounds while improving practical performance. our constant step-size is much larger than existing methods including proximal svrg schemes in the single sample case. we also specify the algorithm to the non-composite case that covers existing state-of-the-arts in terms of complexity bounds. our update also allows one to trade-off between step-sizes and mini-batch sizes to improve performance. we test the proposed algorithms on two composite nonconvex problems and neural networks using several well-known datasets.",,2019-02-14,2019-03-28,"['nhan h. pham', 'lam m. nguyen', 'dzung t. phan', 'quoc tran-dinh']"
198,1902.06894,there are no bit parts for sign bits in black-box attacks,cs.lg cs.cr stat.ml,"we present a black-box adversarial attack algorithm which sets new state-of-the-art model evasion rates for query efficiency in the $\ell_\infty$ and $\ell_2$ metrics, where only loss-oracle access to the model is available. on two public black-box attack challenges, the algorithm achieves the highest evasion rate, surpassing all of the submitted attacks. similar performance is observed on a model that is secure against substitute-model attacks. for standard models trained on the mnist, cifar10, and imagenet datasets, averaged over the datasets and metrics, the algorithm is 3.8x less failure-prone, and spends in total 2.5x fewer queries than the current state-of-the-art attacks combined given a budget of 10, 000 queries per attack attempt. notably, it requires no hyperparameter tuning or any data/time-dependent prior. the algorithm exploits a new approach, namely sign-based rather than magnitude-based gradient estimation. this shifts the estimation from continuous to binary black-box optimization. with three properties of the directional derivative, we examine three approaches to adversarial attacks. this yields a superior algorithm breaking a standard mnist model using just 12 queries on average!",,2019-02-19,2019-04-03,"['abdullah al-dujaili', ""una-may o'reilly""]"
199,1902.07181,measuring compositionality in representation learning,cs.lg cs.cl stat.ml,"many machine learning algorithms represent input data with vector embeddings or discrete codes. when inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. while the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. we describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. we use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.",,2019-02-19,2019-04-06,['jacob andreas']
200,1902.07324,computational hardness of certifying bounds on constrained pca problems,cs.ds cs.cc math.st stat.th,"given a random $n \times n$ symmetric matrix $\boldsymbol w$ drawn from the gaussian orthogonal ensemble (goe), we consider the problem of certifying an upper bound on the maximum value of the quadratic form $\boldsymbol x^\top \boldsymbol w \boldsymbol x$ over all vectors $\boldsymbol x$ in a constraint set $\mathcal{s} \subset \mathbb{r}^n$. for a certain class of normalized constraint sets $\mathcal{s}$ we show that, conditional on certain complexity-theoretic assumptions, there is no polynomial-time algorithm certifying a better upper bound than the largest eigenvalue of $\boldsymbol w$. a notable special case included in our results is the hypercube $\mathcal{s} = \{ \pm 1 / \sqrt{n}\}^n$, which corresponds to the problem of certifying bounds on the hamiltonian of the sherrington-kirkpatrick spin glass model from statistical physics.   our proof proceeds in two steps. first, we give a reduction from the detection problem in the negatively-spiked wishart model to the above certification problem. we then give evidence that this wishart detection problem is computationally hard below the classical spectral threshold, by showing that no low-degree polynomial can (in expectation) distinguish the spiked and unspiked models. this method for identifying computational thresholds was proposed in a sequence of recent works on the sum-of-squares hierarchy, and is believed to be correct for a large class of problems. our proof can be seen as constructing a distribution over symmetric matrices that appears computationally indistinguishable from the goe, yet is supported on matrices whose maximum quadratic form over $\boldsymbol x \in \mathcal{s}$ is much larger than that of a goe matrix.",,2019-02-19,2019-04-06,"['afonso s. bandeira', 'dmitriy kunisky', 'alexander s. wein']"
201,1902.08412,adversarial attacks on graph neural networks via meta learning,cs.lg cs.cr stat.ml,"deep learning models for graphs have advanced the state of the art on many tasks. despite their recent success, little is known about their robustness. we investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. our attacks do not assume any knowledge about or access to the target classifiers.",,2019-02-22,,"['daniel zügner', 'stephan günnemann']"
202,1902.08647,better algorithms for stochastic bandits with adversarial corruptions,cs.lg stat.ml,"we study the stochastic multi-armed bandits problem in the presence of adversarial corruption. we present a new algorithm for this problem whose regret is nearly optimal, substantially improving upon previous work. our algorithm is agnostic to the level of adversarial contamination and can tolerate a significant amount of corruption with virtually no degradation in performance.",,2019-02-22,2019-03-28,"['anupam gupta', 'tomer koren', 'kunal talwar']"
203,1902.10730,degenerate feedback loops in recommender systems,stat.ml cs.lg,"machine learning is used extensively in recommender systems deployed in products. the decisions made by these systems can influence user beliefs and preferences which in turn affect the feedback the learning system receives - thus creating a feedback loop. this phenomenon can give rise to the so-called ""echo chambers"" or ""filter bubbles"" that have user and societal implications. in this paper, we provide a novel theoretical analysis that examines both the role of user dynamics and the behavior of recommender systems, disentangling the echo chamber from the filter bubble effect. in addition, we offer practical solutions to slow down system degeneracy. our study contributes toward understanding and developing solutions to commonly cited issues in the complex temporal scenario, an area that is still largely unexplored.",10.1145/3306618.3314288,2019-02-27,2019-03-27,"['ray jiang', 'silvia chiappa', 'tor lattimore', 'andrás györgy', 'pushmeet kohli']"
204,1903.00197,outcome-driven clustering of acute coronary syndrome patients using   multi-task neural network with attention,q-bio.qm cs.lg stat.ml,"cluster analysis aims at separating patients into phenotypically heterogenous groups and defining therapeutically homogeneous patient subclasses. it is an important approach in data-driven disease classification and subtyping. acute coronary syndrome (acs) is a syndrome due to sudden decrease of coronary artery blood flow, where disease classification would help to inform therapeutic strategies and provide prognostic insights. here we conducted outcome-driven cluster analysis of acs patients, which jointly considers treatment and patient outcome as indicators for patient state. multi-task neural network with attention was used as a modeling framework, including learning of the patient state, cluster analysis, and feature importance profiling. seven patient clusters were discovered. the clusters have different characteristics, as well as different risk profiles to the outcome of in-hospital major adverse cardiac events. the results demonstrate cluster analysis using outcome-driven multi-task neural network as promising for patient classification and subtyping.",,2019-03-01,2019-03-27,"['eryu xia', 'xin du', 'jing mei', 'wen sun', 'suijun tong', 'zhiqing kang', 'jian sheng', 'jian li', 'changsheng ma', 'jianzeng dong', 'shaochun li']"
205,1903.03107,phase-aware speech enhancement with deep complex u-net,cs.sd cs.lg eess.as stat.ml,"most deep learning-based models for speech enhancement have mainly focused on estimating the magnitude of spectrogram while reusing the phase from noisy speech for reconstruction. this is due to the difficulty of estimating the phase of clean speech. to improve speech enhancement performance, we tackle the phase estimation problem in three ways. first, we propose deep complex u-net, an advanced u-net structured model incorporating well-defined complex-valued building blocks to deal with complex-valued spectrograms. second, we propose a polar coordinate-wise complex-valued masking method to reflect the distribution of complex ideal ratio masks. third, we define a novel loss function, weighted source-to-distortion ratio (wsdr) loss, which is designed to directly correlate with a quantitative evaluation measure. our model was evaluated on a mixture of the voice bank corpus and demand database, which has been widely used by many deep learning models for speech enhancement. ablation experiments were conducted on the mixed dataset showing that all three proposed approaches are empirically valid. experimental results show that the proposed method achieves state-of-the-art performance in all metrics, outperforming previous approaches by a large margin.",,2019-03-07,2019-04-02,"['hyeong-seok choi', 'jang-hyun kim', 'jaesung huh', 'adrian kim', 'jung-woo ha', 'kyogu lee']"
206,1903.03149,development and validation of computable phenotype to identify and   characterize kidney health in adult hospitalized patients,stat.ap,"background: acute kidney injury (aki) is a common complication in hospitalized patients and a common cause for chronic kidney disease (ckd) and increased hospital cost and mortality. by timely detection of aki and aki progression, effective preventive or therapeutic measures could be offered. this study aims to develop and validate an electronic phenotype to identify patients with ckd and aki. methods: a database with electronic health records data from a retrospective study cohort of 84,352 hospitalized adults was created. this repository includes demographics, comorbidities, vital signs, laboratory values, medications, diagnoses and procedure codes for all index admission, 12 months prior and 12 months follow-up encounters. we developed algorithms to identify ckd and aki based on the kidney disease: improving global outcomes (kdigo) criteria. to measure diagnostic performance of the algorithms, clinician experts performed clinical adjudication of aki and ckd on 300 selected cases. results: among 149,136 encounters, identified ckd by medical history was 12% which increased to 16% using creatinine criteria. among 130,081 encounters with sufficient data for aki phenotyping 21% had aki. the comparison of ckd phenotyping algorithm to manual chart review yielded ppv of 0.87, npv of 0.99, sensitivity of 0.99, and specificity of 0.89. the comparison of aki phenotyping algorithm to manual chart review yielded ppv of 0.99, npv of 0.95 , sensitivity 0.98, and specificity 0.98. conclusions: we developed phenotyping algorithms that yielded very good performance in identification of patients with ckd and aki in validation cohort. this tool may be useful in identifying patients with kidney disease in a large population, in assessing the quality and value of care in such patients.",,2019-03-07,2019-03-26,"['tezcan ozrazgat-baslanti', 'amir motaei', 'rubab islam', 'haleh hashemighouchani', 'matthew ruppert', 'r. w. m. a. madushani', 'mark s. segal', 'gloria lipori', 'azra bihorac', 'charles hobson']"
207,1903.03784,orthogonal estimation of wasserstein distances,stat.ml cs.lg,"wasserstein distances are increasingly used in a wide variety of applications in machine learning. sliced wasserstein distances form an important subclass which may be estimated efficiently through one-dimensional sorting operations. in this paper, we propose a new variant of sliced wasserstein distance, study the use of orthogonal coupling in monte carlo estimation of wasserstein distances and draw connections with stratified sampling, and evaluate our approaches experimentally in a range of large-scale experiments in generative modelling and reinforcement learning.",,2019-03-09,2019-04-05,"['mark rowland', 'jiri hron', 'yunhao tang', 'krzysztof choromanski', 'tamas sarlos', 'adrian weller']"
208,1903.04455,scaling up deep neural networks: a capacity allocation perspective,cs.lg stat.ml,"following the recent work on capacity allocation, we formulate the conjecture that the shattering problem in deep neural networks can only be avoided if the capacity propagation through layers has a non-degenerate continuous limit when the number of layers tends to infinity. this allows us to study a number of commonly used architectures and determine which scaling relations should be enforced in practice as the number of layers grows large. in particular, we recover the conditions of xavier initialization in the multi-channel case, and we find that weights and biases should be scaled down as the inverse square root of the number of layers for deep residual networks and as the inverse square root of the desired memory length for recurrent networks.",,2019-03-11,2019-03-27,['jonathan donier']
209,1903.05379,transmission matrix inference via pseudolikelihood decimation,stat.ml cond-mat.dis-nn cs.lg eess.iv physics.optics,"one of the biggest challenges in the field of biomedical imaging is the comprehension and the exploitation of the photon scattering through disordered media. many studies have pursued the solution to this puzzle, achieving light-focusing control or reconstructing images in complex media. in the present work, we investigate how statistical inference helps the calculation of the transmission matrix in a complex scrambling environment, enabling its usage like a normal optical element. we convert a linear input-output transmission problem into a statistical formulation based on pseudolikelihood maximization, learning the coupling matrix via random sampling of intensity realizations. our aim is to uncover insights from the scattering problem, encouraging the development of novel imaging techniques for better medical investigations, borrowing a number of statistical tools from spin-glass theory.",,2019-03-13,,"['daniele ancora', 'luca leuzzi']"
210,1903.05726,"a multi-armed bandit mcmc, with applications in sampling from doubly   intractable posterior",stat.co cs.ai physics.data-an stat.me stat.ml,"markov chain monte carlo (mcmc) algorithms are widely used to sample from complicated distributions, especially to sample from the posterior distribution in bayesian inference. however, mcmc is not directly applicable when facing the doubly intractable problem. in this paper, we discussed and compared two existing solutions -- pseudo-marginal monte carlo and exchange algorithm. this paper also proposes a novel algorithm: multi-armed bandit mcmc (mabmc), which chooses between two (or more) randomized acceptance ratios in each step. mabmc could be applied directly to incorporate pseudo-marginal monte carlo and exchange algorithm, with higher average acceptance probability.",,2019-03-13,2019-03-27,['guanyang wang']
211,1903.07273,prototype-based classifiers in the presence of concept drift: a   modelling framework,cs.lg cond-mat.dis-nn stat.ml,"we present a modelling framework for the investigation of prototype-based classifiers in non-stationary environments. specifically, we study learning vector quantization (lvq) systems trained from a stream of high-dimensional, clustered data.we consider standard winner-takes-all updates known as lvq1. statistical properties of the input data change on the time scale defined by the training process. we apply analytical methods borrowed from statistical physics which have been used earlier for the exact description of learning in stationary environments. the suggested framework facilitates the computation of learning curves in the presence of virtual and real concept drift. here we focus on timedependent class bias in the training data. first results demonstrate that, while basic lvq algorithms are suitable for the training in non-stationary environments, weight decay as an explicit mechanism of forgetting does not improve the performance under the considered drift processes.",10.1007/978-3-030-19642-4,2019-03-18,,"['michael biehl', 'fthi abadi', 'christina g\x7föpfert', 'barbara hammer']"
212,1903.08072,max-plus operators applied to filter selection and model pruning in   neural networks,math.st cs.cv cs.lg cs.ne stat.ml stat.th,"following recent advances in morphological neural networks, we propose to study in more depth how max-plus operators can be exploited to define morphological units and how they behave when incorporated in layers of conventional neural networks. besides showing that they can be easily implemented with modern machine learning frameworks , we confirm and extend the observation that a max-plus layer can be used to select important filters and reduce redundancy in its previous layer, without incurring performance loss. experimental results demonstrate that the filter selection strategy enabled by a max-plus is highly efficient and robust, through which we successfully performed model pruning on different neural network architectures. we also point out that there is a close connection between maxout networks and our pruned max-plus networks by comparing their respective characteristics. the code for reproducing our experiments is available online.",,2019-03-19,2019-04-08,"['yunxiang zhang', 'samy blusseau', 'santiago velasco-forero', 'isabelle bloch', 'jesus angulo']"
213,1903.08752,byzantine fault tolerant distributed linear regression,cs.lg cs.dc stat.ml,"this paper considers the problem of byzantine fault tolerance in distributed linear regression in a multi-agent system. however, the proposed algorithms are given for a more general class of distributed optimization problems, of which distributed linear regression is a special case. the system comprises of a server and multiple agents, where each agent is holding a certain number of data points and responses that satisfy a linear relationship (could be noisy). the objective of the server is to determine this relationship, given that some of the agents in the system (up to a known number) are byzantine faulty (aka. actively adversarial). we show that the server can achieve this objective, in a deterministic manner, by robustifying the original distributed gradient descent method using norm based filters, namely 'norm filtering' and 'norm-cap filtering', incurring an additional log-linear computation cost in each iteration. the proposed algorithms improve upon the existing methods on three levels: i) no assumptions are required on the probability distribution of data points, ii) system can be partially asynchronous, and iii) the computational overhead (in order to handle byzantine faulty agents) is log-linear in number of agents and linear in dimension of data points. the proposed algorithms differ from each other in the assumptions made for their correctness, and the gradient filter they use.",,2019-03-20,2019-04-04,"['nirupam gupta', 'nitin h. vaidya']"
214,1903.09094,learning personalized thermal preferences via bayesian active learning   with unimodality constraints,cs.lg cs.hc stat.ml,"thermal preferences vary from person to person and may change over time. the main objective of this paper is to sequentially pose intelligent queries to occupants in order to optimally learn the indoor air temperature values which maximize their satisfaction. our central hypothesis is that an occupant's preference relation over indoor air temperature can be described using a scalar function of these temperatures, which we call the ""occupant's thermal utility function"". information about an occupant's preference over these temperatures is available to us through their response to thermal preference queries : ""prefer warmer,"" ""prefer cooler"" and ""satisfied"" which we interpret as statements about the derivative of their utility function, i.e. the utility function is ""increasing"", ""decreasing"" and ""constant"" respectively. we model this hidden utility function using a gaussian process prior with built-in unimodality constraint, i.e., the utility function has a unique maximum, and we train this model using bayesian inference. this permits an expected improvement based selection of next preference query to pose to the occupant, which takes into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling from areas which are likely to offer an improvement over current best observation). we use this framework to sequentially design experiments and illustrate its benefits by showing that it requires drastically fewer observations to learn the maximally preferred temperature values as compared to other methods. this framework is an important step towards the development of intelligent hvac systems which would be able to respond to occupants' personalized thermal comfort needs. in order to encourage the use of our pe framework and ensure reproducibility in results, we publish an implementation of our work named gpprefelicit as an open-source package in python.",,2019-03-21,2019-04-01,"['nimish awalgaonkar', 'ilias bilionis', 'xiaoqi liu', 'panagiota karava', 'athanasios tzempelikos']"
215,1903.09341,unsupervised speech enhancement based on multichannel nmf-informed   beamforming for noise-robust automatic speech recognition,cs.sd cs.lg eess.as stat.ml,"this paper describes multichannel speech enhancement for improving automatic speech recognition (asr) in noisy environments. recently, the minimum variance distortionless response (mvdr) beamforming has widely been used because it works well if the steering vector of speech and the spatial covariance matrix (scm) of noise are given. to estimating such spatial information, conventional studies take a supervised approach that classifies each time-frequency (tf) bin into noise or speech by training a deep neural network (dnn). the performance of asr, however, is degraded in an unknown noisy environment. to solve this problem, we take an unsupervised approach that decomposes each tf bin into the sum of speech and noise by using multichannel nonnegative matrix factorization (mnmf). this enables us to accurately estimate the scms of speech and noise not from observed noisy mixtures but from separated speech and noise components. in this paper we propose online mvdr beamforming by effectively initializing and incrementally updating the parameters of mnmf. another main contribution is to comprehensively investigate the performances of asr obtained by various types of spatial filters, i.e., time-invariant and variant versions of mvdr beamformers and those of rank-1 and full-rank multichannel wiener filters, in combination with mnmf. the experimental results showed that the proposed method outperformed the state-of-the-art dnn-based beamforming method in unknown environments that did not match training data.",10.1109/taslp.2019.2907015,2019-03-21,2019-03-31,"['kazuki shimada', 'yoshiaki bando', 'masato mimura', 'katsutoshi itoyama', 'kazuyoshi yoshii', 'tatsuya kawahara']"
216,1903.09536,a machine learning approach to risk minimisation in electricity markets   with coregionalized sparse gaussian processes,q-fin.rm stat.ml,"the non-storability of electricity makes it unique among commodity assets, and it is an important driver of its price behaviour in secondary financial markets. the instantaneous and continuous matching of power supply with demand is a key factor explaining its volatility. during periods of high demand, costlier generation capabilities are utilised since electricity cannot be stored and this has the impact of driving prices up very quickly. furthermore, the non-storability also complicates physical hedging. owing to these, the problem of joint price-quantity risk in electricity markets is a commonly studied theme.   we propose using gaussian processes (gps) to tackle this problem since gps provide a versatile and elegant non-parametric approach for regression and time-series modelling. however, gps scale poorly with the amount of training data due to a cubic complexity. these considerations suggest that knowledge transfer between price and load is vital for effective hedging, and that a computationally efficient method is required. to this end, we use the coregionalized (or multi-task) sparse gps which addresses the aforementioned issues.   to gauge the performance of our model, we use an average-load strategy as comparator. the latter is a robust approach commonly used by industry. if the spot and load are uncorrelated and gaussian, then hedging with the expected load will result in the minimum variance position.   our main contributions are twofold. firstly, in developing a coregionalized sparse gp-based approach for hedging. secondly, in demonstrating that our model-based strategy outperforms the comparator, and can thus be employed for effective hedging in electricity markets.",,2019-03-22,2019-04-03,"['daniel poh', 'stephen roberts', 'martin tegnér']"
217,1903.10726,improving image classifiers for small datasets by learning rate   adaptations,stat.ml cs.lg stat.ap,"our paper introduces an efficient combination of established techniques to improve classifier performance, in terms of accuracy and training time. we achieve two-fold to ten-fold speedup in nearing state of the art accuracy, over different model architectures, by dynamically tuning the learning rate. we find it especially beneficial in the case of a small dataset, where reliability of machine reasoning is lower. we validate our approach by comparing our method versus vanilla training on cifar-10. we also demonstrate its practical viability by implementing on an unbalanced corpus of diagnostic images.",,2019-03-26,2019-03-27,"['sourav mishra', 'toshihiko yamasaki', 'hideaki imaizumi']"
218,1903.10905,inference for stochastic kinetic models from multiple data sources for   joint estimation of infection dynamics from aggregate reports and virological   data,stat.ap q-bio.pe,"influenza and respiratory syncytial virus (rsv) are the leading etiological agents of seasonal acute respiratory infections (ari) around the world. medical doctors typically base the diagnosis of ari on patients' symptoms alone and do not always conduct virological tests necessary to identify individual viruses, which limits the ability to study the interaction between multiple pathogens and make public health recommendations. we consider a stochastic kinetic model (skm) for two interacting ari pathogens circulating in a large population and an empirically motivated background process for infections with other pathogens causing similar symptoms. an extended marginal sampling approach based on the linear noise approximation to the skm integrates multiple data sources and additional model components. we infer the parameters defining the pathogens' dynamics and interaction within a bayesian hierarchical model and explore the posterior trajectories of infections for each illness based on aggregate infection reports from six epidemic seasons collected by the state health department, and a subset of virological tests from a sentinel program at a general hospital in san luis potos\'i, m\'exico. we interpret the results based on real and simulated data and make recommendations for future data collection strategies. supplementary materials and software are provided online.",,2019-03-24,2019-03-28,"['yury e. garcía', 'oksana a. chkrebtii', 'marcos a. capistrán and', 'daniel e. noyola']"
219,1903.11040,adversarially learned abnormal trajectory classifier,cs.lg stat.ml,"we address the problem of abnormal event detection from trajectory data. in this paper, a new adversarial approach is proposed for building a deep neural network binary classifier, trained in an unsupervised fashion, that can distinguish normal from abnormal trajectory-based events without the need for setting manual detection threshold. inspired by the generative adversarial network (gan) framework, our gan version is a discriminative one in which the discriminator is trained to distinguish normal and abnormal trajectory reconstruction errors given by a deep autoencoder. with urban traffic videos and their associated trajectories, our proposed method gives the best accuracy for abnormal trajectory detection. in addition, our model can easily be generalized for abnormal trajectory-based event detection and can still yield the best behavioural detection results as demonstrated on the caviar dataset.",,2019-03-26,2019-04-03,"['pankaj raj roy', 'guillaume-alexandre bilodeau']"
220,1903.11101,cross-modal data programming enables rapid medical machine learning,cs.lg eess.iv stat.ml,"labeling training datasets has become a key barrier to building medical machine learning models. one strategy is to generate training labels programmatically, for example by applying natural language processing pipelines to text reports associated with imaging studies. we propose cross-modal data programming, which generalizes this intuitive strategy in a theoretically-grounded way that enables simpler, clinician-driven input, reduces required labeling time, and improves with additional unlabeled data. in this approach, clinicians generate training labels for models defined over a target modality (e.g. images or time series) by writing rules over an auxiliary modality (e.g. text reports). the resulting technical challenge consists of estimating the accuracies and correlations of these rules; we extend a recent unsupervised generative modeling technique to handle this cross-modal setting in a provably consistent way. across four applications in radiography, computed tomography, and electroencephalography, and using only several hours of clinician time, our approach matches or exceeds the efficacy of physician-months of hand-labeling with statistical significance, demonstrating a fundamentally faster and more flexible way of building machine learning models in medicine.",,2019-03-26,,"['jared dunnmon', 'alexander ratner', 'nishith khandwala', 'khaled saab', 'matthew markert', 'hersh sagreiya', 'roger goldman', 'christopher lee-messer', 'matthew lungren', 'daniel rubin', 'christopher ré']"
221,1903.11112,privacy-preserving active learning on sensitive data for user intent   classification,cs.lg cs.cl stat.ml,"active learning holds promise of significantly reducing data annotation costs while maintaining reasonable model performance. however, it requires sending data to annotators for labeling. this presents a possible privacy leak when the training set includes sensitive user data. in this paper, we describe an approach for carrying out privacy preserving active learning with quantifiable guarantees. we evaluate our approach by showing the tradeoff between privacy, utility and annotation budget on a binary classification task in a active learning setting.",,2019-03-26,,"['oluwaseyi feyisetan', 'thomas drake', 'borja balle', 'tom diethe']"
222,1903.11158,weighted multisource tradaboost,cs.lg stat.ml,"in this paper we propose an improved method for transfer learning that takes into account the balance between target and source data. this method builds on the state-of-the-art multisource tradaboost, but weighs the importance of each datapoint taking into account the amount of target and source data available. a comparative study is then presented exposing the performance of four transfer learning methods as well as the proposed weighted multisource tradaboost. the experimental results show that the proposed method is able to outperform the base method as the number of target samples increase. these results are promising in the sense that source-target ratio weighing may be a path to improve current methods of transfer learning. however, against the asymptotic conjecture, all transfer learning methods tested in this work get outperformed by a no-transfer svm for large number on target samples.",,2019-03-26,,"['joão antunes', 'alexandre bernardino', 'asim smailagic', 'daniel siewiorek']"
223,1903.11176,on evaluating cnn representations for low resource medical image   classification,eess.iv cs.lg stat.ml,"convolutional neural networks (cnns) have revolutionized performances in several machine learning tasks such as image classification, object tracking, and keyword spotting. however, given that they contain a large number of parameters, their direct applicability into low resource tasks is not straightforward. in this work, we experiment with an application of cnn models to gastrointestinal landmark classification with only a few thousands of training samples through transfer learning. as in a standard transfer learning approach, we train cnns on a large external corpus, followed by representation extraction for the medical images. finally, a classifier is trained on these cnn representations. however, given that several variants of cnns exist, the choice of cnn is not obvious. to address this, we develop a novel metric that can be used to predict test performances, given cnn representations on the training set. not only we demonstrate the superiority of the cnn based transfer learning approach against an assembly of knowledge driven features, but the proposed metric also carries an 87% correlation with the test set performances as obtained using various cnn representations.",,2019-03-26,,"['taruna agrawal', 'rahul gupta', 'shrikanth narayanan']"
224,1903.11187,a layered multiple importance sampling scheme for focused optimal   bayesian experimental design,stat.co stat.me,"we develop a new computational approach for ""focused"" optimal bayesian experimental design with nonlinear models, with the goal of maximizing expected information gain in targeted subsets of model parameters. our approach considers uncertainty in the full set of model parameters, but employs a design objective that can exploit learning trade-offs among different parameter subsets. we introduce a new layered multiple importance sampling scheme that provides consistent estimates of expected information gain in this focused setting. this sampling scheme yields significant reductions in estimator bias and variance for a given computational effort, making optimal design more tractable for a wide range of computationally intensive problems.",,2019-03-26,,"['chi feng', 'youssef m. marzouk']"
225,1903.11200,maximum likelihood estimation of a semiparametric two-component mixture   model using log-concave approximation,stat.me stat.ap stat.co,"motivated by studies in biological sciences to detect differentially expressed genes, a semiparametric two-component mixture model with one known component is being studied in this paper. assuming the density of the unknown component to be log-concave, which contains a very broad family of densities, we develop a semiparametric maximum likelihood estimator and propose an em algorithm to compute it. our new estimation method finds the mixing proportions and the distribution of the unknown component simultaneously. we establish the identifiability of the proposed semiparametric mixture model and prove the existence and consistency of the proposed estimators. we further compare our estimator with several existing estimators through simulation studies and apply our method to two real data sets from biological sciences and astronomy.",,2019-03-26,,"['yangmei zhou', 'weixin yao']"
226,1903.11220,on the adversarial robustness of multivariate robust estimation,stat.ml cs.it cs.lg math.it math.st stat.th,"in this paper, we investigate the adversarial robustness of multivariate $m$-estimators. in the considered model, after observing the whole dataset, an adversary can modify all data points with the goal of maximizing inference errors. we use adversarial influence function (aif) to measure the asymptotic rate at which the adversary can change the inference result. we first characterize the adversary's optimal modification strategy and its corresponding aif. from the defender's perspective, we would like to design an estimator that has a small aif. for the case of joint location and scale estimation problem, we characterize the optimal $m$-estimator that has the smallest aif. we further identify a tradeoff between robustness against adversarial modifications and robustness against outliers, and derive the optimal $m$-estimator that achieves the best tradeoff.",,2019-03-26,,"['erhan bayraktar', 'lifeng lai']"
227,1903.11240,eigenvalue and generalized eigenvalue problems: tutorial,stat.ml cs.lg,"this paper is a tutorial for eigenvalue and generalized eigenvalue problems. we first introduce eigenvalue problem, eigen-decomposition (spectral decomposition), and generalized eigenvalue problem. then, we mention the optimization problems which yield to the eigenvalue and generalized eigenvalue problems. we also provide examples from machine learning, including principal component analysis, kernel supervised principal component analysis, and fisher discriminant analysis, which result in eigenvalue and generalized eigenvalue problems. finally, we introduce the solutions to both eigenvalue and generalized eigenvalue problems.",,2019-03-25,,"['benyamin ghojogh', 'fakhri karray', 'mark crowley']"
228,1903.11253,improving route choice models by incorporating contextual factors via   knowledge distillation,cs.lg cs.ai stat.ml,"route choice models predict the route choices of travelers traversing an urban area. most of the route choice models link route characteristics of alternative routes to those chosen by the drivers. the models play an important role in prediction of traffic levels on different routes and thus assist in development of efficient traffic management strategies that result in minimizing traffic delay and maximizing effective utilization of transport system. high fidelity route choice models are required to predict traffic levels with higher accuracy. existing route choice models do not take into account dynamic contextual conditions such as the occurrence of an accident, the socio-cultural and economic background of drivers, other human behaviors, the dynamic personal risk level, etc. as a result, they can only make predictions at an aggregate level and for a fixed set of contextual factors. for higher fidelity, it is highly desirable to use a model that captures significance of subjective or contextual factors in route choice. this paper presents a novel approach for developing high-fidelity route choice models with increased predictive power by augmenting existing aggregate level baseline models with information on drivers' responses to contextual factors obtained from stated choice experiments carried out in an immersive virtual environment through the use of knowledge distillation.",,2019-03-27,,"['qun liu', 'supratik mukhopadhyay', 'yimin zhu', 'ravindra gudishala', 'sanaz saeidi', 'alimire nabijiang']"
229,1903.11257,how can we be so dense? the benefits of using highly sparse   representations,cs.lg stat.ml,"most artificial networks today rely on dense representations, whereas biological networks rely on sparse representations. in this paper we show how sparse representations can be more robust to noise and interference, as long as the underlying dimensionality is sufficiently high. a key intuition that we develop is that the ratio of the operable volume around a sparse vector divided by the volume of the representational space decreases exponentially with dimensionality. we then analyze computationally efficient sparse networks containing both sparse weights and activations. simulations on mnist and the google speech command dataset show that such networks demonstrate significantly improved robustness and stability compared to dense networks, while maintaining competitive accuracy. we discuss the potential benefits of sparsity on accuracy, noise robustness, hyperparameter tuning, learning speed, computational efficiency, and power requirements.",,2019-03-27,2019-04-02,"['subutai ahmad', 'luiz scheinkman']"
230,1903.11334,hierarchical attention generative adversarial networks for cross-domain   sentiment classification,cs.lg stat.ml,"cross-domain sentiment classification (cdsc) is an importance task in domain adaptation and sentiment classification. due to the domain discrepancy, a sentiment classifier trained on source domain data may not works well on target domain data. in recent years, many researchers have used deep neural network models for cross-domain sentiment classification task, many of which use gradient reversal layer (grl) to design an adversarial network structure to train a domain-shared sentiment classifier. different from those methods, we proposed hierarchical attention generative adversarial networks (hagan) which alternately trains a generator and a discriminator in order to produce a document representation which is sentiment-distinguishable but domain-indistinguishable. besides, the hagan model applies bidirectional gated recurrent unit (bi-gru) to encode the contextual information of a word and a sentence into the document representation. in addition, the hagan model use hierarchical attention mechanism to optimize the document representation and automatically capture the pivots and non-pivots. the experiments on amazon review dataset show the effectiveness of hagan.",,2019-03-27,,"['yuebing zhang', 'duoqian miao', 'jiaqi wang']"
231,1903.11370,large deviations of bivariate gaussian extrema,math.pr math.st stat.th,"we establish sharp tail asymptotics for component-wise extreme values of bivariate gaussian random vectors with arbitrary correlation between the components. we consider two scaling regimes for the tail event in which we demonstrate the existence of a restricted large deviations principle, and identify the unique rate function associated with these asymptotics. our results identify when the maxima of both coordinates are typically attained by two different vs. the same index, and how this depends on the correlation between the coordinates of the bivariate gaussian random vectors. our results complement a growing body of work on the extremes of gaussian processes. the results are also relevant for steady-state performance and simulation analysis of networks of infinite server queues.",,2019-03-27,,"['remco van der hofstad', 'harsha honnappa']"
232,1903.11371,non-parametric archimedean generator estimation with implications for   multiple testing,stat.me,"in multiple testing, the family-wise error rate can be bounded under some conditions by the copula of the test statistics. assuming that this copula is archimedean, we consider two non-parametric archimedean generator estimators. more specifically, we use the non-parametric estimator from genest et al. (2011) and a slight modification thereof. in simulations, we compare the resulting multiple tests with the bonferroni test and the multiple test derived from the true generator as baselines.",,2019-03-27,,"['andré neumann', 'thorsten dickhaus']"
233,1903.11385,"signal demodulation with machine learning methods for physical layer   visible light communications: prototype platform, open dataset and algorithms",eess.sp cs.lg stat.ml,"in this paper, we investigate the design and implementation of machine learning (ml) based demodulation methods in the physical layer of visible light communication (vlc) systems. we build a flexible hardware prototype of an end-to-end vlc system, from which the received signals are collected as the real data. the dataset is available online, which contains eight types of modulated signals. then, we propose three ml demodulators based on convolutional neural network (cnn), deep belief network (dbn), and adaptive boosting (adaboost), respectively. specifically, the cnn based demodulator converts the modulated signals to images and recognizes the signals by the image classification. the proposed dbn based demodulator contains three restricted boltzmann machines (rbms) to extract the modulation features. the adaboost method includes a strong classifier that is constructed by the weak classifiers with the k-nearest neighbor (knn) algorithm. these three demodulators are trained and tested by our online open dataset. experimental results show that the demodulation accuracy of the three data-driven demodulators drops as the transmission distance increases. a higher modulation order negatively influences the accuracy for a given transmission distance. among the three ml methods, the adaboost modulator achieves the best performance.",10.1109/access.2019.2903375,2019-03-13,,"['shuai ma', 'jiahui dai', 'songtao lu', 'hang li', 'han zhang', 'chun du', 'shiyin li']"
234,1903.11454,machine learning approaches in detecting the depression from   resting-state electroencephalogram (eeg): a review study,q-bio.nc cs.lg stat.ap stat.ml,"in this paper, we aimed at reviewing several different approaches present today in the search for more accurate diagnostic and treatment management in mental healthcare. our focus is on mood disorders, and in particular on the major depressive disorder (mdd). we are reviewing and discussing findings based on neuroimaging studies (mri and fmri) first to get the impression of the body of knowledge about the anatomical and functional differences in depression. then, we are focusing on less expensive data-driven approach, applicable for everyday clinical practice, in particular, those based on electroencephalographic (eeg) recordings. among those studies utilizing eeg, we are discussing a group of applications used for detecting of depression based on the resting state eeg (detection studies) and interventional studies (using stimulus in their protocols or aiming to predict the outcome of therapy). we conclude with a discussion and review of guidelines to improve the reliability of developed models that could serve improvement of diagnostic of depression in psychiatry.",,2019-03-26,,['milena cukic radenkovic']
235,1903.11524,autoregressive policies for continuous control deep reinforcement   learning,cs.lg cs.ai cs.ro stat.ml,"reinforcement learning algorithms rely on exploration to discover new behaviors, which is typically achieved by following a stochastic policy. in continuous control tasks, policies with a gaussian distribution have been widely adopted. gaussian exploration however does not result in smooth trajectories that generally correspond to safe and rewarding behaviors in practical tasks. in addition, gaussian policies do not result in an effective exploration of an environment and become increasingly inefficient as the action rate increases. this contributes to a low sample efficiency often observed in learning continuous control tasks. we introduce a family of stationary autoregressive (ar) stochastic processes to facilitate exploration in continuous control domains. we show that proposed processes possess two desirable features: subsequent process observations are temporally coherent with continuously adjustable degree of coherence, and the process stationary distribution is standard normal. we derive an autoregressive policy (arp) that implements such processes maintaining the standard agent-environment interface. we show how arps can be easily used with the existing off-the-shelf learning algorithms. empirically we demonstrate that using arps results in improved exploration and sample efficiency in both simulated and real world domains, and, furthermore, provides smooth exploration trajectories that enable safe operation of robotic hardware.",,2019-03-27,,"['dmytro korenkevych', 'a. rupam mahmood', 'gautham vasan', 'james bergstra']"
236,1903.11551,transfer learning for image-based malware classification,cs.lg cs.cr stat.ml,"in this paper, we consider the problem of malware detection and classification based on image analysis. we convert executable files to images and apply image recognition using deep learning (dl) models. to train these models, we employ transfer learning based on existing dl models that have been pre-trained on massive image datasets. we carry out various experiments with this technique and compare its performance to that of an extremely simple machine learning technique, namely, k-nearest neighbors (\knn). for our k-nn experiments, we use features extracted directly from executables, rather than image analysis. while our image-based dl technique performs well in the experiments, surprisingly, it is outperformed by k-nn. we show that dl models are better able to generalize the data, in the sense that they outperform k-nn in simulated zero-day experiments.",,2019-01-20,,"['niket bhodia', 'pratikkumar prajapati', 'fabio di troia', 'mark stamp']"
237,1903.11555,the shortest confidence interval for the weighted sum of two binomial   proportions,math.st stat.me stat.th,interval estimation of the probability of success in a binomial model is considered. zieli\'nski (2018) showed that the confidence interval which uses information about non-homogeneity of the sample is better than the classical one. in the following paper the shortest confidence interval for non-homogenous sample is constructed.,,2019-03-27,,"['stanisław jaworski', 'wojciech zieliński']"
238,1903.11565,"modeling the health expenditure in japan, 2011. a healthy life years   lost methodology",stat.ot,"the healthy life years lost methodology (hlyl) is introduced to model and estimate the health expenditure in japan in 2011. the hlyl theory and estimation methods are presented in our books in the springer series on demographic methods and population analysis vol. 45 and 46 titled: exploring the health state of a population by dynamic modeling methods and demography and health issues: population aging, mortality and data analysis. special applications appear in chapters of these books as in the health-mortality approach in estimating the healthy life years lost compared to the global burden of disease studies and applications in world, usa and japan and in estimation of the healthy life expectancy in italy through a simple model based on mortality rate by skiadas and arezzo. here further to present the main part of the methodology with more details and illustrations, we develop and extend a life table important to estimate the healthy life years lost along with the fitting to the health expenditure in the related case. the application results are quite promising and important to support decision makers and health agencies with a powerful tool to improve the health expenditure allocation and the future predictions.",,2019-02-24,,"['christos h skiadas', 'charilaos skiadas']"
239,1903.11576,an alternating manifold proximal gradient method for sparse pca and   sparse cca,stat.ml cs.lg math.oc stat.co,"sparse principal component analysis (pca) and sparse canonical correlation analysis (cca) are two essential techniques from high-dimensional statistics and machine learning for analyzing large-scale data. both problems can be formulated as an optimization problem with nonsmooth objective and nonconvex constraints. since non-smoothness and nonconvexity bring numerical difficulties, most algorithms suggested in the literature either solve some relaxations or are heuristic and lack convergence guarantees. in this paper, we propose a new alternating manifold proximal gradient method to solve these two high-dimensional problems and provide a unified convergence analysis. numerical experiment results are reported to demonstrate the advantages of our algorithm.",,2019-03-27,,"['shixiang chen', 'shiqian ma', 'lingzhou xue', 'hui zou']"
240,1903.11582,asymptotics and optimal designs of slope for sparse linear regression,cs.it math.it math.st stat.th,"in sparse linear regression, the slope estimator generalizes lasso by assigning magnitude-dependent regularizations to different coordinates of the estimate. in this paper, we present an asymptotically exact characterization of the performance of slope in the high-dimensional regime where the number of unknown parameters grows in proportion to the number of observations. our asymptotic characterization enables us to derive optimal regularization sequences to either minimize the mse or to maximize the power in variable selection under any given level of type-i error. in both cases, we show that the optimal design can be recast as certain infinite-dimensional convex optimization problems, which have efficient and accurate finite-dimensional approximations. numerical simulations verify our asymptotic predictions. they also demonstrate the superiority of our optimal design over lasso and a regularization sequence previously proposed in the literature.",,2019-03-27,,"['hong hu', 'yue m. lu']"
241,1903.11647,approximate bayesian inference for multivariate point pattern analysis   in disease mapping,stat.me,"we present a novel approach for the analysis of multivariate case-control georeferenced data using bayesian inference in the context of disease mapping, where the spatial distribution of different types of cancers is analyzed. extending other methodology in point pattern analysis, we propose a log-gaussian cox process for point pattern of cases and the controls, which accounts for risk factors, such as exposure to pollution sources, and includes a term to measure spatial residual variation.   for each disease, its intensity is modeled on a baseline spatial effect (estimated from both controls and cases), a disease-specific spatial term and the effects on covariates that account for risk factors. by fitting these models the effect of the covariates on the set of cases can be assessed, and the residual spatial terms can be easily compared to detect areas of high risk not explained by the covariates.   three different types of effects to model exposure to pollution sources are considered. first of all, a fixed effect on the distance to the source. next, smooth terms on the distance are used to model non-linear effects by means of a discrete random walk of order one and a gaussian process in one dimension with a mat\'ern covariance.   models are fit using the integrated nested laplace approximation (inla) so that the spatial terms are approximated using an approach based on solving stochastic partial differential equations (spde). finally, this new framework is applied to a dataset of three different types of cancer and a set of controls from alcal\'a de henares (madrid, spain). covariates available include the distance to several polluting industries and socioeconomic indicators. our findings point to a possible risk increase due to the proximity to some of these industries.",,2019-03-27,,"['francisco palmi-perales', 'virgilio gomez-rubio', 'gonzalo lopez-abente', 'rebeca ramis-prieto', 'jose miguel sanz-anquela', 'pablo fernandez-navarro']"
242,1903.11673,adversarial deep learning in eeg biometrics,cs.lg eess.sp stat.ml,"deep learning methods for person identification based on electroencephalographic (eeg) brain activity encounters the problem of exploiting the temporally correlated structures or recording session specific variability within eeg. furthermore, recent methods have mostly trained and evaluated based on single session eeg data. we address this problem from an invariant representation learning perspective. we propose an adversarial inference approach to extend such deep learning models to learn session-invariant person-discriminative representations that can provide robustness in terms of longitudinal usability. using adversarial learning within a deep convolutional network, we empirically assess and show improvements with our approach based on longitudinally collected eeg data for person identification from half-second eeg epochs.",10.1109/lsp.2019.2906826,2019-03-27,,"['ozan ozdenizci', 'ye wang', 'toshiaki koike-akino', 'deniz erdogmus']"
243,1903.11695,bayesian multinomial logistic normal models through marginally latent   matrix-t processes,stat.me,"bayesian multinomial logistic-normal (mln) models are popular for the analysis of sequence count data (e.g., microbiome or gene expression data) due to their ability to model multivariate count data with complex covariance structure. however, existing implementations of mln models are limited to handling small data sets due to the non-conjugacy of the multinomial and logistic-normal distributions. we introduce mln models which can be written as marginally latent matrix-t process (ltp) models. marginally ltp models describe a flexible class of generalized linear regression, non-linear regression, and time series models. we develop inference schemes for marginally ltp models and, through application to mln models, demonstrate that our inference schemes are both highly accurate and often 4-5 orders of magnitude faster than mcmc.",,2019-03-27,2019-04-01,"['justin d. silverman', 'kimberly roche', 'zachary c. holmes', 'lawrence a. david', 'sayan mukherjee']"
244,1903.11696,stable prediction with radiomics data,stat.ml cs.lg eess.iv q-bio.qm stat.ap stat.me,"motivation: radiomics refers to the high-throughput mining of quantitative features from radiographic images. it is a promising field in that it may provide a non-invasive solution for screening and classification. standard machine learning classification and feature selection techniques, however, tend to display inferior performance in terms of (the stability of) predictive performance. this is due to the heavy multicollinearity present in radiomic data. we set out to provide an easy-to-use approach that deals with this problem.   results: we developed a four-step approach that projects the original high-dimensional feature space onto a lower-dimensional latent-feature space, while retaining most of the covariation in the data. it consists of (i) penalized maximum likelihood estimation of a redundancy filtered correlation matrix. the resulting matrix (ii) is the input for a maximum likelihood factor analysis procedure. this two-stage maximum-likelihood approach can be used to (iii) produce a compact set of stable features that (iv) can be directly used in any (regression-based) classifier or predictor. it outperforms other classification (and feature selection) techniques in both external and internal validation settings regarding survival in squamous cell cancers.",,2019-03-27,,"['carel f. w. peeters', 'caroline übelhör', 'steven w. mes', 'roland martens', 'thomas koopman', 'pim de graaf', 'floris h. p. van velden', 'ronald boellaard', 'jonas a. castelijns', 'dennis e. te beest', 'martijn w. heymans', 'mark a. van de wiel']"
245,1903.11697,bayesian experimental design for oral glucose tolerance tests (ogtt),stat.co stat.me,"ogtt is a common test, frequently used to diagnose insulin resistance or diabetes, in which a patient's blood sugar is measured at various times over the course of a few hours. recent developments in the study of ogtt results have framed it as an inverse problem which has been the subject of bayesian inference. this is a powerful new tool for analyzing the results of an ogtt test,and the question arises as to whether the test itself can be improved. it is of particular interest to discover whether the times at which a patient's glucose is measured can be changed to improve the effectiveness of the test. the purpose of this paper is to explore the possibility of finding a better experimental design, that is, a set of times to perform the test. we review the theory of bayesian experimental design and propose an estimator for the expected utility of a design. we then study the properties of this estimator and propose a new method for quantifying the uncertainty in comparisons between designs. we implement this method to find a new design and the proposed design is compared favorably to the usual testing scheme.",,2019-03-27,,"['nicolás e. kuschinski', 'j. andrés christen', 'adriana monroy', 'silvestre alavez']"
246,1903.11719,fairness in algorithmic decision making: an excursion through the lens   of causality,cs.lg stat.ml,"as virtually all aspects of our lives are increasingly impacted by algorithmic decision making systems, it is incumbent upon us as a society to ensure such systems do not become instruments of unfair discrimination on the basis of gender, race, ethnicity, religion, etc. we consider the problem of determining whether the decisions made by such systems are discriminatory, through the lens of causal models. we introduce two definitions of group fairness grounded in causality: fair on average causal effect (face), and fair on average causal effect on the treated (fact). we use the rubin-neyman potential outcomes framework for the analysis of cause-effect relationships to robustly estimate face and fact. we demonstrate the effectiveness of our proposed approach on synthetic data. our analyses of two real-world data sets, the adult income data set from the uci repository (with gender as the protected attribute), and the nyc stop and frisk data set (with race as the protected attribute), show that the evidence of discrimination obtained by face and fact, or lack thereof, is often in agreement with the findings from other studies. we further show that fact, being somewhat more nuanced compared to face, can yield findings of discrimination that differ from those obtained using face.",10.1145/3308558.3313559,2019-03-27,,"['aria khademi', 'sanghack lee', 'david foley', 'vasant honavar']"
247,1903.11774,how to pick the domain randomization parameters for sim-to-real transfer   of reinforcement learning policies?,cs.lg cs.ai stat.ml,"recently, reinforcement learning (rl) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. however, most of this success is limited to simulation. while there are promising successes in applying rl algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of rl algorithms. domain randomization is a promising direction of research that has demonstrated impressive results using rl algorithms to control real robots. at a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. if the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. a human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. it is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. this extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world",,2019-03-27,,"['quan vuong', 'sharad vikram', 'hao su', 'sicun gao', 'henrik i. christensen']"
248,1903.11775,atrial fibrillation detection using deep features and convolutional   networks,cs.lg stat.ml,"atrial fibrillation is a cardiac arrhythmia that affects an estimated 33.5 million people globally and is the potential cause of 1 in 3 strokes in people over the age of 60. detection and diagnosis of atrial fibrillation (afib) is done noninvasively in the clinical environment through the evaluation of electrocardiograms (ecgs). early research into automated methods for the detection of afib in ecg signals focused on traditional bio-medical signal analysis to extract important features for use in statistical classification models. artificial intelligence models have more recently been used that employ convolutional and/or recurrent network architectures. in this work, significant time and frequency domain characteristics of the ecg signal are extracted by applying the short-time fourier trans-form and then visually representing the information in a spectrogram. two different classification approaches were investigated that utilized deep features in the spectrograms construct-ed from ecg segments. the first approach used a pretrained densenet model to extract features that were then classified using support vector machines, and the second approach used the spectrograms as direct input into a convolutional network. both approaches were evaluated against the mit-bih afib dataset, where the convolutional network approach achieved a classification accuracy of 93.16%. while these results do not surpass established automated atrial fibrillation detection methods, they are promising and warrant further investigation given they did not require any noise prefiltering, hand-crafted features, nor a reliance on beat detection.",,2019-03-27,,"['sara ross-howe', 'h. r. tizhoosh']"
249,1903.11780,wasserstein dependency measure for representation learning,cs.lg stat.ml,"mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. however, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. this limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. in these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. this leads to incomplete representations that are not optimal for downstream tasks. in this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. to mitigate these problems we introduce the wasserstein dependency measure, which learns more complete representations by using the wasserstein distance instead of the kl divergence in the mutual information estimator. we show that a practical approximation to this theoretically motivated solution, constructed using lipschitz constraint techniques from the gan literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.",,2019-03-27,,"['sherjil ozair', 'corey lynch', 'yoshua bengio', 'aaron van den oord', 'sergey levine', 'pierre sermanet']"
250,1903.11789,step change improvement in admet prediction with potentialnet deep   featurization,cs.lg stat.ml,"the absorption, distribution, metabolism, elimination, and toxicity (admet) properties of drug candidates are estimated to account for up to 50% of all clinical trial failures. predicting admet properties has therefore been of great interest to the cheminformatics and medicinal chemistry communities in recent decades. traditional cheminformatics approaches, whether the learner is a random forest or a deep neural network, leverage fixed fingerprint feature representations of molecules. in contrast, in this paper, we learn the features most relevant to each chemical task at hand by representing each molecule explicitly as a graph, where each node is an atom and each edge is a bond. by applying graph convolutions to this explicit molecular representation, we achieve, to our knowledge, unprecedented accuracy in prediction of admet properties. by challenging our methodology with rigorous cross-validation procedures and prospective analyses, we show that deep featurization better enables molecular predictors to not only interpolate but also extrapolate to new regions of chemical space.",,2019-03-28,,"['evan n. feinberg', 'robert sheridan', 'elizabeth joshi', 'vijay s. pande', 'alan c. cheng']"
251,1903.11865,correlating paleoclimate time series: sources of uncertainty and   potential pitfalls,stat.ap,"comparing paleoclimate time series is complicated by a variety of typical features, including irregular sampling, age model uncertainty (e.g., errors due to interpolation between radiocarbon sampling points) and time uncertainty (uncertainty in calibration), which, taken together, result in unequal and uncertain observation times of the individual time series to be correlated. several methods have been proposed to approximate the joint probability distribution needed to estimate correlations, most of which rely either on interpolation or temporal downsampling.   here, we compare the performance of some popular approximation methods using synthetic data resembling common properties of real world marine sediment records. correlations are determined by estimating the parameters of a bivariate gaussian model from the data using markov chain monte carlo sampling. we complement our pseudoproxy experiments by applying the same methodology to a pair of marine benthic oxygen records from the atlantic ocean.   we find that methods based upon interpolation yield better results in terms of precision and accuracy than those which reduce the number of observations. in all cases, the specific characteristics of the studied time series are, however, more important than the choice of a particular interpolation method. relevant features include the number of observations, the persistence of each record, and the imposed coupling strength between the paired series. in most of our pseudoproxy experiments, uncertainty in observation times introduces less additional uncertainty than unequal sampling and errors in observation times do. thus, it can be reasonable to rely on published time scales as long as calibration uncertainties are not known.",,2019-03-28,,"['jasper g. franke', 'reik v. donner']"
252,1903.11867,classification of sparse binary vectors,math.st stat.th,"in this work we consider a problem of multi-label classification, where each instance is associated with some binary vector. our focus is to find a classifier which minimizes false negative discoveries under constraints. depending on the considered set of constraints we propose plug-in methods and provide non-asymptotic analysis under margin type assumptions. specifically, we analyze two particular examples of constraints that promote sparse predictions: in the first one, we focus on classifiers with $\ell_0$-type constraints and in the second one, we address classifiers with bounded false positive discoveries. both formulations lead to different bayes rules and, thus, different plug-in approaches. the first considered scenario is the popular multi-label top-$k$ procedure: a label is predicted to be relevant if its score is among the $k$ largest ones. for this case, we provide an excess risk bound that achieves so called `fast' rates of convergence under a generalization of the margin assumption to this settings. the second scenario differs significantly from the top-$k$ settings, as the constraints are distribution dependent. we demonstrate that in this scenario the almost sure control of false positive discoveries is impossible without extra assumptions. to alleviate this issue we propose a sufficient condition for the consistent estimation and provide non-asymptotic upper-bound.",,2019-03-28,,['evgenii chzhen']
253,1903.11908,generalizing the balance heuristic estimator in multiple importance   sampling,stat.co,"in this paper, we propose a novel and generic family of multiple importance sampling estimators. we first revisit the celebrated balance heuristic estimator, a widely used monte carlo technique for the approximation of intractable integrals. then, we establish a generalized framework for the combination of samples simulated from multiple proposals. we show that the novel framework contains the balance heuristic as a particular case. in addition, we study the optimal choice of the free parameters in such a way the variance of the resulting estimator is minimized. a theoretical variance study shows the optimal solution is always better than the balance heuristic estimator (except in degenerate cases where both are the same). as a side result of this analysis, we also provide new upper bounds for the balance heuristic estimator. finally, we show the gap in the variance of both estimators by means of five numerical examples.",,2019-03-28,2019-04-06,"['mateu sbert', 'víctor elvira']"
254,1903.11983,sentiment analysis on imdb movie comments and twitter data by machine   learning and vector space techniques,cs.ir cs.cl cs.lg stat.ml,"this study's goal is to create a model of sentiment analysis on a 2000 rows imdb movie comments and 3200 twitter data by using machine learning and vector space techniques; positive or negative preliminary information about the text is to provide. in the study, a vector space was created in the knime analytics platform, and a classification study was performed on this vector space by decision trees, na\""ive bayes and support vector machines classification algorithms. the conclusions obtained were compared in terms of each algorithms. the classification results for imdb movie comments are obtained as 94,00%, 73,20%, and 85,50% by decision tree, naive bayes and svm algorithms. the classification results for twitter data set are presented as 82,76%, 75,44% and 72,50% by decision tree, naive bayes svm algorithms as well. it is seen that the best classification results presented in both data sets are which calculated by svm algorithm.",,2019-03-18,,"['i̇lhan tarımer', 'adil çoban', 'arif emre kocaman']"
255,1903.11990,on the stability and generalization of learning with kernel activation   functions,stat.ml cs.lg,"in this brief we investigate the generalization properties of a recently-proposed class of non-parametric activation functions, the kernel activation functions (kafs). kafs introduce additional parameters in the learning process in order to adapt nonlinearities individually on a per-neuron basis, exploiting a cheap kernel expansion of every activation value. while this increase in flexibility has been shown to provide significant improvements in practice, a theoretical proof for its generalization capability has not been addressed yet in the literature. here, we leverage recent literature on the stability properties of non-convex models trained via stochastic gradient descent (sgd). by indirectly proving two key smoothness properties of the models under consideration, we prove that neural networks endowed with kafs generalize well when trained with sgd for a finite number of steps. interestingly, our analysis provides a guideline for selecting one of the hyper-parameters of the model, the bandwidth of the scalar gaussian kernel. a short experimental evaluation validates the proof.",,2019-03-28,,"['michele cirillo', 'simone scardapane', 'steven van vaerenbergh', 'aurelio uncini']"
256,1903.12012,forecasting model based on information-granulated ga-svr and arima for   producer price index,stat.ap cs.cy,"the accuracy of predicting the producer price index (ppi) plays an indispensable role in government economic work. however, it is difficult to forecast the ppi. in our research, we first propose an unprecedented hybrid model based on fuzzy information granulation that integrates the ga-svr and arima (autoregressive integrated moving average model) models. the fuzzy-information-granulation-based ga-svr-arima hybrid model is intended to deal with the problem of imprecision in ppi estimation. the proposed model adopts the fuzzy information-granulation algorithm to pre-classification-process monthly training samples of the ppi, and produced three different sequences of fuzzy information granules, whose support vector regression (svr) machine forecast models were separately established for their genetic algorithm (ga) optimization parameters. finally, the residual errors of the ga-svr model were rectified through arima modeling, and the ppi estimate was reached. research shows that the ppi value predicted by this hybrid model is more accurate than that predicted by other models, including arima, grnn, and ga-svr, following several comparative experiments. research also indicates the precision and validation of the ppi prediction of the hybrid model and demonstrates that the model has consistent ability to leverage the forecasting advantage of ga-svr in non-linear space and of arima in linear space.",,2019-03-28,,"['xiangyan tang', 'liang wang', 'jieren cheng', 'jing chen']"
257,1903.12019,multimodal deep network embedding with integrated structure and   attribute information,cs.lg stat.ml,"network embedding is the process of learning low-dimensional representations for nodes in a network, while preserving node features. existing studies only leverage network structure information and focus on preserving structural features. however, nodes in real-world networks often have a rich set of attributes providing extra semantic information. it has been demonstrated that both structural and attribute features are important for network analysis tasks. to preserve both features, we investigate the problem of integrating structure and attribute information to perform network embedding and propose a multimodal deep network embedding (mdne) method. mdne captures the non-linear network structures and the complex interactions among structures and attributes, using a deep model consisting of multiple layers of non-linear functions. since structures and attributes are two different types of information, a multimodal learning method is adopted to pre-process them and help the model to better capture the correlations between node structure and attribute information. we employ both structural proximity and attribute proximity in the loss function to preserve the respective features and the representations are obtained by minimizing the loss function. results of extensive experiments on four real-world datasets show that the proposed method performs significantly better than baselines on a variety of tasks, which demonstrate the effectiveness and generality of our method.",,2019-03-28,,"['conghui zheng', 'li pan', 'peng wu']"
258,1903.12067,buffered environmental contours,stat.ap,"the main idea of this paper is to use the notion of buffered failure probability from probabilistic structural design, to introduce buffered environmental contours. classical environmental contours are used in structural design in order to obtain upper bounds on the failure probabilities of a large class of designs. the purpose of buffered failure probabilities is the same. however, in constrast to classical environmental contours, this new concept does not just take into account failure vs. functioning, but also to which extent the system is failing. for example, this is relevant when considering the risk of flooding: we are not just interested in knowing whether a river has flooded. the damages caused by the flooding greatly depends on how much the water has risen above the standard level.",,2019-02-28,,"['kristina rognlien dahl', 'arne bang huseby']"
259,1903.12070,comprehensive analysis of dynamic message sign impact on driver   behavior: a random forest approach,cs.cy cs.lg stat.ml,"this study investigates the potential effects of different dynamic message signs (dmss) on driver behavior using a full-scale high-fidelity driving simulator. different dmss are categorized by their content, structure, and type of messages. a random forest algorithm is used for three separate behavioral analyses; a route diversion analysis, a route choice analysis and a compliance analysis; to identify the potential and relative influences of different dmss on these aspects of driver behavior. a total of 390 simulation runs are conducted using a sample of 65 participants from diverse socioeconomic backgrounds. results obtained suggest that dmss displaying lane closure and delay information with advisory messages are most influential with regards to diversion while color-coded dmss and dmss with avoid route advice are the top contributors impacting route choice decisions and dms compliance. in this first-of-a-kind study, based on the responses to the pre and post simulation surveys as well as results obtained from the analysis of driving-simulation-session data, the authors found that color-blind-friendly, color-coded dmss are more effective than alphanumeric dmss - especially in scenarios that demand high compliance from drivers. the increased effectiveness may be attributed to reduced comprehension time and ease with which such dmss are understood by a greater percentage of road users.",,2019-03-09,,"['snehanshu banerjee', 'mansoureh jeihani', 'danny d. brown', 'samira ahangari']"
260,1903.12074,interpretation of machine learning predictions for patient outcomes in   electronic health records,cs.cy cs.lg stat.ml,"electronic health records are an increasingly important resource for understanding the interactions between patient health, environment, and clinical decisions. in this paper we report an empirical study of predictive modeling of several patient outcomes using three state-of-the-art machine learning methods. our primary goal is to validate the models by interpreting the importance of predictors in the final models. central to interpretation is the use of feature importance scores, which vary depending on the underlying methodology. in order to assess feature importance, we compared univariate statistical tests, information-theoretic measures, permutation testing, and normalized coefficients from multivariate logistic regression models. in general we found poor correlation between methods in their assessment of feature importance, even when their performance is comparable and relatively good. however, permutation tests applied to random forest and gradient boosting models showed the most agreement, and the importance scores matched the clinical interpretation most frequently.",,2019-03-14,,"['william la cava', 'christopher bauer', 'jason h. moore', 'sarah a pendergrass']"
261,1903.12078,error analysis for the particle filter: methods and theoretical support,stat.co stat.ap,"the particle filter is a popular bayesian filtering algorithm for use in cases where the state-space model is nonlinear and/or the random terms (initial state or noises) are non-gaussian distributed. we study the behavior of the error in the particle filter algorithm as the number of particles gets large. after a decomposition of the error into two terms, we show that the difference between the estimator and the conditional mean is asymptotically normal when the resampling is done at every step in the filtering process. two nonlinear/non-gaussian examples are tested to verify this conclusion.",,2019-03-26,,"['ziyu liu', 'shihong wei', 'james c. spall']"
262,1903.12080,detecting activities of daily living and routine behaviours in dementia   patients living alone using smart meter load disaggregation,cs.cy cs.lg stat.ml,"the emergence of an ageing population is a significant public health concern. this has led to an increase in the number of people living with progressive neurodegenerative disorders like dementia. consequently, the strain this is places on health and social care services means providing 24-hour monitoring is not sustainable. technological intervention is being considered, however no solution exists to non-intrusively monitor the independent living needs of patients with dementia. as a result many patients hit crisis point before intervention and support is provided. in parallel, patient care relies on feedback from informal carers about significant behavioural changes. yet, not all people have a social support network and early intervention in dementia care is often missed. the smart meter rollout has the potential to change this. using machine learning and signal processing techniques, a home energy supply can be disaggregated to detect which home appliances are turned on and off. this will allow activities of daily living (adls) to be assessed, such as eating and drinking, and observed changes in routine to be detected for early intervention. the primary aim is to help reduce deterioration and enable patients to stay in their homes for longer. a support vector machine (svm) and random decision forest classifier are modelled using data from three test homes. the trained models are then used to monitor two patients with dementia during a six-month clinical trial undertaken in partnership with mersey care nhs foundation trust. in the case of load disaggregation for appliance detection, the svm achieved (auc=0.86074, sen=0.756 and spec=0.92838). while the decision forest achieved (auc=0.9429, sen=0.9634 and spec=0.9634). adls are also analysed to identify the behavioural patterns of the occupant while detecting alterations in routine.",,2019-03-18,,"['c. chalmers', 'p. fergus', 'c. aday curbelo montanez', 's. sikdar', 'f. ball', 'b. kendall']"
263,1903.12090,learning to weight for text classification,cs.lg cs.ir stat.ml,"in information retrieval (ir) and related tasks, term weighting approaches typically consider the frequency of the term in the document and in the collection in order to compute a score reflecting the importance of the term for the document. in tasks characterized by the presence of training data (such as text classification) it seems logical that the term weighting function should take into account the distribution (as estimated from training data) of the term across the classes of interest. although `supervised term weighting' approaches that use this intuition have been described before, they have failed to show consistent improvements. in this article we analyse the possible reasons for this failure, and call consolidated assumptions into question. following this criticism we propose a novel supervised term weighting approach that, instead of relying on any predefined formula, learns a term weighting function optimised on the training set of interest; we dub this approach \emph{learning to weight} (ltw). the experiments that we run on several well-known benchmarks, and using different learning methods, show that our method outperforms previous term weighting approaches in text classification.",,2019-03-28,,"['alejandro moreo fernández', 'andrea esuli', 'fabrizio sebastiani']"
264,1903.12125,nearest-neighbor neural networks for geostatistics,stat.ml cs.lg,"kriging is the predominant method used for spatial prediction, but relies on the assumption that predictions are linear combinations of the observations. kriging often also relies on additional assumptions such as normality and stationarity. we propose a more flexible spatial prediction method based on the nearest-neighbor neural network (4n) process that embeds deep learning into a geostatistical model. we show that the 4n process is a valid stochastic process and propose a series of new ways to construct features to be used as inputs to the deep learning model based on neighboring information. our model framework outperforms some existing state-of-art geostatistical modelling methods for simulated non-gaussian data and is applied to a massive forestry dataset.",,2019-03-28,,"['haoyu wang', 'yawen guan', 'brian j reich']"
265,1903.12127,using latent class analysis to identify ards sub-phenotypes for enhanced   machine learning predictive performance,cs.lg stat.ap stat.ml,"in this work, we utilize machine learning for early recognition of patients at high risk of acute respiratory distress syndrome (ards), which is critical for successful prevention strategies for this devastating syndrome. the difficulty in early ards recognition stems from its complex and heterogenous nature. in this study, we integrate knowledge of the heterogeneity of ards patients into predictive model building. using mimic-iii data, we first apply latent class analysis (lca) to identify homogeneous sub-groups in the ards population, and then build predictive models on the partitioned data. the results indicate that significantly improved performances of prediction can be obtained for two of the three identified sub-phenotypes of ards. experiments suggests that identifying sub-phenotypes is beneficial for building predictive model for ards.",,2019-03-28,,"['tony wang', 'tim tschampel', 'emilia apostolova', 'tom velez']"
266,1903.12235,information theoretic feature transformation learning for brain   interfaces,cs.lg cs.hc cs.it eess.sp math.it stat.ml,"objective: a variety of pattern analysis techniques for model training in brain interfaces exploit neural feature dimensionality reduction based on feature ranking and selection heuristics. in the light of broad evidence demonstrating the potential sub-optimality of ranking based feature selection by any criterion, we propose to extend this focus with an information theoretic learning driven feature transformation concept. methods: we present a maximum mutual information linear transformation (mmi-lint), and a nonlinear transformation (mmi-nonlint) framework derived by a general definition of the feature transformation learning problem. empirical assessments are performed based on electroencephalographic (eeg) data recorded during a four class motor imagery brain-computer interface (bci) task. exploiting state-of-the-art methods for initial feature vector construction, we compare the proposed approaches with conventional feature selection based dimensionality reduction techniques which are widely used in brain interfaces. furthermore, for the multi-class problem, we present and exploit a hierarchical graphical model based bci decoding system. results: both binary and multi-class decoding analyses demonstrate significantly better performances with the proposed methods. conclusion: information theoretic feature transformations are capable of tackling potential confounders of conventional approaches in various settings. significance: we argue that this concept provides significant insights to extend the focus on feature selection heuristics to a broader definition of feature transformation learning in brain interfaces.",10.1109/tbme.2019.2908099,2019-03-28,2019-04-05,"['ozan ozdenizci', 'deniz erdogmus']"
267,1903.12258,using deep learning neural networks and candlestick chart representation   to predict stock market,q-fin.gn cs.lg q-fin.st stat.ml,"stock market prediction is still a challenging problem because there are many factors effect to the stock market price such as company news and performance, industry performance, investor sentiment, social media sentiment and economic factors. this work explores the predictability in the stock market using deep convolutional network and candlestick charts. the outcome is utilized to design a decision support framework that can be used by traders to provide suggested indications of future stock price direction. we perform this work using various types of neural networks like convolutional neural network, residual network and visual geometry group network. from stock market historical data, we converted it to candlestick charts. finally, these candlestick charts will be feed as input for training a convolutional neural network model. this convolutional neural network model will help us to analyze the patterns inside the candlestick chart and predict the future movements of stock market. the effectiveness of our method is evaluated in stock market prediction with a promising results 92.2% and 92.1% accuracy for taiwan and indonesian stock market dataset respectively. the constructed model have been implemented as a web-based system freely available at http://140.138.155.216/deepcandle/ for predicting stock market using candlestick chart and deep learning neural networks.",,2019-02-25,,"['rosdyana mangir irawan kusuma', 'trang-thi ho', 'wei-chun kao', 'yu-yen ou', 'kai-lung hua']"
268,1903.12261,benchmarking neural network robustness to common corruptions and   perturbations,cs.lg cs.cv stat.ml,"in this paper we establish rigorous benchmarks for image classifier robustness. our first benchmark, imagenet-c, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. then we propose a new dataset called imagenet-p which enables researchers to benchmark a classifier's robustness to common perturbations. unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. we find that there are negligible changes in relative corruption robustness from alexnet classifiers to resnet classifiers. afterward we discover ways to enhance corruption and perturbation robustness. we even find that a bypassed adversarial defense provides substantial common perturbation robustness. together our benchmarks may aid future work toward networks that robustly generalize.",,2019-03-28,,"['dan hendrycks', 'thomas dietterich']"
269,1903.12262,towards standardization of data licenses: the montreal data license,cs.cy cs.lg stat.ml,"this paper provides a taxonomy for the licensing of data in the fields of artificial intelligence and machine learning. the paper's goal is to build towards a common framework for data licensing akin to the licensing of open source software. increased transparency and resolving conceptual ambiguities in existing licensing language are two noted benefits of the approach proposed in the paper. in parallel, such benefits may help foster fairer and more efficient markets for data through bringing about clearer tools and concepts that better define how data can be used in the fields of ai and ml. the paper's approach is summarized in a new family of data license language - \textit{the montreal data license (mdl)}. alongside this new license, the authors and their collaborators have developed a web-based tool to generate license language espousing the taxonomies articulated in this paper.",,2019-03-20,,"['misha benjamin', 'paul gagnon', 'negar rostamzadeh', 'chris pal', 'yoshua bengio', 'alex shee']"
270,1903.12266,generative adversarial networks: recent developments,cs.lg cs.cv stat.ml,"in traditional generative modeling, good data representation is very often a base for a good machine learning model. it can be linked to good representations encoding more explanatory factors that are hidden in the original data. with the invention of generative adversarial networks (gans), a subclass of generative models that are able to learn representations in an unsupervised and semi-supervised fashion, we are now able to adversarially learn good mappings from a simple prior distribution to a target data distribution. this paper presents an overview of recent developments in gans with a focus on learning latent space representations.",,2019-03-16,,"['maciej zamorski', 'adrian zdobylak', 'maciej zięba', 'jerzy świątek']"
271,1903.12286,toroidal autoencoder,stat.ml cs.lg,"enforcing distributions of latent variables in neural networks is an active subject. it is vital in all kinds of generative models, where we want to be able to interpolate between points in the latent space, or sample from it. modern generative autoencoders (ae) like wae, swae, cwae add a regularizer to the standard (deterministic) ae, which allows to enforce gaussian distribution in the latent space. enforcing different distributions, especially topologically nontrivial, might bring some new interesting possibilities, but this subject seems unexplored so far.   this article proposes a new approach to enforce uniform distribution on d-dimensional torus. we introduce a circular spring loss, which enforces minibatch points to be equally spaced and satisfy cyclic boundary conditions.   as example of application we propose multiple-path morphing. minimal distance geodesic between two points in uniform distribution on latent space of angles becomes a line, however, torus topology allows us to choose such lines in alternative ways, going through different edges of $[-\pi,\pi]^d$.   further applications to explore can be for example trying to learn real-life topologically nontrivial spaces of features, like rotations to automatically recognize 2d rotation of an object in picture by training on relative angles, or even 3d rotations by additionally using spherical features - this way morphing should be close to object rotation.",,2019-03-28,,"['maciej mikulski', 'jaroslaw duda']"
272,1903.12297,"an analysis of the cost of hyper-parameter selection via split-sample   validation, with applications to penalized regression",stat.ml cs.lg,"in the regression setting, given a set of hyper-parameters, a model-estimation procedure constructs a model from training data. the optimal hyper-parameters that minimize generalization error of the model are usually unknown. in practice they are often estimated using split-sample validation. up to now, there is an open question regarding how the generalization error of the selected model grows with the number of hyper-parameters to be estimated. to answer this question, we establish finite-sample oracle inequalities for selection based on a single training/test split and based on cross-validation. we show that if the model-estimation procedures are smoothly parameterized by the hyper-parameters, the error incurred from tuning hyper-parameters shrinks at nearly a parametric rate. hence for semi- and non-parametric model-estimation procedures with a fixed number of hyper-parameters, this additional error is negligible. for parametric model-estimation procedures, adding a hyper-parameter is roughly equivalent to adding a parameter to the model itself. in addition, we specialize these ideas for penalized regression problems with multiple penalty parameters. we establish that the fitted models are lipschitz in the penalty parameters and thus our oracle inequalities apply. this result encourages development of regularization methods with many penalty parameters.",10.5705/ss.202017.0310,2019-03-28,,"['jean feng', 'noah simon']"
273,1903.12322,implicit langevin algorithms for sampling from log-concave densities,stat.ml cs.lg stat.co,"for sampling from a log-concave density, we study implicit integrators resulting from $\theta$-method discretization of the overdamped langevin diffusion stochastic differential equation. theoretical and algorithmic properties of the resulting sampling methods for $ \theta \in [0,1] $ and a range of step sizes are established. our results generalize and extend prior works in several directions. in particular, for $\theta\ge1/2$, we prove geometric ergodicity and stability of the resulting methods for all step sizes. we show that obtaining subsequent samples amounts to solving a strongly-convex optimization problem, which is readily achievable using one of numerous existing methods. numerical examples supporting our theoretical analysis are also presented.",,2019-03-28,,"['liam hodgkinson', 'robert salomone', 'fred roosta']"
274,1903.12342,statistical matching of non-gaussian data,stat.me,"the statistical matching problem is a data integration problem with structured missing data. the general form involves the analysis of multiple datasets that only have a strict subset of variables jointly observed across all datasets. the simplest version involves two datasets, labelled a and b, with three variables of interest $x, y$ and $z$. variables $x$ and $y$ are observed in dataset a and variables $x$ and $z$ are observed in dataset $b$. statistical inference is complicated by the absence of joint $(y, z)$ observations. parametric modelling can be challenging due to identifiability issues and the difficulty of parameter estimation. we develop computationally feasible procedures for the statistical matching of non-gaussian data using suitable data augmentation schemes and identifiability constraints. nearest-neighbour imputation is a common alternative technique due to its ease of use and generality. nearest-neighbour matching is based on a conditional independence assumption that may be inappropriate for non-gaussian data. the violation of the conditional independence assumption can lead to improper imputations. we compare model based approaches to nearest-neighbour imputation on a number of flow cytometry datasets and find that the model based approach can address some of the weaknesses of the nonparametric nearest-neighbour technique.",,2019-03-28,,"['daniel ahfock', 'saumyadipta pyne', 'geoffrey j. mclachlan']"
275,1903.12344,learning good representation via continuous attention,cs.lg stat.ml,"in this paper we present our scientific discovery that good representation can be learned via continuous attention during the interaction between unsupervised learning(ul) and reinforcement learning(rl) modules driven by intrinsic motivation. specifically, we designed intrinsic rewards generated from ul modules for driving the rl agent to focus on objects for a period of time and to learn good representations of objects for later object recognition task. we evaluate our proposed algorithm in both with and without extrinsic reward settings. experiments with end-to-end training in simulated environments with applications to few-shot object recognition demonstrated the effectiveness of the proposed algorithm.",,2019-03-28,2019-04-01,"['liang zhao', 'wei xu']"
276,1903.12347,the challenge of predicting meal-to-meal blood glucose concentrations   for patients with type i diabetes,cs.lg stat.ml,"patients with type i diabetes (t1d) must take insulin injections to prevent the serious long term effects of hyperglycemia - high blood glucose (bg). patients must also be careful not to inject too much insulin because this could induce hypoglycemia (low bg), which can potentially be fatal. patients therefore follow a ""regimen"" that determines how much insulin to inject at certain times. current methods for managing this disease require adjusting the patient's regimen over time based on the disease's behavior (recorded in the patient's diabetes diary). if we can accurately predict a patient's future bg values from his/her current features (e.g., predicting today's lunch bg value given today's diabetes diary entry for breakfast, including insulin injections, and perhaps earlier entries), then it is relatively easy to produce an effective regimen. this study explores the challenges of bg modeling by applying several machine learning algorithms and various data preprocessing variations (corresponding to 312 [learner, preprocessed-dataset] combinations), to a new t1d dataset containing 29 601 entries from 47 different patients. our most accurate predictor is a weighted ensemble of two gaussian process regression models, which achieved an errl1 loss of 2.70 mmol/l (48.65 mg/dl). this was an unexpectedly poor result given that one can obtain an errl1 of 2.91 mmol/l (52.43 mg/dl) using the naive approach of simply predicting the patient's average bg. for each of data-variant/model combination we report several evaluation metrics, including glucose-specific metrics, and find similarly disappointing results (the best model was only incrementally better than the simplest measure). these results suggest that the diabetes diary data that is typically collected may not be sufficient to produce accurate bg prediction models; additional data may be necessary to build accurate bg prediction models.",,2019-03-28,,"['neil c. borle', 'edmond a. ryan', 'russell greiner']"
277,1903.12384,deep representation with relu neural networks,cs.lg eess.sp stat.ml,"we consider deep feedforward neural networks with rectified linear units from a signal processing perspective. in this view, such representations mark the transition from using a single (data-driven) linear representation to utilizing a large collection of affine linear representations tailored to particular regions of the signal space. this paper provides a precise description of the individual affine linear representations and corresponding domain regions that the (data-driven) neural network associates to each signal of the input space. in particular, we describe atomic decompositions of the representations and, based on estimating their lipschitz regularity, suggest some conditions that can stabilize learning independent of the network depth. such an analysis may promote further theoretical insight from both the signal processing and machine learning communities.",,2019-03-29,,"['andreas heinecke', 'wen-liang hwang']"
278,1903.12389,joint training framework for text-to-speech and voice conversion using   multi-source tacotron and wavenet,eess.as cs.cl cs.sd stat.ml,"we investigated the training of a shared model for both text-to-speech (tts) and voice conversion (vc) tasks. we propose using an extended model architecture of tacotron, that is a multi-source sequence-to-sequence model with a dual attention mechanism as the shared model for both the tts and vc tasks. this model can accomplish these two different tasks respectively according to the type of input. an end-to-end speech synthesis task is conducted when the model is given text as the input while a sequence-to-sequence voice conversion task is conducted when it is given the speech of a source speaker as the input. waveform signals are generated by using wavenet, which is conditioned by using a predicted mel-spectrogram. we propose jointly training a shared model as a decoder for a target speaker that supports multiple sources. listening experiments show that our proposed multi-source encoder-decoder model can efficiently achieve both the tts and vc tasks.",,2019-03-29,2019-04-07,"['mingyang zhang', 'xin wang', 'fuming fang', 'haizhou li', 'junichi yamagishi']"
279,1903.12392,training a neural speech waveform model using spectral losses of   short-time fourier transform and continuous wavelet transform,eess.as cs.cl cs.sd stat.ml,"recently, we proposed short-time fourier transform (stft)-based loss functions for training a neural speech waveform model. in this paper, we generalize the above framework and propose a training scheme for such models based on spectral amplitude and phase losses obtained by either stft or continuous wavelet transform (cwt), or both of them. since cwt is capable of having time and frequency resolutions different from those of stft and is cable of considering those closer to human auditory scales, the proposed loss functions could provide complementary information on speech signals. experimental results showed that it is possible to train a high-quality model by using the proposed cwt spectral loss and is as good as one using stft-based loss.",,2019-03-29,2019-04-07,"['shinji takaki', 'hirokazu kameoka', 'junichi yamagishi']"
280,1903.12416,online variance reduction with mixtures,cs.lg stat.ml,"adaptive importance sampling for stochastic optimization is a promising approach that offers improved convergence through variance reduction. in this work, we propose a new framework for variance reduction that enables the use of mixtures over predefined sampling distributions, which can naturally encode prior knowledge about the data. while these sampling distributions are fixed, the mixture weights are adapted during the optimization process. we propose vrm, a novel and efficient adaptive scheme that asymptotically recovers the best mixture weights in hindsight and can also accommodate sampling distributions over sets of points. we empirically demonstrate the versatility of vrm in a range of applications.",,2019-03-29,,"['zalán borsos', 'sebastian curi', 'kfir y. levy', 'andreas krause']"
281,1903.12536,deep network for capacitive ecg denoising,cs.lg eess.sp stat.ml,"continuous monitoring of cardiac health under free living condition is crucial to provide effective care for patients undergoing post operative recovery and individuals with high cardiac risk like the elderly. capacitive electrocardiogram (cecg) is one such technology which allows comfortable and long term monitoring through its ability to measure biopotential in conditions without having skin contact. cecg monitoring can be done using many household objects like chairs, beds and even car seats allowing for seamless monitoring of individuals. this method is unfortunately highly susceptible to motion artifacts which greatly limits its usage in clinical practice. the current use of cecg systems has been limited to performing rhythmic analysis. in this paper we propose a novel end-to-end deep learning architecture to perform the task of denoising capacitive ecg. the proposed network is trained using motion corrupted three channel cecg and a reference lead i ecg collected on individuals while driving a car. further, we also propose a novel joint loss function to apply loss on both signal and frequency domain. we conduct extensive rhythmic analysis on the model predictions and the ground truth. we further evaluate the signal denoising using mean square error(mse) and cross correlation between model predictions and ground truth. we report mse of 0.167 and cross correlation of 0.476. the reported results highlight the feasibility of performing morphological analysis using the filtered cecg. the proposed approach can allow for continuous and comprehensive monitoring of the individuals in free living conditions.",,2019-03-29,,"['vignesh ravichandran', 'balamurali murugesan', 'sharath m shankaranarayana', 'keerthi ram', 'preejith s. p', 'jayaraj joseph', 'mohanasankar sivaprakasam']"
282,1903.12584,the false positive control lasso,stat.ml cs.lg,"in high dimensional settings where a small number of regressors are expected to be important, the lasso estimator can be used to obtain a sparse solution vector with the expectation that most of the non-zero coefficients are associated with true signals. while several approaches have been developed to control the inclusion of false predictors with the lasso, these approaches are limited by relying on asymptotic theory, having to empirically estimate terms based on theoretical quantities, assuming a continuous response class with gaussian noise and design matrices, or high computation costs. in this paper we show how: (1) an existing model (the sqrt-lasso) can be recast as a method of controlling the number of expected false positives, (2) how a similar estimator can used for all other generalized linear model classes, and (3) this approach can be fit with existing fast lasso optimization solvers. our justification for false positive control using randomly weighted self-normalized sum theory is to our knowledge novel. moreover, our estimator's properties hold in finite samples up to some approximation error which we find in practical settings to be negligible under a strict mutual incoherence condition.",,2019-03-29,,"['erik drysdale', 'yingwei peng', 'timothy p. hanna', 'paul nguyen', 'anna goldenberg']"
283,1903.12650,yet another accelerated sgd: resnet-50 training on imagenet in 74.7   seconds,cs.lg stat.ml,"there has been a strong demand for algorithms that can execute machine learning as faster as possible and the speed of deep learning has accelerated by 30 times only in the past two years. distributed deep learning using the large mini-batch is a key technology to address the demand and is a great challenge as it is difficult to achieve high scalability on large clusters without compromising accuracy. in this paper, we introduce optimization methods which we applied to this challenge. we achieved the training time of 74.7 seconds using 2,048 gpus on abci cluster applying these methods. the training throughput is over 1.73 million images/sec and the top-1 validation accuracy is 75.08%.",,2019-03-29,,"['masafumi yamazaki', 'akihiko kasagi', 'akihiro tabuchi', 'takumi honda', 'masahiro miwa', 'naoto fukumoto', 'tsuguchika tabaru', 'atsushi ike', 'kohta nakashima']"
284,1904.00035,autonomous highway driving using deep reinforcement learning,cs.ro cs.lg cs.sy stat.ml,"the operational space of an autonomous vehicle (av) can be diverse and vary significantly. this may lead to a scenario that was not postulated in the design phase. due to this, formulating a rule based decision maker for selecting maneuvers may not be ideal. similarly, it may not be effective to design an a-priori cost function and then solve the optimal control problem in real-time. in order to address these issues and to avoid peculiar behaviors when encountering unforeseen scenario, we propose a reinforcement learning (rl) based method, where the ego car, i.e., an autonomous vehicle, learns to make decisions by directly interacting with simulated traffic. the decision maker for av is implemented as a deep neural network providing an action choice for a given system state. in a critical application such as driving, an rl agent without explicit notion of safety may not converge or it may need extremely large number of samples before finding a reliable policy. to best address the issue, this paper incorporates reinforcement learning with an additional short horizon safety check (sc). in a critical scenario, the safety check will also provide an alternate safe action to the agent provided if it exists. this leads to two novel contributions. first, it generalizes the states that could lead to undesirable ""near-misses"" or ""collisions "". second, inclusion of safety check can provide a safe and stable training environment. this significantly enhances learning efficiency without inhibiting meaningful exploration to ensure safe and optimal learned behavior. we demonstrate the performance of the developed algorithm in highway driving scenario where the trained av encounters varying traffic density in a highway setting.",,2019-03-29,,"['subramanya nageshrao', 'eric tseng', 'dimitar filev']"
285,1904.00070,data amplification: a unified and competitive approach to property   estimation,stat.ml cs.lg math.st stat.th,"estimating properties of discrete distributions is a fundamental problem in statistical learning. we design the first unified, linear-time, competitive, property estimator that for a wide class of properties and for all underlying distributions uses just $2n$ samples to achieve the performance attained by the empirical estimator with $n\sqrt{\log n}$ samples. this provides off-the-shelf, distribution-independent, ""amplification"" of the amount of data available relative to common-practice estimators.   we illustrate the estimator's practical advantages by comparing it to existing estimators for a wide variety of properties and distributions. in most cases, its performance with $n$ samples is even as good as that of the empirical estimator with $n\log n$ samples, and for essentially all properties, its performance is comparable to that of the best existing estimator designed specifically for that property.",,2019-03-29,,"['yi hao', 'alon orlitsky', 'ananda t. suresh', 'yihong wu']"
286,1904.00117,estimation of cell lineage trees by maximum-likelihood phylogenetics,q-bio.qm stat.ap,"crispr technology has enabled large-scale cell lineage tracing for complex multicellular organisms by mutating synthetic genomic barcodes during organismal development. however, these sophisticated biological tools currently use ad-hoc and outmoded computational methods to reconstruct the cell lineage tree from the mutated barcodes. because these methods are agnostic to the biological mechanism, they are unable to take full advantage of the data's structure. we propose a statistical model for the mutation process and develop a procedure to estimate the tree topology, branch lengths, and mutation parameters by iteratively applying penalized maximum likelihood estimation. in contrast to existing techniques, our method estimates time along each branch, rather than number of mutation events, thus providing a detailed account of tissue-type differentiation. via simulations, we demonstrate that our method is substantially more accurate than existing approaches. our reconstructed trees also better recapitulate known aspects of zebrafish development and reproduce similar results across fish replicates.",,2019-03-29,,"['jean feng', 'william s dewitt', 'aaron mckenna', 'noah simon', 'amy willis', 'frederick a matsen']"
287,1904.00148,bayesian mixed effect sparse tensor response regression model with joint   estimation of activation and connectivity,stat.ap stat.me,"brain activation and connectivity analyses in task-based functional magnetic resonance imaging (fmri) experiments with multiple subjects are currently at the forefront of data-driven neuroscience. in such experiments, interest often lies in understanding activation of brain voxels due to external stimuli and strong association or connectivity between the measurements on a set of pre-specified group of brain voxels, also known as regions of interest (roi). this article proposes a joint bayesian additive mixed modeling framework that simultaneously assesses brain activation and connectivity patterns from multiple subjects. in particular, fmri measurements from each individual obtained in the form of a multi-dimensional array/tensor at each time are regressed on functions of the stimuli. we impose a low-rank parafac decomposition on the tensor regression coefficients corresponding to the stimuli to achieve parsimony. multiway stick breaking shrinkage priors are employed to infer activation patterns and associated uncertainties in each voxel. further, the model introduces region specific random effects which are jointly modeled with a bayesian gaussian graphical prior to account for the connectivity among pairs of rois. empirical investigations under various simulation studies demonstrate the effectiveness of the method as a tool to simultaneously assess brain activation and connectivity. the method is then applied to a multi-subject fmri dataset from a balloon-analog risk-taking experiment in order to make inference about how the brain processes risk.",,2019-03-30,,"['daniel spencer', 'rajarshi guhaniyogi', 'raquel prado']"
288,1904.00170,adaptive adjustment with semantic feature space for zero-shot   recognition,cs.cv cs.lg stat.ml,"in most recent years, zero-shot recognition (zsr) has gained increasing attention in machine learning and image processing fields. it aims at recognizing unseen class instances with knowledge transferred from seen classes. this is typically achieved by exploiting a pre-defined semantic feature space (fs), i.e., semantic attributes or word vectors, as a bridge to transfer knowledge between seen and unseen classes. however, due to the absence of unseen classes during training, the conventional zsr easily suffers from domain shift and hubness problems. in this paper, we propose a novel zsr learning framework that can handle these two issues well by adaptively adjusting semantic fs. to the best of our knowledge, our work is the first to consider the adaptive adjustment of semantic fs in zsr. moreover, our solution can be formulated to a more efficient framework that significantly boosts the training. extensive experiments show the remarkable performance improvement of our model compared with other existing methods.",,2019-03-30,,"['jingcai guo', 'song guo']"
289,1904.00172,ee-ae: an exclusivity enhanced unsupervised feature learning approach,cs.lg stat.ml,"unsupervised learning is becoming more and more important recently. as one of its key components, the autoencoder (ae) aims to learn a latent feature representation of data which is more robust and discriminative. however, most ae based methods only focus on the reconstruction within the encoder-decoder phase, which ignores the inherent relation of data, i.e., statistical and geometrical dependence, and easily causes overfitting. in order to deal with this issue, we propose an exclusivity enhanced (ee) unsupervised feature learning approach to improve the conventional ae. to the best of our knowledge, our research is the first to utilize such exclusivity concept to cooperate with feature extraction within ae. moreover, in this paper we also make some improvements to the stacked ae structure especially for the connection of different layers from decoders, this could be regarded as a weight initialization trial. the experimental results show that our proposed approach can achieve remarkable performance compared with other related methods.",,2019-03-30,,"['jingcai guo', 'song guo']"
290,1904.00173,asymptotic nonparametric statistical analysis of stationary time series,math.st cs.it cs.lg math.it stat.ml stat.th,"stationarity is a very general, qualitative assumption, that can be assessed on the basis of application specifics. it is thus a rather attractive assumption to base statistical analysis on, especially for problems for which less general qualitative assumptions, such as independence or finite memory, clearly fail. however, it has long been considered too general to allow for statistical inference to be made. one of the reasons for this is that rates of convergence, even of frequencies to the mean, are not available under this assumption alone. recently, it has been shown that, while some natural and simple problems such as homogeneity, are indeed provably impossible to solve if one only assumes that the data is stationary (or stationary ergodic), many others can be solved using rather simple and intuitive algorithms. the latter problems include clustering and change point estimation. in this volume i summarize these results. the emphasis is on asymptotic consistency, since this the strongest property one can obtain assuming stationarity alone. while for most of the problems for which a solution is found this solution is algorithmically realizable, the main objective in this area of research, the objective which is only partially attained, is to understand what is possible and what is not possible to do for stationary time series. the considered problems include homogeneity testing, clustering with respect to distribution, clustering with respect to independence, change-point estimation, identity testing, and the general question of composite hypotheses testing. for the latter problem, a topological criterion for the existence of a consistent test is presented. in addition, several open questions are discussed.",10.1007/978-3-030-12564-6,2019-03-30,,['daniil ryabko']
291,1904.00176,nonparametric density estimation for high-dimensional data - algorithms   and applications,stat.ml cs.lg stat.co,"density estimation is one of the central areas of statistics whose purpose is to estimate the probability density function underlying the observed data. it serves as a building block for many tasks in statistical inference, visualization, and machine learning. density estimation is widely adopted in the domain of unsupervised learning especially for the application of clustering. as big data become pervasive in almost every area of data sciences, analyzing high-dimensional data that have many features and variables appears to be a major focus in both academia and industry. high-dimensional data pose challenges not only from the theoretical aspects of statistical inference, but also from the algorithmic/computational considerations of machine learning and data analytics. this paper reviews a collection of selected nonparametric density estimation algorithms for high-dimensional data, some of them are recently published and provide interesting mathematical insights. the important application domain of nonparametric density estimation, such as { modal clustering}, are also included in this paper. several research directions related to density estimation and high-dimensional data analysis are suggested by the authors.",10.1002/wics.1461,2019-03-30,,"['zhipeng wang', 'david w. scott']"
292,1904.00197,exploiting sift descriptor for rotation invariant convolutional neural   network,cs.cv cs.lg stat.ml,"this paper presents a novel approach to exploit the distinctive invariant features in convolutional neural network. the proposed cnn model uses scale invariant feature transform (sift) descriptor instead of the max-pooling layer. max-pooling layer discards the pose, i.e., translational and rotational relationship between the low-level features, and hence unable to capture the spatial hierarchies between low and high level features. the sift descriptor layer captures the orientation and the spatial relationship of the features extracted by convolutional layer. the proposed sift descriptor cnn therefore combines the feature extraction capabilities of cnn model and rotation invariance of sift descriptor. experimental results on the mnist and fashionmnist datasets indicates reasonable improvements over conventional methods available in literature.",,2019-03-30,,"['abhay kumar', 'nishant jain', 'chirag singh', 'suraj tripathi']"
293,1904.00204,combining smoothing spline with conditional gaussian graphical model for   density and graph estimation,stat.me,"multivariate density estimation and graphical models play important roles in statistical learning. the estimated density can be used to construct a graphical model that reveals conditional relationships whereas a graphical structure can be used to build models for density estimation. our goal is to construct a consolidated framework that can perform both density and graph estimation. denote $\bm{z}$ as the random vector of interest with density function $f(\bz)$. splitting $\bm{z}$ into two parts, $\bm{z}=(\bm{x}^t,\bm{y}^t)^t$ and writing $f(\bz)=f(\bx)f(\by|\bx)$ where $f(\bx)$ is the density function of $\bm{x}$ and $f(\by|\bx)$ is the conditional density of $\bm{y}|\bm{x}=\bx$. we propose a semiparametric framework that models $f(\bx)$ nonparametrically using a smoothing spline anova (ss anova) model and $f(\by|\bx)$ parametrically using a conditional gaussian graphical model (cggm). combining flexibility of the ss anova model with succinctness of the cggm, this framework allows us to deal with high-dimensional data without assuming a joint gaussian distribution. we propose a backfitting estimation procedure for the cggm with a computationally efficient approach for selection of tuning parameters. we also develop a geometric inference approach for edge selection. we establish asymptotic convergence properties for both the parameter and density estimation. the performance of the proposed method is evaluated through extensive simulation studies and two real data applications.",,2019-03-30,,"['runfei luo', 'anna liu', 'yuedong wang']"
294,1904.00231,lane change decision-making through deep reinforcement learning with   rule-based constraints,cs.ro cs.ai cs.lg stat.ml,"autonomous driving decision-making is a great challenge due to the complexity and uncertainty of the traffic environment. combined with the rule-based constraints, a deep q-network (dqn) based method is applied for autonomous driving lane change decision-making task in this study. through the combination of high-level lateral decision-making and low-level rule-based trajectory modification, a safe and efficient lane change behavior can be achieved. with the setting of our state representation and reward function, the trained agent is able to take appropriate actions in a real-world-like simulator. the generated policy is evaluated on the simulator for 10 times, and the results demonstrate that the proposed rule-based dqn method outperforms the rule-based approach and the dqn method.",,2019-03-30,2019-04-01,"['junjie wang', 'qichao zhang', 'dongbin zhao', 'yaran chen']"
295,1904.00275,prediction model for semitransparent watercolor pigment mixtures using   deep learning with a dataset of transmittance and reflectance,cs.lg cs.gr stat.ml,"learning color mixing is difficult for novice painters. in order to support novice painters in learning color mixing, we propose a prediction model for semitransparent pigment mixtures and use its prediction results to create a smart palette system. such a system is constructed by first building a watercolor dataset with two types of color mixing data, indicated by transmittance and reflectance: incrementation of the same primary pigment and a mixture of two different pigments. next, we apply the collected data to a deep neural network to train a model for predicting the results of semitransparent pigment mixtures. finally, we constructed a smart palette that provides easily-followable instructions on mixing a target color with two primary pigments in real life: when users pick a pixel, an rgb color, from an image, the system returns its mixing recipe which indicates the two primary pigments being used and their quantities.",,2019-03-30,,"['mei-yun chen', 'ya-bo huang', 'sheng-ping chang', 'ming ouhyoung']"
296,1904.00326,medgcn: graph convolutional networks for multiple medical tasks,cs.lg cs.ai stat.ml,"laboratory testing and medication prescription are two of the most important routines in daily clinical practice. developing an artificial intelligence system that can automatically make lab test imputations and medication recommendations can save cost on potentially redundant lab tests and inform physicians in more effective prescription. we present an intelligent model that can automatically recommend the patients' medications based on their incomplete lab tests, and can even accurately estimate the lab values that have not been taken. we model the complex relations between multiple types of medical entities with their inherent features in a heterogeneous graph. then we learn a distributed representation for each entity in the graph based on graph convolutional networks to make the representations integrate information from multiple types of entities. since the entity representations incorporate multiple types of medical information, they can be used for multiple medical tasks. in our experiments, we construct a graph to associate patients, encounters, lab tests and medications, and conduct the two tasks: medication recommendation and lab test imputation. the experimental results demonstrate that our model can outperform the state-of-the-art models in both tasks.",,2019-03-30,,"['chengsheng mao', 'liang yao', 'yuan luo']"
297,1904.00340,cusum arl - conditional or unconditional?,stat.me,"the behavior of cusum charts depends strongly on how they are initialized. recent work has suggested that self-starting cusum methods retain some dependence on their very first readings, and introduced the concept of ""conditional average run length"" (carl) -- the average run length conditioned on the first few process readings -- as a result of which is it claimed that different practitioners using the same methodology could experience different arls because of the random differences in their earliest readings. we cast doubt on whether carl is relevant to practitioners who use self-starting methods and argue that the unconditional arl is the relevant measure there.",,2019-03-31,,"['f. lombard', 'd. m. hawkins']"
298,1904.00350,conversation model fine-tuning for classifying client utterances in   counseling dialogues,cs.cl cs.lg stat.ml,"the recent surge of text-based online counseling applications enables us to collect and analyze interactions between counselors and clients. a dataset of those interactions can be used to learn to automatically classify the client utterances into categories that help counselors in diagnosing client status and predicting counseling outcome. with proper anonymization, we collect counselor-client dialogues, define meaningful categories of client utterances with professional counselors, and develop a novel neural network model for classifying the client utterances. the central idea of our model, convmfit, is a pre-trained conversation model which consists of a general language model built from an out-of-domain corpus and two role-specific language models built from unlabeled in-domain dialogues. the classification result shows that convmfit outperforms state-of-the-art comparison models. further, the attention weights in the learned model confirm that the model finds expected linguistic patterns for each category.",,2019-03-31,,"['sungjoon park', 'donghyun kim', 'alice oh']"
299,1904.00364,small area estimation with linked data,stat.me,"in small area estimation data linkage can be used to combine values of the variableof interest from a national survey with values of auxiliary variables obtained from another source like a population register. linkage errors can induce bias when fitting regression models; moreover, they can create non-representative outliers in the linked data in addition to the presence of potential representative outliers. in this paper we adopt a secondary analyst's point view, assuming limited information is available on the linkage process, and we develop small area estimators based on linear mixed and linear m-quantile models to accommodate linked data containing a mix of both types of outliers. we illustrate the properties of these small area estimators, as well as estimators of their mean squared error, by means of model-based and design-based simulation experiments. these experiments show that the proposed predictors can lead to more efficient estimators when there is linkage error. furthermore, the proposed mean-squared error estimation methods appear to perform well.",,2019-03-31,,"['ray chambers', 'enrico fabrizi', 'nicola salvati']"
300,1904.00442,spamhmm: sparse mixture of hidden markov models for graph connected   entities,cs.lg stat.ml,"we propose a framework to model the distribution of sequential data coming from a set of entities connected in a graph with a known topology. the method is based on a mixture of shared hidden markov models (hmms), which are jointly trained in order to exploit the knowledge of the graph structure and in such a way that the obtained mixtures tend to be sparse. experiments in different application domains demonstrate the effectiveness and versatility of the method.",,2019-03-31,,"['diogo pernes', 'jaime s. cardoso']"
301,1904.00459,differentially private inference for binomial data,math.st cs.cr stat.th,"we derive uniformly most powerful (ump) tests for simple and one-sided hypotheses for a population proportion within the framework of differential privacy (dp), optimizing finite sample performance. we show that in general, dp hypothesis tests can be written in terms of linear constraints, and for exchangeable data can always be expressed as a function of the empirical distribution. using this structure, we prove a 'neyman-pearson lemma' for binomial data under dp, where the dp-ump only depends on the sample sum. our tests can also be stated as a post-processing of a random variable, whose distribution we coin ''truncated-uniform-laplace'' (tulap), a generalization of the staircase and discrete laplace distributions. furthermore, we obtain exact $p$-values, which are easily computed in terms of the tulap random variable.   using the above techniques, we show that our tests can be applied to give uniformly most accurate one-sided confidence intervals and optimal confidence distributions. we also derive uniformly most powerful unbiased (umpu) two-sided tests, which lead to uniformly most accurate unbiased (umau) two-sided confidence intervals. we show that our results can be applied to distribution-free hypothesis tests for continuous data. our simulation results demonstrate that all our tests have exact type i error, and are more powerful than current techniques.",,2019-03-31,,"['jordan awan', 'aleksandra slavkovic']"
302,1904.00516,summarizing event sequences with serial episodes: a statistical model   and an application,cs.lg stat.ml,"in this paper we address the problem of discovering a small set of frequent serial episodes from sequential data so as to adequately characterize or summarize the data. we discuss an algorithm based on the minimum description length (mdl) principle and the algorithm is a slight modification of an earlier method, called csc-2. we present a novel generative model for sequence data containing prominent pairs of serial episodes and, using this, provide some statistical justification for the algorithm. we believe this is the first instance of such a statistical justification for an mdl based algorithm for summarizing event sequence data. we then present a novel application of this data mining algorithm in text classification. by considering text documents as temporal sequences of words, the data mining algorithm can find a set of characteristic episodes for all the training data as a whole. the words that are part of these characteristic episodes could then be considered the only relevant words for the dictionary thus resulting in a considerably reduced feature vector dimension. we show, through simulation experiments using benchmark data sets, that the discovered frequent episodes can be used to achieve more than four-fold reduction in dictionary size without losing any classification accuracy.",,2019-03-31,,"['soumyajit mitra', 'p s sastry']"
303,1904.00521,adaptive ensemble learning of spatiotemporal processes with calibrated   predictive uncertainty: a bayesian nonparametric approach,stat.me,"ensemble learning is a mainstay in modern data science practice. conventional ensemble algorithms assign to base models a set of deterministic, constant model weights that (1) do not fully account for individual models' varying accuracy across data subgroups, nor (2) provide uncertainty estimates for the ensemble prediction. these shortcomings can yield predictions that are precise but biased, which can negatively impact the performance of the algorithm in real-word applications. in this work, we present an adaptive, probabilistic approach to ensemble learning using a transformed gaussian process as a prior for the ensemble weights. given input features, our method optimally combines base models based on their predictive accuracy in the feature space, and provides interpretable estimates of the uncertainty associated with both model selection, as reflected by the ensemble weights, and the overall ensemble predictions. furthermore, to ensure that this quantification of the model uncertainty is accurate, we propose additional machinery to non-parametrically model the ensemble's predictive cumulative density function (cdf) so that it is consistent with the empirical distribution of the data. we apply the proposed method to data simulated from a nonlinear regression model, and to generate a spatial prediction model and associated prediction uncertainties for fine particle levels in eastern massachusetts, usa.",,2019-03-31,,"['jeremiah zhe liu', 'john paisley', 'marianthi-anna kioumourtzoglou', 'brent a. coull']"
304,1904.00542,multi-task ordinal regression for jointly predicting the trustworthiness   and the leading political ideology of news media,cs.ir cs.lg stat.ml,"in the context of fake news, bias, and propaganda, we study two important but relatively under-explored problems: (i) trustworthiness estimation (on a 3-point scale) and (ii) political ideology detection (left/right bias on a 7-point scale) of entire news outlets, as opposed to evaluating individual articles. in particular, we propose a multi-task ordinal regression framework that models the two problems jointly. this is motivated by the observation that hyper-partisanship is often linked to low trustworthiness, e.g., appealing to emotions rather than sticking to the facts, while center media tend to be generally more impartial and trustworthy. we further use several auxiliary tasks, modeling centrality, hyperpartisanship, as well as left-vs.-right bias on a coarse-grained scale. the evaluation results show sizable performance gains by the joint models over models that target the problems in isolation.",,2019-03-31,,"['ramy baly', 'georgi karadzhov', 'abdelrhman saleh', 'james glass', 'preslav nakov']"
305,1904.00548,unsupervised contextual anomaly detection using joint deep variational   generative models,stat.ml cs.lg,a method for unsupervised contextual anomaly detection is proposed using a cross-linked pair of variational auto-encoders for assigning a normality score to an observation. the method enables a distinct separation of contextual from behavioral attributes and is robust to the presence of anomalous or novel contextual attributes. the method can be trained with data sets that contain anomalies without any special pre-processing.,,2019-03-31,,['yaniv shulman']
306,1904.00561,vine: visualizing statistical interactions in black box models,cs.lg stat.ml,"as machine learning becomes more pervasive, there is an urgent need for interpretable explanations of predictive models. prior work has developed effective methods for visualizing global model behavior, as well as generating local (instance-specific) explanations. however, relatively little work has addressed regional explanations - how groups of similar instances behave in a complex model, and the related issue of visualizing statistical feature interactions. the lack of utilities available for these analytical needs hinders the development of models that are mission-critical, transparent, and align with social goals. we present vine (visual interaction effects), a novel algorithm to extract and visualize statistical interaction effects in black box models. we also present a novel evaluation metric for visualizations in the interpretable ml space.",,2019-04-01,,['matthew britton']
307,1904.00562,deep clustering with intra-class distance constraint for hyperspectral   images,cs.lg eess.iv stat.ml,"the high dimensionality of hyperspectral images often results in the degradation of clustering performance. due to the powerful ability of deep feature extraction and non-linear feature representation, the clustering algorithm based on deep learning has become a hot research topic in the field of hyperspectral remote sensing. however, most deep clustering algorithms for hyperspectral images utilize deep neural networks as feature extractor without considering prior knowledge constraints that are suitable for clustering. to solve this problem, we propose an intra-class distance constrained deep clustering algorithm for high-dimensional hyperspectral images. the proposed algorithm constrains the feature mapping procedure of the auto-encoder network by intra-class distance so that raw images are transformed from the original high-dimensional space to the low-dimensional feature space that is more conducive to clustering. furthermore, the related learning process is treated as a joint optimization problem of deep feature extraction and clustering. experimental results demonstrate the intense competitiveness of the proposed algorithm in comparison with state-of-the-art clustering methods of hyperspectral images.",,2019-04-01,,"['jinguang sun', 'wanli wang', 'xian wei', 'li fang', 'xiaoliang tang', 'yusheng xu', 'hui yu', 'wei yao']"
308,1904.00575,a novel gan-based fault diagnosis approach for imbalanced industrial   time series,cs.lg stat.ml,"this paper proposes a novel fault diagnosis approach based on generative adversarial networks (gan) for imbalanced industrial time series where normal samples are much larger than failure cases. we combine a well-designed feature extractor with gan to help train the whole network. aimed at obtaining data distribution and hidden pattern in both original distinguishing features and latent space, the encoder-decoder-encoder three-sub-network is employed in gan, based on deep convolution generative adversarial networks (dcgan) but without tanh activation layer and only trained on normal samples. in order to verify the validity and feasibility of our approach, we test it on rolling bearing data from case western reserve university and further verify it on data collected from our laboratory. the results show that our proposed approach can achieve excellent performance in detecting faulty by outputting much larger evaluation scores.",,2019-04-01,,"['wenqian jiang', 'cheng cheng', 'beitong zhou', 'guijun ma', 'ye yuan']"
309,1904.00679,default bayesian model selection of constrained multivariate normal   linear models,stat.me,"a default bayes factor is proposed for evaluating multivariate normal linear models with competing sets of equality and order constraints on the parameters of interest. the default bayes factor is based on generalized fractional bayes methodology where different fractions are used for different observations and where the default prior is centered on the boundary of the constrained space under investigation. first, the method is fully automatic and therefore can be applied when prior information is weak or completely unavailable. second, using group specific fractions, the same amount of information is used from each group resulting in a minimally informative default prior having a matrix cauchy distribution. this results in a consistent default bayes factor. third, numerical computation can be done using parallelization which makes it computationally cheap. fourth, the evidence can be updated in a relatively simple manner when observing new data. fifth, the selection criterion can be applied relatively straightforwardly in the presence of missing data that are missing at random. finally the methodology can be used for default model selection and hypothesis testing of commonly used models such as (m)an(c)ova, (multivariate) multiple regression, or repeated measures.",,2019-04-01,,"['j. mulder', 'h. hoijtink', 'x. gu']"
310,1904.00689,defending against adversarial attacks by randomized diversification,cs.lg cs.cr stat.ml,"the vulnerability of machine learning systems to adversarial attacks questions their usage in many applications. in this paper, we propose a randomized diversification as a defense strategy. we introduce a multi-channel architecture in a gray-box scenario, which assumes that the architecture of the classifier and the training data set are known to the attacker. the attacker does not only have access to a secret key and to the internal states of the system at the test time. the defender processes an input in multiple channels. each channel introduces its own randomization in a special transform domain based on a secret key shared between the training and testing stages. such a transform based randomization with a shared key preserves the gradients in key-defined sub-spaces for the defender but it prevents gradient back propagation and the creation of various bypass systems for the attacker. an additional benefit of multi-channel randomization is the aggregation that fuses soft-outputs from all channels, thus increasing the reliability of the final score. the sharing of a secret key creates an information advantage to the defender. experimental evaluation demonstrates an increased robustness of the proposed method to a number of known state-of-the-art attacks.",,2019-04-01,,"['olga taran', 'shideh rezaeifar', 'taras holotyak', 'slava voloshynovskiy']"
311,1904.00690,customer churn prediction in telecom using machine learning and social   network analysis in big data platform,cs.cy cs.dc cs.lg cs.si stat.ml,"customer churn is a major problem and one of the most important concerns for large companies. due to the direct effect on the revenues of the companies, especially in the telecom field, companies are seeking to develop means to predict potential customer to churn. therefore, finding factors that increase customer churn is important to take necessary actions to reduce this churn. the main contribution of our work is to develop a churn prediction model which assists telecom operators to predict customers who are most likely subject to churn. the model developed in this work uses machine learning techniques on big data platform and builds a new way of features' engineering and selection. in order to measure the performance of the model, the area under curve (auc) standard measure is adopted, and the auc value obtained is 93.3%. another main contribution is to use customer social network in the prediction model by extracting social network analysis (sna) features. the use of sna enhanced the performance of the model from 84 to 93.3% against auc standard. the model was prepared and tested through spark environment by working on a large dataset created by transforming big raw data provided by syriatel telecom company. the dataset contained all customers' information over 9 months, and was used to train, test, and evaluate the system at syriatel. the model experimented four algorithms: decision tree, random forest, gradient boosted machine tree ""gbm"" and extreme gradient boosting ""xgboost"". however, the best results were obtained by applying xgboost algorithm. this algorithm was used for classification in this churn predictive model.",10.1186/s40537-019-0191-6,2019-04-01,,"['abdelrahim kasem ahmad', 'assef jafar', 'kadan aljoumaa']"
312,1904.00735,a comparative analysis of android malware,cs.cr cs.lg stat.ml,"in this paper, we present a comparative analysis of benign and malicious android applications, based on static features. in particular, we focus our attention on the permissions requested by an application. we consider both binary classification of malware versus benign, as well as the multiclass problem, where we classify malware samples into their respective families. our experiments are based on substantial malware datasets and we employ a wide variety of machine learning techniques, including decision trees and random forests, support vector machines, logistic model trees, adaboost, and artificial neural networks. we find that permissions are a strong feature and that by careful feature engineering, we can significantly reduce the number of features needed for highly accurate detection and classification.",,2019-01-20,,"['neeraj chavan', 'fabio di troia', 'mark stamp']"
313,1904.00737,defending via strategic ml selection,cs.cr cs.lg stat.ml,"the results of a learning process depend on the input data. there are cases in which an adversary can strategically tamper with the input data to affect the outcome of the learning process. while some datasets are difficult to attack, many others are susceptible to manipulation. a resourceful attacker can tamper with large portions of the dataset and affect them. an attacker can additionally strategically focus on a preferred subset of the attributes in the dataset to maximize the effectiveness of the attack and minimize the resources allocated to data manipulation. in light of this vulnerability, we introduce a solution according to which the defender implements an array of learners, and their activation is performed strategically. the defender computes the (game theoretic) strategy space and accordingly applies a dominant strategy where possible, and a nash-stable strategy otherwise. in this paper we provide the details of this approach. we analyze nash equilibrium in such a strategic learning environment, and demonstrate our solution by specific examples.",,2019-01-16,,"['eitan farchi', 'onn shehory', 'guy barash']"
314,1904.00741,fashion outfit generation for e-commerce,cs.cv cs.lg stat.ml,"combining items of clothing into an outfit is a major task in fashion retail. recommending sets of items that are compatible with a particular seed item is useful for providing users with guidance and inspiration, but is currently a manual process that requires expert stylists and is therefore not scalable or easy to personalise. we use a multilayer neural network fed by visual and textual features to learn embeddings of items in a latent style space such that compatible items of different types are embedded close to one another. we train our model using the asos outfits dataset, which consists of a large number of outfits created by professional stylists and which we release to the research community. our model shows strong performance in an offline outfit compatibility prediction task. we use our model to generate outfits and for the first time in this field perform an ab test, comparing our generated outfits to those produced by a baseline model which matches appropriate product types but uses no information on style. users approved of outfits generated by our model 21% and 34% more frequently than those generated by the baseline model for womenswear and menswear respectively.",,2019-03-18,,"['elaine m. bettaney', 'stephen r. hardwick', 'odysseas zisimopoulos', 'benjamin paul chamberlain']"
315,1904.00749,forecasting the volatilities of philippine stock exchange composite   index using the generalized autoregressive conditional heteroskedasticity   modeling,q-fin.st stat.co,"this study was conducted to find an appropriate statistical model to forecast the volatilities of psei using the model generalized autoregressive conditional heteroskedasticity (garch). using the r software, the log returns of psei is modeled using various arima models and with the presence of heteroskedasticity, the log returns was modeled using garch. based on the analysis, garch models are the most appropriate to use for the log returns of psei. among the selected garch models, garch (1,2) has the lowest aic value and also has the highest ll value implying that garch (1,2) is the best model for the log returns of psei.",,2019-02-21,,"['novy ann m. etac', 'roel f. ceballos']"
316,1904.00760,approximating cnns with bag-of-local-features models works surprisingly   well on imagenet,cs.cv cs.lg stat.ml,"deep neural networks (dnns) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. we here introduce a high-performance dnn architecture on imagenet whose decisions are considerably easier to explain. our model, a simple variant of the resnet-50 architecture called bagnet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. this strategy is closely related to the bag-of-feature (bof) models popular before the onset of deep learning and reaches a surprisingly high accuracy on imagenet (87.6% top-5 for 33 x 33 px features and alexnet performance for 17 x 17 px features). the constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. furthermore, the bagnets behave similar to state-of-the art deep neural networks such as vgg-16, resnet-152 or densenet-169 in terms of feature sensitivity, error distribution and interactions between image parts. this suggests that the improvements of dnns over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.",,2019-03-20,,"['wieland brendel', 'matthias bethge']"
317,1904.00761,neural speed reading with structural-jump-lstm,cs.cl cs.lg stat.ml,"recurrent neural networks (rnns) can model natural language by sequentially 'reading' input tokens and outputting a distributed representation of each token. due to the sequential nature of rnns, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. efforts to speed up this inference, known as 'neural speed reading', either ignore or skim over part of the input. we present structural-jump-lstm: the first neural speed reading model to both skip and jump text during inference. the model consists of a standard lstm and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word. a comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that structural-jump-lstm achieves the best overall floating point operations (flop) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla lstm that reads the whole text.",,2019-03-20,2019-04-02,"['christian hansen', 'casper hansen', 'stephen alstrup', 'jakob grue simonsen', 'christina lioma']"
318,1904.00762,affect in tweets using experts model,cs.ir cs.cl cs.lg stat.ml,"estimating the intensity of emotion has gained significance as modern textual inputs in potential applications like social media, e-retail markets, psychology, advertisements etc., carry a lot of emotions, feelings, expressions along with its meaning. however, the approaches of traditional sentiment analysis primarily focuses on classifying the sentiment in general (positive or negative) or at an aspect level(very positive, low negative, etc.) and cannot exploit the intensity information. moreover, automatically identifying emotions like anger, fear, joy, sadness, disgust etc., from text introduces challenging scenarios where single tweet may contain multiple emotions with different intensities and some emotions may even co-occur in some of the tweets. in this paper, we propose an architecture, experts model, inspired from the standard mixture of experts (moe) model. the key idea here is each expert learns different sets of features from the feature vector which helps in better emotion detection from the tweet. we compared the results of our experts model with both baseline results and top five performers of semeval-2018 task-1, affect in tweets (ait). the experimental results show that our proposed approach deals with the emotion detection problem and stands at top-5 results.",,2019-03-20,,"['subba reddy oota', 'adithya avvaru', 'mounika marreddy', 'radhika mamidi']"
319,1904.00763,part-based approximations for morphological operators using asymmetric   auto-encoders,cs.cv cs.lg math.st stat.ml stat.th,"this paper addresses the issue of building a part-based representation of a dataset of images. more precisely, we look for a non-negative, sparse decomposition of the images on a reduced set of atoms, in order to unveil a morphological and interpretable structure of the data. additionally, we want this decomposition to be computed online for any new sample that is not part of the initial dataset. therefore, our solution relies on a sparse, non-negative auto-encoder where the encoder is deep (for accuracy) and the decoder shallow (for interpretability). this method compares favorably to the state-of-the-art online methods on two datasets (mnist and fashion mnist), according to classical metrics and to a new one we introduce, based on the invariance of the representation to morphological dilation.",,2019-03-20,2019-04-03,"['bastien ponchon', 'santiago velasco-forero', 'samy blusseau', 'jesus angulo', 'isabelle bloch']"
320,1904.00764,3d human action analysis and recognition through glac descriptor on 2d   motion and static posture images,cs.cv cs.lg eess.iv stat.ml,"in this paper, we present an approach for identification of actions within depth action videos. first, we process the video to get motion history images (mhis) and static history images (shis) corresponding to an action video based on the use of 3d motion trail model (3dmtm). we then characterize the action video by extracting the gradient local auto-correlations (glac) features from the shis and the mhis. the two sets of features i.e., glac features from mhis and glac features from shis are concatenated to obtain a representation vector for action. finally, we perform the classification on all the action samples by using the l2-regularized collaborative representation classifier (l2-crc) to recognize different human actions in an effective way. we perform evaluation of the proposed method on three action datasets, msr-action3d, dha and utd-mhad. through experimental results, we observe that the proposed method performs superior to other approaches.",10.1007/s11042-019-7365-2,2019-03-19,,"['mohammad farhad bulbul', 'saiful islam', 'hazrat ali']"
321,1904.00768,concatenated feature pyramid network for instance segmentation,cs.cv cs.lg stat.ml,"low level features like edges and textures play an important role in accurately localizing instances in neural networks. in this paper, we propose an architecture which improves feature pyramid networks commonly used instance segmentation networks by incorporating low level features in all layers of the pyramid in an optimal and efficient way. specifically, we introduce a new layer which learns new correlations from feature maps of multiple feature pyramid levels holistically and enhances the semantic information of the feature pyramid to improve accuracy. our architecture is simple to implement in instance segmentation or object detection frameworks to boost accuracy. using this method in mask rcnn, our model achieves consistent improvement in precision on coco dataset with the computational overhead compared to the original feature pyramid network.",,2019-03-16,,"['yongqing sun', 'pranav shenoy k p', 'jun shimamura', 'atsushi sagata']"
322,1904.00770,netherlands dataset: a new public dataset for machine learning in   seismic interpretation,cs.lg cs.cv physics.geo-ph stat.ml,"machine learning and, more specifically, deep learning algorithms have seen remarkable growth in their popularity and usefulness in the last years. this is arguably due to three main factors: powerful computers, new techniques to train deeper networks and larger datasets. although the first two are readily available in modern computers and ml libraries, the last one remains a challenge for many domains. it is a fact that big data is a reality in almost all fields nowadays, and geosciences are not an exception. however, to achieve the success of general-purpose applications such as imagenet - for which there are +14 million labeled images for 1000 target classes - we not only need more data, we need more high-quality labeled data. when it comes to the oil&gas industry, confidentiality issues hamper even more the sharing of datasets. in this work, we present the netherlands interpretation dataset, a contribution to the development of machine learning in seismic interpretation. the netherlands f3 dataset acquisition was carried out in the north sea, netherlands offshore. the data is publicly available and contains pos-stack data, 8 horizons and well logs of 4 wells. for the purposes of our machine learning tasks, the original dataset was reinterpreted, generating 9 horizons separating different seismic facies intervals. the interpreted horizons were used to generate approximatelly 190,000 labeled images for inlines and crosslines. finally, we present two deep learning applications in which the proposed dataset was employed and produced compelling results.",,2019-03-26,,"['reinaldo mozart silva', 'lais baroni', 'rodrigo s. ferreira', 'daniel civitarese', 'daniela szwarcman', 'emilio vital brazil']"
323,1904.00771,training multi-speaker neural text-to-speech systems using   speaker-imbalanced speech corpora,eess.as cs.cl cs.sd stat.ml,"when the available data of a target speaker is insufficient to train a high quality speaker-dependent neural text-to-speech (tts) system, we can combine data from multiple speakers and train a multi-speaker tts model instead. many studies have shown that neural multi-speaker tts model trained with a small amount data from multiple speakers combined can generate synthetic speech with better quality and stability than a speaker-dependent one. however when the amount of data from each speaker is highly unbalanced, the best approach to make use of the excessive data remains unknown. our experiments showed that simply combining all available data from every speaker to train a multi-speaker model produces better than or at least similar performance to its speaker-dependent counterpart. moreover by using an ensemble multi-speaker model, in which each subsystem is trained on a subset of available data, we can further improve the quality of the synthetic speech especially for underrepresented speakers whose training data is limited.",,2019-04-01,2019-04-07,"['hieu-thi luong', 'xin wang', 'junichi yamagishi', 'nobuyuki nishizawa']"
324,1904.00785,question embeddings based on shannon entropy: solving intent   classification task in goal-oriented dialogue system,cs.cl stat.ml,"question-answering systems and voice assistants are becoming major part of client service departments of many organizations, helping them to reduce the labor costs of staff. in many such systems, there is always natural language understanding module that solves intent classification task. this task is complicated because of its case-dependency - every subject area has its own semantic kernel. the state of art approaches for intent classification are different machine learning and deep learning methods that use text vector representations as input. the basic vector representation models such as bag of words and tf-idf generate sparse matrixes, which are becoming very big as the amount of input data grows. modern methods such as word2vec and fasttext use neural networks to evaluate word embeddings with fixed dimension size. as we are developing a question-answering system for students and enrollees of the perm national research polytechnic university, we have faced the problem of user's intent detection. the subject area of our system is very specific, that is why there is a lack of training data. this aspect makes intent classification task more challenging for using state of the art deep learning methods. in this paper, we propose an approach of the questions embeddings representation based on calculation of shannon entropy.the goal of the approach is to produce low dimensional question vectors as neural approaches do and to outperform related methods, described above in condition of small dataset. we evaluate and compare our model with existing ones using logistic regression and dataset that contains questions asked by students and enrollees. the data is labeled into six classes. experimental comparison of proposed approach and other models revealed that proposed model performed better in the given task.",10.25673/13485,2019-03-25,,"['aleksandr perevalov', 'daniil kurushin', 'rustam faizrakhmanov', 'farida khabibrakhmanova']"
325,1904.00791,dsl: discriminative subgraph learning via sparse self-representation,cs.lg stat.ml,"the goal in network state prediction (nsp) is to classify the global state (label) associated with features embedded in a graph. this graph structure encoding feature relationships is the key distinctive aspect of nsp compared to classical supervised learning. nsp arises in various applications: gene expression samples embedded in a protein-protein interaction (ppi) network, temporal snapshots of infrastructure or sensor networks, and fmri coherence network samples from multiple subjects to name a few. instances from these domains are typically ``wide'' (more features than samples), and thus, feature sub-selection is required for robust and generalizable prediction. how to best employ the network structure in order to learn succinct connected subgraphs encompassing the most discriminative features becomes a central challenge in nsp. prior work employs connected subgraph sampling or graph smoothing within optimization frameworks, resulting in either large variance of quality or weak control over the connectivity of selected subgraphs.   in this work we propose an optimization framework for discriminative subgraph learning (dsl) which simultaneously enforces (i) sparsity, (ii) connectivity and (iii) high discriminative power of the resulting subgraphs of features. our optimization algorithm is a single-step solution for the nsp and the associated feature selection problem. it is rooted in the rich literature on maximal-margin optimization, spectral graph methods and sparse subspace self-representation. dsl simultaneously ensures solution interpretability and superior predictive power (up to 16% improvement in challenging instances compared to baselines), with execution times up to an hour for large instances.",,2019-03-24,,"['lin zhang', 'petko bogdanov']"
326,1904.00805,a convolutional neural network for language-agnostic source code   summarization,cs.cl cs.lg stat.ml,"descriptive comments play a crucial role in the software engineering process. they decrease development time, enable better bug detection, and facilitate the reuse of previously written code. however, comments are commonly the last of a software developer's priorities and are thus either insufficient or missing entirely. automatic source code summarization may therefore have the ability to significantly improve the software development process. we introduce a novel encoder-decoder model that summarizes source code, effectively writing a comment to describe the code's functionality. we make two primary innovations beyond current source code summarization models. first, our encoder is fully language-agnostic and requires no complex input preprocessing. second, our decoder has an open vocabulary, enabling it to predict any word, even ones not seen in training. we demonstrate results comparable to state-of-the-art methods on a single-language data set and provide the first results on a data set consisting of multiple programming languages.",,2019-03-29,,"['jessica moore', 'ben gelman', 'david slater']"
327,1904.00824,training object detectors on synthetic images containing reflecting   materials,cs.cv cs.gr stat.ml,"one of the grand challenges of deep learning is the requirement to obtain large labeled training data sets. while synthesized data sets can be used to overcome this challenge, it is important that these data sets close the reality gap, i.e., a model trained on synthetic image data is able to generalize to real images. whereas, the reality gap can be considered bridged in several application scenarios, training on synthesized images containing reflecting materials requires further research. since the appearance of objects with reflecting materials is dominated by the surrounding environment, this interaction needs to be considered during training data generation. therefore, within this paper we examine the effect of reflecting materials in the context of synthetic image generation for training object detectors. we investigate the influence of rendering approach used for image synthesis, the effect of domain randomization, as well as the amount of used training data. to be able to compare our results to the state-of-the-art, we focus on indoor scenes as they have been investigated extensively. within this scenario, bathroom furniture is a natural choice for objects with reflecting materials, for which we report our findings on real and synthetic testing data.",,2019-03-29,,"['sebastian hartwig', 'timo ropinski']"
328,1904.00864,tree search network for sparse regression,cs.lg eess.sp stat.ml,"we consider the classical sparse regression problem of recovering a sparse signal $x_0$ given a measurement vector $y = \phi x_0+w$. we propose a tree search algorithm driven by the deep neural network for sparse regression (tsn). tsn improves the signal reconstruction performance of the deep neural network designed for sparse regression by performing a tree search with pruning. it is observed in both noiseless and noisy cases, tsn recovers synthetic and real signals with lower complexity than a conventional tree search and is superior to existing algorithms by a large margin for various types of the sensing matrix $\phi$, widely used in sparse regression.",,2019-04-01,,"['kyung-su kim', 'sae-young chung']"
329,1904.00917,asymptotic independence and support detection techniques for   heavy-tailed multivariate data,math.st stat.th,"one of the central objectives of modern risk management is to find a set of risks where the probability of multiple simultaneous catastrophic events is negligible. that is, risks are taken only when their joint behavior seems sufficiently independent. this paper aims to help to identify asymptotically independent risks by providing additional tools for describing dependence structures of multiple risks when the individual risks can obtain very large values.   the study is performed in the setting of multivariate regular variation. we show how asymptotic independence is connected to properties of the support of the angular measure and present an asymptotically consistent estimator of the support. the estimator generalizes to any dimension $n\geq 2$ and requires no prior knowledge of the support. the validity of the support estimate can be rigorously tested under mild assumptions by an asymptotically normal test statistic.",,2019-04-01,,"['jaakko lehtomaa', 'sidney resnick']"
330,1904.00935,style-analyzer: fixing code style inconsistencies with interpretable   unsupervised algorithms,cs.lg cs.se stat.ml,"source code reviews are manual, time-consuming, and expensive. human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. they are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. some machine learning-based solutions exist, but they remain uninterpretable black boxes. this paper introduces style-analyzer, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. style-analyzer is built on top of our novel assisted code review framework, lookout. it accurately mines the formatting style of each analyzed git repository and expresses the found format patterns with compact human-readable rules. style-analyzer can then suggest style inconsistency fixes in the form of code review comments. we evaluate the output quality and practical relevance of style-analyzer by demonstrating that it can reproduce the original style with high precision, measured on 19 popular javascript projects, and by showing that it yields promising results in fixing real style mistakes. style-analyzer includes a web application to visualize how the rules are triggered. we release style-analyzer as a reusable and extendable open source software package on github for the benefit of the community.",,2019-04-01,,"['vadim markovtsev', 'waren long', 'hugo mougard', 'konstantin slavnov', 'egor bulychev']"
331,1904.00938,lutnet: rethinking inference in fpga soft logic,cs.lg stat.ml,"research has shown that deep neural networks contain significant redundancy, and that high classification accuracies can be achieved even when weights and activations are quantised down to binary values. network binarisation on fpgas greatly increases area efficiency by replacing resource-hungry multipliers with lightweight xnor gates. however, an fpga's fundamental building block, the k-lut, is capable of implementing far more than an xnor: it can perform any k-input boolean operation. inspired by this observation, we propose lutnet, an end-to-end hardware-software framework for the construction of area-efficient fpga-based neural network accelerators using the native luts as inference operators. we demonstrate that the exploitation of lut flexibility allows for far heavier pruning than possible in prior works, resulting in significant area savings while achieving comparable accuracy. against the state-of-the-art binarised neural network implementation, we achieve twice the area efficiency for several standard network models when inferencing popular datasets. we also demonstrate that even greater energy efficiency improvements are obtainable.",,2019-04-01,,"['erwei wang', 'james j. davis', 'peter y. k. cheung', 'george a. constantinides']"
332,1904.00942,controlling for biasing signals in images for prognostic models:   survival predictions for lung cancer with deep learning,cs.lg stat.ml,"deep learning has shown remarkable results for image analysis and is expected to aid individual treatment decisions in health care. to achieve this, deep learning methods need to be promoted from the level of mere associations to being able to answer causal questions. we present a scenario with real-world medical images (ct-scans of lung cancers) and simulated outcome data. through the sampling scheme, the images contain two distinct factors of variation that represent a collider and a prognostic factor. we show that when this collider can be quantified, unbiased individual prognosis predictions are attainable with deep learning. this is achieved by (1) setting a dual task for the network to predict both the outcome and the collider and (2) enforcing independence of the activation distributions of the last layer with ordinary least squares. our method provides an example of combining deep learning and structural causal models for unbiased individual prognosis predictions.",,2019-04-01,,"['wouter a. c. van amsterdam', 'marinus j. c. eijkemans']"
333,1904.01002,on the vulnerability of cnn classifiers in eeg-based bcis,cs.lg cs.cr stat.ml,"deep learning has been successfully used in numerous applications because of its outstanding performance and the ability to avoid manual feature engineering. one such application is electroencephalogram (eeg) based brain-computer interface (bci), where multiple convolutional neural network (cnn) models have been proposed for eeg classification. however, it has been found that deep learning models can be easily fooled with adversarial examples, which are normal examples with small deliberate perturbations. this paper proposes an unsupervised fast gradient sign method (ufgsm) to attack three popular cnn classifiers in bcis, and demonstrates its effectiveness. we also verify the transferability of adversarial examples in bcis, which means we can perform attacks even without knowing the architecture and parameters of the target models, or the datasets they were trained on. to our knowledge, this is the first study on the vulnerability of cnn classifiers in eeg-based bcis, and hopefully will trigger more attention on the security of bci systems.",,2019-03-31,,"['xiao zhang', 'dongrui wu']"
334,1904.01058,tree boosted varying coefficient models,stat.me,"this paper investigates the integration of gradient boosted decision trees and varying coefficient models. we introduce the tree boosted varying coefficient framework which justifies the implementation of decision tree boosting as the nonparametric effect modifiers in varying coefficient models. this framework requires no structural assumptions in the space containing the varying coefficient covariates, is easy to implement, and keeps a balance between model complexity and interpretability. to provide statistical guarantees, we prove the asymptotic consistency of the proposed method under the regression settings with $l^2$ loss. we further conduct a thorough empirical study to show that the proposed method is capable of providing accurate predictions as well as intelligible visual explanations.",,2019-04-01,,"['yichen zhou', 'giles hooker']"
335,1904.01070,multimodal sparse classifier for adolescent brain age prediction,cs.lg cs.ne q-bio.nc stat.ml,"the study of healthy brain development helps to better understand the brain transformation and brain connectivity patterns which happen during childhood to adulthood. this study presents a sparse machine learning solution across whole-brain functional connectivity (fc) measures of three sets of data, derived from resting state functional magnetic resonance imaging (rs-fmri) and task fmri data, including a working memory n-back task (nb-fmri) and an emotion identification task (em-fmri). these multi-modal image data are collected on a sample of adolescents from the philadelphia neurodevelopmental cohort (pnc) for the prediction of brain ages. due to extremely large variable-to-instance ratio of pnc data, a high dimensional matrix with several irrelevant and highly correlated features is generated and hence a pattern learning approach is necessary to extract significant features. we propose a sparse learner based on the residual errors along the estimation of an inverse problem for the extreme learning machine (elm) neural network. the purpose of the approach is to overcome the overlearning problem through pruning of several redundant features and their corresponding output weights. the proposed multimodal sparse elm classifier based on residual errors (res-elm) is highly competitive in terms of the classification accuracy compared to its counterparts such as conventional elm, and sparse bayesian learning elm.",,2019-04-01,,"['peyman hosseinzadeh kassani', 'alexej gossmann', 'yu-ping wang']"
336,1904.01083,"deepcloud. the application of a data-driven, generative model in design",cs.lg stat.ml,"generative systems have a significant potential to synthesize innovative design alternatives. still, most of the common systems that have been adopted in design require the designer to explicitly define the specifications of the procedures and in some cases the design space. in contrast, a generative system could potentially learn both aspects through processing a database of existing solutions without the supervision of the designer. to explore this possibility, we review recent advancements of generative models in machine learning and current applications of learning techniques in design. then, we describe the development of a data-driven generative system titled deepcloud. it combines an autoencoder architecture for point clouds with a web-based interface and analog input devices to provide an intuitive experience for data-driven generation of design alternatives. we delineate the implementation of two prototypes of deepcloud, their contributions, and potentials for generative design.",,2019-04-01,,"['ardavan bidgoli', 'pedro veloso']"
337,1904.01116,gene-based association analysis for bivariate time-to-event data through   functional regression with copula models,stat.ap,"several gene-based association tests for time-to-event traits have been proposed recently, to detect whether a gene region (containing multiple variants), as a set, is associated with the survival outcome. however, for bivariate survival outcomes, to the best of our knowledge, there is no statistical method that can be directly applied for gene-based association analysis. motivated by a genetic study to discover gene regions associated with the progression of a bilateral eye disease, age-related macular degeneration (amd), we implement a novel functional regression method under the copula framework. specifically, the effects of variants within a gene region are modeled through a functional linear model, which then contributes to the marginal survival functions within the copula. generalized score test and likelihood ratio test statistics are derived to test for the association between bivariate survival traits and the genetic region. extensive simulation studies are conducted to evaluate the type-i error control and power performance of the proposed approach, with comparisons to several existing methods for a single survival trait, as well as the marginal cox functional regression model using the robust sandwich estimator for bivariate survival traits. finally, we apply our method to a large amd study, the age-related eye disease study (areds), to identify gene regions that are associated with amd progression.",,2019-04-01,,"['yue wei', 'yi liu', 'wei chen', 'ying ding']"
338,1904.01125,the impact of extraneous variables on the performance of recurrent   neural network models in clinical tasks,stat.ml cs.lg,"electronic medical records (emr) are a rich source of patient information, including measurements reflecting physiologic signs and administered therapies. identifying which variables are useful in predicting clinical outcomes can be challenging. advanced algorithms such as deep neural networks were designed to process high-dimensional inputs containing variables in their measured form, thus bypass separate feature selection or engineering steps. we investigated the effect of extraneous input variables on the predictive performance of recurrent neural networks (rnn) by including in the input vector extraneous variables randomly drawn from theoretical and empirical distributions. rnn models using different input vectors (emr variables; emr and extraneous variables; extraneous variables only) were trained to predict three clinical outcomes: in-icu mortality, 72-hour icu re-admission, and 30-day icu-free days. the measured degradations of the rnn's predictive performance with the addition of extraneous variables to emr variables were negligible.",,2019-04-01,,"['eugene laksana', 'melissa aczon', 'long ho', 'cameron carlin', 'david ledbetter', 'randall wetzel']"
339,1904.01127,cyberthreat detection from twitter using deep neural networks,cs.lg cs.cr cs.si stat.ml,"to be prepared against cyberattacks, most organizations resort to security information and event management systems to monitor their infrastructures. these systems depend on the timeliness and relevance of the latest updates, patches and threats provided by cyberthreat intelligence feeds. open source intelligence platforms, namely social media networks such as twitter, are capable of aggregating a vast amount of cybersecurity-related sources. to process such information streams, we require scalable and efficient tools capable of identifying and summarizing relevant information for specified assets. this paper presents the processing pipeline of a novel tool that uses deep neural networks to process cybersecurity information received from twitter. a convolutional neural network identifies tweets containing security-related information relevant to assets in an it infrastructure. then, a bidirectional long short-term memory network extracts named entities from these tweets to form a security alert or to fill an indicator of compromise. the proposed pipeline achieves an average 94% true positive rate and 91% true negative rate for the classification task and an average f1-score of 92% for the named entity recognition task, across three case study infrastructures.",,2019-04-01,,"['nuno dionísio', 'fernando alves', 'pedro m. ferreira', 'alysson bessani']"
340,1904.01128,analysis of large heterogeneous repairable system reliability data with   static system attributes and dynamic sensor measurement in big data   environment,stat.ap,"in big data environment, one pressing challenge facing engineers is to perform reliability analysis for a large fleet of heterogeneous repairable systems with covariates. in addition to static covariates, which include time-invariant system attributes such as nominal operating conditions, geo-locations, etc., the recent advances of sensing technologies have also made it possible to obtain dynamic sensor measurement of system operating and environmental conditions. as a common practice in the big data environment, the massive reliability data are typically stored in some distributed storage systems. leveraging the power of modern statistical learning, this paper investigates a statistical approach which integrates the random forests algorithm and the classical data analysis methodologies for repairable system reliability, such as the nonparametric estimator for the mean cumulative function and the parametric models based on the nonhomogeneous poisson process. we show that the proposed approach effectively addresses some common challenges arising from practice, including system heterogeneity, covariate selection, model specification and data locality due to the distributed data storage. the large sample properties as well as the uniform consistency of the proposed estimator is established. two numerical examples and a case study are presented to illustrate the application of the proposed approach. the strengths of the proposed approach are demonstrated by comparison studies.",,2019-04-01,,"['xiao liu', 'rong pan']"
341,1904.01139,generative predecessor models for sample-efficient imitation learning,cs.lg stat.ml,"we propose generative predecessor models for imitation learning (gpril), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. we show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. we derive this approach from first principles and compare it empirically to a state-of-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot.",,2019-04-01,,"['yannick schroecker', 'mel vecerik', 'jonathan scholz']"
342,1904.01156,learning mixtures of smooth product distributions: identifiability and   algorithm,eess.sp cs.lg stat.ml,"we study the problem of learning a mixture model of non-parametric product distributions. the problem of learning a mixture model is that of finding the component distributions along with the mixing weights using observed samples generated from the mixture. the problem is well-studied in the parametric setting, i.e., when the component distributions are members of a parametric family -- such as gaussian distributions. in this work, we focus on multivariate mixtures of non-parametric product distributions and propose a two-stage approach which recovers the component distributions of the mixture under a smoothness condition. our approach builds upon the identifiability properties of the canonical polyadic (low-rank) decomposition of tensors, in tandem with fourier and shannon-nyquist sampling staples from signal processing. we demonstrate the effectiveness of the approach on synthetic and real datasets.",,2019-04-01,,"['nikos kargas', 'nicholas d. sidiropoulos']"
343,1904.01184,towards efficient and unbiased implementation of lipschitz continuity in   gans,cs.lg stat.ml,"lipschitz continuity recently becomes popular in generative adversarial networks (gans). it was observed that the lipschitz regularized discriminator leads to improved training stability and sample quality. the mainstream implementations of lipschitz continuity include gradient penalty and spectral normalization. in this paper, we demonstrate that gradient penalty introduces undesired bias, while spectral normalization might be over restrictive. we accordingly propose a new method which is efficient and unbiased. our experiments verify our analysis and show that the proposed method is able to achieve successful training in various situations where gradient penalty and spectral normalization fail.",,2019-04-01,,"['zhiming zhou', 'jian shen', 'yuxuan song', 'weinan zhang', 'yong yu']"
344,1904.01202,non-smooth backfitting for excess risk additive regression model with   two survival time-scales,stat.me,we present a new backfitting algorithm estimating the complex structured non-parametric survival model of scheike (2001) without having to use smoothing. the considered model is a non-parametric survival model with two time-scales that are equivalent up to a constant that varies over the subjects. covariate effects are modelled linearly on each time scale by additive aalen models. estimators of the cumulative intensities on the two time-scales are suggested by solving local estimating equations jointly on the two time-scales. we are able to estimate the cumulative intensities solving backfitting estimating equations without using smoothing methods and we provide large sample properties and simultaneous confidence bands. the model is applied to data on myocardial infarction providing a separation of the two effects stemming from time since diagnosis and age.,,2019-04-01,,"['munir hiabu', 'jens p. nielsen', 'thomas h. scheike']"
345,1904.01209,fence gan: towards better anomaly detection,cs.lg stat.ml,"anomaly detection is a classical problem where the aim is to detect anomalous data that do not belong to the normal data distribution. current state-of-the-art methods for anomaly detection on complex high-dimensional data are based on the generative adversarial network (gan). however, the traditional gan loss is not directly aligned with the anomaly detection objective: it encourages the distribution of the generated samples to overlap with the real data and so the resulting discriminator has been found to be ineffective as an anomaly detector. in this paper, we propose simple modifications to the gan loss such that the generated samples lie at the boundary of the real data distribution. with our modified gan loss, our anomaly detection method, called fence gan (fgan), directly uses the discriminator score as an anomaly threshold. our experimental results using the mnist, cifar10 and kdd99 datasets show that fence gan yields the best anomaly classification accuracy compared to state-of-the-art methods.",,2019-04-02,,"['cuong phuc ngo', 'amadeus aristo winarto', 'connie kou khor li', 'sojeong park', 'farhan akram', 'hwee kuan lee']"
346,1904.01214,enhancement of energy-based swing-up controller via entropy search,cs.lg cs.sy math.oc stat.ml,"an energy based approach for stabilizing a mechanical system has offered a simple yet powerful control scheme. however, since it does not impose such strong constraints on parameter space of the controller, finding appropriate parameter values for an optimal controller is known to be hard. this paper intends to generate an optimal energy-based controller for swinging up a rotary inverted pendulum, also known as the furuta pendulum, by applying the bayesian optimization called entropy search. simulations and experiments show that the optimal controller has an improved performance compared to a nominal controller for various initial conditions.",,2019-04-02,2019-04-03,"['chang sik lee', 'dong eui chang']"
347,1904.01269,experiments on open-set speaker identification with discriminatively   trained neural networks,cs.lg cs.sd eess.as stat.ml,this paper presents a study on discriminative artificial neural network classifiers in the context of open-set speaker identification. both 2-class and multi-class architectures are tested against the conventional gaussian mixture model based classifier on enrolled speaker sets of different sizes. the performance evaluation shows that the multi-class neural network system has superior performance for large population sizes.,,2019-04-02,,"['stefano imoscopi', 'volodya grancharov', 'sigurdur sverrisson', 'erlendur karlsson', 'harald pobloth']"
348,1904.01320,bivariate change point detection: joint detection of changes in   expectation and variance,stat.me math.st stat.th,"a method for change point detection is proposed. in a sequence of independent and piecewise identically distributed random variables we aim at detecting both, changes in the expectation as well as changes in the variance. we propose a statistical test for the null hypothesis of the absence of change points, and an algorithm for change point detection. for that we exploit the joint dynamics of the mean and the empirical variance in the context of bivariate moving sum statistics. the joint consideration helps to improve change point inference as compared to separate univariate approaches. we infer on the effects, i.e., on the strength and the type of the changes, with confidence. non-parametric methodology allows for a high variety of data to be analyzed. a multi-scale aspect addresses the detection of complex patterns in change points and effects. we demonstrate the performance through theoretical results and simulations studies. a companion r-package jcp (available on cran) is discussed.",,2019-04-02,2019-04-07,['michael messer']
349,1904.01334,correlated parameters to accurately measure uncertainty in deep neural   networks,cs.lg cs.cv stat.ml,"in this article a novel approach for training deep neural networks using bayesian techniques is presented. the bayesian methodology allows for an easy evaluation of model uncertainty and additionally is robust to overfitting. these are commonly the two main problems classical, i.e. non-bayesian, architectures have to struggle with. the proposed approach applies variational inference in order to approximate the intractable posterior distribution. in particular, the variational distribution is defined as product of multiple multivariate normal distributions with tridiagonal covariance matrices. each single normal distribution belongs either to the weights, or to the biases corresponding to one network layer. the layer-wise a posteriori variances are defined based on the corresponding expectation values and further the correlations are assumed to be identical. therefore, only a few additional parameters need to be optimized compared to non-bayesian settings. the novel approach is successfully evaluated on basis of the popular benchmark datasets mnist and cifar-10.",,2019-04-02,,"['konstantin posch', 'jürgen pilz']"
350,1904.01340,unsupervised training of a deep clustering model for multichannel blind   source separation,cs.lg eess.sp stat.ml,"we propose a training scheme to train neural network-based source separation algorithms from scratch when parallel clean data is unavailable. in particular, we demonstrate that an unsupervised spatial clustering algorithm is sufficient to guide the training of a deep clustering system. we argue that previous work on deep clustering requires strong supervision and elaborate on why this is a limitation. we demonstrate that (a) the single-channel deep clustering system trained according to the proposed scheme alone is able to achieve a similar performance as the multi-channel teacher in terms of word error rates and (b) initializing the spatial clustering approach with the deep clustering result yields a relative word error rate reduction of 26 % over the unsupervised teacher.",,2019-04-02,,"['lukas drude', 'daniel hasenklever', 'reinhold haeb-umbach']"
351,1904.01341,looking back at labels: a class based domain adaptation technique,cs.lg cs.cv stat.ml,"in this paper, we solve the problem of adapting classifiers across domains. we consider the problem of domain adaptation for multi-class classification where we are provided a labeled set of examples in a source dataset and we are provided a target dataset with no supervision. in this setting, we propose an adversarial discriminator based approach. while the approach based on adversarial discriminator has been previously proposed; in this paper, we present an informed adversarial discriminator. our observation relies on the analysis that shows that if the discriminator has access to all the information available including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structure adapted space. using this formulation, we obtain state-of-the-art results for the standard evaluation on benchmark datasets. we further provide detailed analysis which shows that using all the labeled information results in an improved domain adaptation.",,2019-04-02,,"['vinod kumar kurmi', 'vinay p. namboodiri']"
352,1904.01367,why resnet works? residuals generalize,stat.ml cs.lg,"residual connections significantly boost the performance of deep neural networks. however, there are few theoretical results that address the influence of residuals on the hypothesis complexity and the generalization ability of deep neural networks. this paper studies the influence of residual connections on the hypothesis complexity of the neural network in terms of the covering number of its hypothesis space. we prove that the upper bound of the covering number is the same as chain-like neural networks, if the total numbers of the weight matrices and nonlinearities are fixed, no matter whether they are in the residuals or not. this result demonstrates that residual connections may not increase the hypothesis complexity of the neural network compared with the chain-like counterpart. based on the upper bound of the covering number, we then obtain an $\mathcal o(1 / \sqrt{n})$ margin-based multi-class generalization bound for resnet, as an exemplary case of any deep neural network with residual connections. generalization guarantees for similar state-of-the-art neural network architectures, such as densenet and resnext, are straight-forward. from our generalization bound, a practical implementation is summarized: to approach a good generalization ability, we need to use regularization terms to control the magnitude of the norms of weight matrices not to increase too much, which justifies the standard technique of weight decay.",,2019-04-02,,"['fengxiang he', 'tongliang liu', 'dacheng tao']"
353,1904.01378,an adapted geographically weighted lasso(ada-gwl) model for estimating   metro ridership,stat.ap,"ridership estimation at station level plays a critical role in metro transportation planning. among various existing ridership estimation methods, direct demand model has been recognized as an effective approach. however, existing direct demand models including geographically weighted regression (gwr) have rarely included local model selection in ridership estimation. in practice, acquiring insights into metro ridership under multiple influencing factors from a local perspective is important for passenger volume management and transportation planning operations adapting to local conditions. in this study, we propose an adapted geographically weighted lasso (ada-gwl) framework for modelling metro ridership, which performs regression-coefficient shrinkage and local model selection. it takes metro network connection intermedia into account and adopts network-based distance metric instead of euclidean-based distance metric, making it so-called adapted to the context of metro networks. the real-world case of shenzhen metro is used to validate the superiority of our proposed model. the results show that the ada-gwl model performs the best compared with the global model (ordinary least square (ols), gwr, gwr calibrated with network-based distance metric and gwl in terms of estimation error of the dependent variable and goodness-of-fit. through understanding the variation of each coefficient across space (elasticities) and variables selection of each station, it provides more realistic conclusions based on local analysis. besides, through clustering analysis of the stations according to the regression coefficients, clusters' functional characteristics are found to be in compliance with the facts of the functional land use policy of shenzhen. these results of the proposed ada-gwl model demonstrate a great spatial explanatory power in transportation planning.",,2019-04-02,,"['yuxin he', 'yang zhao', 'kwok leung tsui']"
354,1904.01383,can we trust bayesian uncertainty quantification from gaussian process   priors with squared exponential covariance kernel?,math.st stat.th,"we investigate the frequentist coverage properties of credible sets resulting in from gaussian process priors with squared exponential covariance kernel. first we show that by selecting the scaling hyper-parameter using the maximum marginal likelihood estimator in the (slightly modified) squared exponential covariance kernel the corresponding credible sets will provide overconfident, misleading uncertainty statements for a large, representative subclass of the functional parameters in context of the gaussian white noise model. then we show that by either blowing up the credible sets with a logarithmic factor or modifying the maximum marginal likelihood estimator with a logarithmic term one can get reliable uncertainty statement and adaptive size of the credible sets under some additional restriction. finally we demonstrate on a numerical study that the derived negative and positive results extend beyond the gaussian white noise model to the nonparametric regression and classification models for small sample sizes as well.",,2019-04-02,,"['amine hadji', 'botond szábo']"
355,1904.01399,on geometric structure of activation spaces in neural networks,cs.lg stat.ml,"in this paper, we investigate the geometric structure of activation spaces of fully connected layers in neural networks and then show applications of this study. we propose an efficient approximation algorithm to characterize the convex hull of massive points in high dimensional space. based on this new algorithm, four common geometric properties shared by the activation spaces are concluded, which gives a rather clear description of the activation spaces. we then propose an alternative classification method grounding on the geometric structure description, which works better than neural networks alone. surprisingly, this data classification method can be an indicator of overfitting in neural networks. we believe our work reveals several critical intrinsic properties of modern neural networks and further gives a new metric for evaluating them.",,2019-04-02,,"['yuting jia', 'haiwen wang', 'shuo shao', 'huan long', 'yunsong zhou', 'xinbing wang']"
356,1904.01401,bcma-es: a bayesian approach to cma-es,cs.lg stat.ml,"this paper introduces a novel theoretically sound approach for the celebrated cma-es algorithm. assuming the parameters of the multi variate normal distribution for the minimum follow a conjugate prior distribution, we derive their optimal update at each iteration step. not only provides this bayesian framework a justification for the update of the cma-es algorithm but it also gives two new versions of cma-es either assuming normal-wishart or normal-inverse wishart priors, depending whether we parametrize the likelihood by its covariance or precision matrix. we support our theoretical findings by numerical experiments that show fast convergence of these modified versions of cma-es.",,2019-04-02,,"['eric benhamou', 'david saltiel', 'sebastien verel', 'fabien teytaud']"
357,1904.01490,synthetic learner: model-free inference on treatments over time,stat.me cs.lg econ.em stat.ml,"understanding of the effect of a particular treatment or a policy pertains to many areas of interest -- ranging from political economics, marketing to health-care and personalized treatment studies. in this paper, we develop a non-parametric, model-free test for detecting the effects of treatment over time that extends widely used synthetic control tests. the test is built on counterfactual predictions arising from many learning algorithms. in the neyman-rubin potential outcome framework with possible carry-over effects, we show that the proposed test is asymptotically consistent for stationary, beta mixing processes. we do not assume that class of learners captures the correct model necessarily. we also discuss estimates of the average treatment effect, and we provide regret bounds on the predictive performance. to the best of our knowledge, this is the first set of results that allow for example any random forest to be useful for provably valid statistical inference in the synthetic control setting. in experiments, we show that our synthetic learner is substantially more powerful than classical methods based on synthetic control or difference-in-differences, especially in the presence of non-linear outcome models.",,2019-04-02,,"['davide viviano', 'jelena bradic']"
358,1904.01491,statistical testing in a linear probability space,stat.ot,"imagine that you could calculate of posttest probabilities, i.e. bayes theorem with simple addition. this is possible if we stop thinking of probabilities as ranging from 0 to 1.0. there is a naturally occurring linear probability space when data are transformed into the logarithm of the odds ratio (log10 odds). in this space, probabilities are replaced by w (weight) where w=log10(probability/(1-probability)). i would like to argue the multiple benefits of performing statistical testing in a linear probability space: 1) statistical testing is accurate in linear probability space but not in other spaces. 2) effect size is called impact (i) and is the difference in means between two treatments (i=wmean2-wmean1). 3) bayes theorem is simply wposttest=wpretest+itest. 4) significance (p value) is replaced by certainty (c) which is the w of the p value. methods to transform data into and out of linear probability space are described.",,2019-04-02,,['christopher m rembold']
359,1904.01493,new item response models: application to school bullying data,stat.ap,"school bullying victimization is a variable that cannot be measured directly. taking into account that this variable has a lower bound, given by the absence of bullying victimization, this paper proposes irt logistic models, where the latent parameter ranges from $0$ to $\infty$ or from $0$ to a positive real number r, defining the irt parameters and proposing an empirical anchor procedure. as the academic abilities and the school bullying victimization can be explained due to associated factors such as habits, sex, socioeconomic level and education level of parents, irt regression models are proposed to make joint inferences about individual and school characteristic effects. results from the application of the proposed models to the bogot\'a school bullying dataset are presented. the need for testing based in statistical models increases in different fields.",,2019-04-02,,['edilberto cepeda-cuervo']
360,1904.01494,linearity of data and linear probability space,math.st stat.th,"some data is linearly additive, other data is not. in this paper, i discuss types of data based on the boundedness of the data and their linearity. 1) unbounded data can be linear. 2) one-side bounded data is usually log transformed to be linear. 3) two-side bounded data is not linear. 4) untidy data do not fit in these categories. an example of two-sided bounded data is probabilities which should be transformed into a linear probability space by taking the logarithm of the odds ratio (log10 odds) which is termed weight (w). calculations of means and standard deviation is more accurate when calculated as w values than when calculated as probabilities. a methods to analyze untidy data is discussed.",,2019-04-02,,['christopher m. rembold']
361,1904.01508,detection of lddos attacks based on tcp connection parameters,cs.ni cs.lg stat.ml,"low-rate application layer distributed denial of service (lddos) attacks are both powerful and stealthy. they force vulnerable webservers to open all available connections to the adversary, denying resources to real users. mitigation advice focuses on solutions that potentially degrade quality of service for legitimate connections. furthermore, without accurate detection mechanisms, distributed attacks can bypass these defences. a methodology for detection of lddos attacks, based on characteristics of malicious tcp flows, is proposed within this paper. research will be conducted using combinations of two datasets: one generated from a simulated network, the other from the publically available cic dos dataset. both contain the attacks slowread, slowheaders and slowbody, alongside legitimate web browsing. tcp flow features are extracted from all connections. experimentation was carried out using six supervised ai algorithms to categorise attack from legitimate flows. decision trees and k-nn accurately classified up to 99.99% of flows, with exceptionally low false positive and false negative rates, demonstrating the potential of ai in lddos detection.",10.1109/giis.2018.8635701,2019-03-12,,"['michael siracusano', 'stavros shiaeles', 'bogdan ghita']"
362,1904.01509,feafa: a well-annotated dataset for facial expression analysis and 3d   facial animation,cs.lg cs.cv cs.gr eess.iv stat.ml,"facial expression analysis based on machine learning requires large number of well-annotated data to reflect different changes in facial motion. publicly available datasets truly help to accelerate research in this area by providing a benchmark resource, but all of these datasets, to the best of our knowledge, are limited to rough annotations for action units, including only their absence, presence, or a five-level intensity according to the facial action coding system. to meet the need for videos labeled in great detail, we present a well-annotated dataset named feafa for facial expression analysis and 3d facial animation. one hundred and twenty-two participants, including children, young adults and elderly people, were recorded in real-world conditions. in addition, 99,356 frames were manually labeled using expression quantitative tool developed by us to quantify 9 symmetrical facs action units, 10 asymmetrical (unilateral) facs action units, 2 symmetrical facs action descriptors and 2 asymmetrical facs action descriptors, and each action unit or action descriptor is well-annotated with a floating point number between 0 and 1. to provide a baseline for use in future research, a benchmark for the regression of action unit values based on convolutional neural networks are presented. we also demonstrate the potential of our feafa dataset for 3d facial animation. almost all state-of-the-art algorithms for facial animation are achieved based on 3d face reconstruction. we hence propose a novel method that drives virtual characters only based on action unit value regression of the 2d video frames of source actors.",,2019-04-02,,"['yanfu yan', 'ke lu', 'jian xue', 'pengcheng gao', 'jiayi lyu']"
363,1904.01524,direction selection in stochastic directional distance functions,stat.ap,"researchers rely on the distance function to model multiple product production using multiple inputs. a stochastic directional distance function (sddf) allows for noise in potentially all input and output variables. yet, when estimated, the direction selected will affect the functional estimates because deviations from the estimated function are minimized in the specified direction. the set of identified parameters of a parametric sddf can be narrowed via data-driven approaches to restrict the directions considered. we demonstrate a similar narrowing of the identified parameter set for a shape constrained nonparametric method, where the shape constraints impose standard features of a cost function such as monotonicity and convexity.   our monte carlo simulation studies reveal significant improvements, as measured by out of sample radial mean squared error, in functional estimates when we use a directional distance function with an appropriately selected direction. from our monte carlo simulations we conclude that selecting a direction that is approximately orthogonal to the estimated function in the central region of the data gives significantly better estimates relative to the directions commonly used in the literature. for practitioners, our results imply that selecting a direction vector that has non-zero components for all variables that may have measurement error provides a significant improvement in the estimator's performance. we illustrate these results using cost and production data from samples of approximately 500 us hospitals per year operating in 2007, 2008, and 2009, respectively, and find that the shape constrained nonparametric methods provide a significant increase in flexibility over second order local approximation parametric methods.",,2019-04-02,2019-04-03,"['kevin layer', 'andrew l. johnson', 'robin c. sickles', 'gary d. ferrier']"
364,1904.01555,active learning for network intrusion detection,cs.lg cs.cr stat.ml,"network operators are generally aware of common attack vectors that they defend against. for most networks the vast majority of traffic is legitimate. however new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. one strategy for finding such activity is to look for anomalous behavior. investigating anomalous behavior requires significant time and resources. collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. a purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. a methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. false positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. we propose a general active learning framework and experiment with different choices of learners and sampling strategies.",,2019-04-02,,['amir ziai']
365,1904.01557,analysing mathematical reasoning abilities of neural models,cs.lg stat.ml,"mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. in this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. the structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.",,2019-04-02,,"['david saxton', 'edward grefenstette', 'felix hill', 'pushmeet kohli']"
366,1904.01578,unsupervised training of neural mask-based beamforming,cs.sd cs.lg stat.ml,"we present an unsupervised training approach for a neural network-based mask estimator in an acoustic beamforming application. the network is trained to maximize a likelihood criterion derived from a spatial mixture model of the observations. it is trained from scratch without requiring any parallel data consisting of degraded input and clean training targets. thus, training can be carried out on real recordings of noisy speech rather than simulated ones. in contrast to previous work on unsupervised training of neural mask estimators, our approach avoids the need for a possibly pre-trained teacher model entirely. we demonstrate the effectiveness of our approach by speech recognition experiments on two different datasets: one mainly deteriorated by noise (chime 4) and one by reverberation (reverb). the results show that the performance of the proposed system is on par with a supervised system using oracle target masks for training and with a system trained using a model-based teacher.",,2019-04-02,2019-04-08,"['lukas drude', 'jahn heymann', 'reinhold haeb-umbach']"
367,1904.01624,lessons from building acoustic models with a million hours of speech,cs.lg cs.sd eess.as stat.ml,"this is a report of our lessons learned building acoustic models from 1 million hours of unlabeled speech, while labeled speech is restricted to 7,000 hours. we employ student/teacher training on unlabeled data, helping scale out target generation in comparison to confidence model based methods, which require a decoder and a confidence model. to optimize storage and to parallelize target generation, we store high valued logits from the teacher model. introducing the notion of scheduled learning, we interleave learning on unlabeled and labeled data. to scale distributed training across a large number of gpus, we use bmuf with 64 gpus, while performing sequence training only on labeled data with gradient threshold compression sgd using 16 gpus. our experiments show that extremely large amounts of data are indeed useful; with little hyper-parameter tuning, we obtain relative wer improvements in the 10 to 20% range, with higher gains in noisier conditions.",,2019-04-02,,"['sree hari krishnan parthasarathi', 'nikko strom']"
368,1904.01628,"identification, interpretability, and bayesian word embeddings",cs.cl stat.ap,"social scientists have recently turned to analyzing text using tools from natural language processing like word embeddings to measure concepts like ideology, bias, and affinity. however, word embeddings are difficult to use in the regression framework familiar to social scientists: embeddings are are neither identified, nor directly interpretable. i offer two advances on standard embedding models to remedy these problems. first, i develop bayesian word embeddings with automatic relevance determination priors, relaxing the assumption that all embedding dimensions have equal weight. second, i apply work identifying latent variable models to anchor the dimensions of the resulting embeddings, identifying them, and making them interpretable and usable in a regression. i then apply this model and anchoring approach to two cases, the shift in internationalist rhetoric in the american presidents' inaugural addresses, and the relationship between bellicosity in american foreign policy decision-makers' deliberations. i find that inaugural addresses became less internationalist after 1945, which goes against the conventional wisdom, and that an increase in bellicosity is associated with an increase in hostile actions by the united states, showing that elite deliberations are not cheap talk, and helping confirm the validity of the model.",,2019-04-02,,['adam m. lauretig']
369,1904.01631,tony: an orchestrator for distributed machine learning jobs,cs.dc cs.lg stat.ml,"training machine learning (ml) models on large datasets requires considerable computing power. to speed up training, it is typical to distribute training across several machines, often with specialized hardware like gpus or tpus. managing a distributed training job is complex and requires dealing with resource contention, distributed configurations, monitoring, and fault tolerance. in this paper, we describe tony, an open-source orchestrator for distributed ml jobs built at linkedin to address these challenges.",,2019-03-23,,"['anthony hsu', 'keqiu hu', 'jonathan hung', 'arun suresh', 'zhe zhang']"
370,1904.01668,causal comparative effectiveness analysis of dynamic continuous-time   treatment initiation rules with sparsely measured outcomes and death,stat.ap stat.me,"evidence supporting the current world health organization recommendations of early antiretroviral therapy (art) initiation for adolescents is inconclusive. we leverage a large observational data and compare, in terms of mortality and cd4 cell count, the dynamic treatment initiation rules for hiv-infected adolescents. our approaches extend the marginal structural model for estimating outcome distributions under dynamic treatment regimes (dtr), developed in robins et al. (2008), to allow the causal comparisons of both specific regimes and regimes along a continuum. furthermore, we propose strategies to address three challenges posed by the complex data set: continuous-time measurement of the treatment initiation process; sparse measurement of longitudinal outcomes of interest, leading to incomplete data; and censoring due to dropout and death. we derive a weighting strategy for continuous time treatment initiation; use imputation to deal with missingness caused by sparse measurements and dropout; and define a composite outcome that incorporates both death and cd4 count as a basis for comparing treatment regimes. our analysis suggests that immediate art initiation leads to lower mortality and higher median values of the composite outcome, relative to other initiation rules.",,2019-04-02,,"['liangyuan hu', 'joseph w. hogan']"
371,1904.01676,"evaluation of a meta-analysis of air quality and heart attacks, a case   study",stat.ap,"it is generally acknowledged that claims from observational studies often fail to replicate. an exploratory study was undertaken to assess the reliability of base studies used in meta-analysis of short-term air quality-myocardial infarction risk and to judge the reliability of statistical evidence from meta-analysis that uses data from observational studies. a highly cited meta-analysis paper examining whether short-term air quality exposure triggers myocardial infarction was evaluated as a case study. the paper considered six air quality components - carbon monoxide, nitrogen dioxide, sulfur dioxide, particulate matter 10 and 2.5 micrometers in diameter (pm10 and pm2.5), and ozone. the number of possible questions and statistical models at issue in each of 34 base papers used were estimated and p-value plots for each of the air components were constructed to evaluate the effect heterogeneity of p-values used from the base papers. analysis search spaces (number of statistical tests possible) in the base papers were large, median of 12,288, interquartile range: 2,496 to 58,368, in comparison to actual statistical test results presented. statistical test results taken from the base papers may not provide unbiased measures of effect for meta-analysis. shapes of p-value plots for the six air components were consistent with the possibility of analysis manipulation to obtain small p-values in several base papers. results suggest the appearance of heterogeneous, researcher-generated p-values used in the meta-analysis rather than unbiased evidence of real effects for air quality. we conclude that this meta-analysis does not provide reliable evidence for an association of air quality components with myocardial risk.",10.1080/10408444.2019.1576587,2019-04-02,,"['s. stanley young', 'warren b. kindzierski']"
372,1904.01720,neural program repair by jointly learning to localize and repair,cs.lg stat.ml,"due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. in this work, we consider a recently identified class of bugs called variable-misuse bugs. the state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. we show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. we present multi-headed pointer networks for this purpose, with one head each for localization and repair. the experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.",,2019-04-02,,"['marko vasic', 'aditya kanade', 'petros maniatis', 'david bieber', 'rishabh singh']"
373,1904.01747,on better exploring and exploiting task relationships in multi-task   learning: joint model and feature learning,cs.lg cs.ai stat.ml,"multitask learning (mtl) aims to learn multiple tasks simultaneously through the interdependence between different tasks. the way to measure the relatedness between tasks is always a popular issue. there are mainly two ways to measure relatedness between tasks: common parameters sharing and common features sharing across different tasks. however, these two types of relatedness are mainly learned independently, leading to a loss of information. in this paper, we propose a new strategy to measure the relatedness that jointly learns shared parameters and shared feature representations. the objective of our proposed method is to transform the features from different tasks into a common feature space in which the tasks are closely related and the shared parameters can be better optimized. we give a detailed introduction to our proposed multitask learning method. additionally, an alternating algorithm is introduced to optimize the nonconvex objection. a theoretical bound is given to demonstrate that the relatedness between tasks can be better measured by our proposed multitask learning algorithm. we conduct various experiments to verify the superiority of the proposed joint model and feature a multitask learning method.",10.1109/tnnls.2017.2690683,2019-04-02,,"['ya li', 'xinmei tian', 'tongliang liu', 'dacheng tao']"
374,1904.01750,exponentially convergent stochastic k-pca without variance reduction,cs.lg stat.ml,"we present matrix krasulina, an algorithm for online k-pca, by generalizing the classic krasulina's method (krasulina, 1969) from vector to matrix case. we show, both theoretically and empirically, that the algorithm naturally adapts to data low-rankness and converges exponentially fast to the ground-truth principal subspace. notably, our result suggests that despite various recent efforts to accelerate the convergence of stochastic-gradient based methods by adding a o(n)-time variance reduction step, for the k-pca problem, a truly online sgd variant suffices to achieve exponential convergence on intrinsically low-rank data.",,2019-04-02,,['cheng tang']
375,1904.01806,deep reinforcement learning on a budget: 3d control and reasoning   without a supercomputer,cs.lg stat.ml,"an important goal of research in deep reinforcement learning in mobile robotics is to train agents capable of solving complex tasks, which require a high level of scene understanding and reasoning from an egocentric perspective. when trained from simulations, optimal environments should satisfy a currently unobtainable combination of high-fidelity photographic observations, massive amounts of different environment configurations and fast simulation speeds. in this paper we argue that research on training agents capable of complex reasoning can be simplified by decoupling from the requirement of high fidelity photographic observations. we present a suite of tasks requiring complex reasoning and exploration in continuous, partially observable 3d environments. the objective is to provide challenging scenarios and a robust baseline agent architecture that can be trained on mid-range consumer hardware in under 24h. our scenarios combine two key advantages: (i) they are based on a simple but highly efficient 3d environment (vizdoom) which allows high speed simulation (12000fps); (ii) the scenarios provide the user with a range of difficulty settings, in order to identify the limitations of current state of the art algorithms and network architectures. we aim to increase accessibility to the field of deep-rl by providing baselines for challenging scenarios where new ideas can be iterated on quickly. we argue that the community should be able to address challenging problems in reasoning of mobile agents without the need for a large compute infrastructure.",,2019-04-03,,"['edward beeching', 'christian wolf', 'jilles dibangoye', 'olivier simonin']"
376,1904.01814,deep neural networks for rotation-invariance approximation and learning,cs.lg stat.ml,"based on the tree architecture, the objective of this paper is to design deep neural networks with two or more hidden layers (called deep nets) for realization of radial functions so as to enable rotational invariance for near-optimal function approximation in an arbitrarily high dimensional euclidian space. it is shown that deep nets have much better performance than shallow nets (with only one hidden layer) in terms of approximation accuracy and learning capabilities. in particular, for learning radial functions, it is shown that near-optimal rate can be achieved by deep nets but not by shallow nets. our results illustrate the necessity of depth in neural network design for realization of rotation-invariance target functions.",,2019-04-03,,"['charles k. chui', 'shao-bo lin', 'ding-xuan zhou']"
377,1904.01820,large deviations for the largest eigenvalues and eigenvectors of spiked   random matrices,math.pr cond-mat.dis-nn cond-mat.stat-mech math-ph math.mp math.st stat.th,"we consider matrices formed by a random $n\times n$ matrix drawn from the gaussian orthogonal ensemble (or gaussian unitary ensemble) plus a rank-one perturbation of strength $\theta$, and focus on the largest eigenvalue, $x$, and the component, $u$, of the corresponding eigenvector in the direction associated to the rank-one perturbation. we obtain the large deviation principle governing the atypical joint fluctuations of $x$ and $u$. interestingly, for $\theta>1$, in large deviations characterized by a small value of $u$, i.e. $u<1-1/\theta$, the second-largest eigenvalue pops out from the wigner semi-circle and the associated eigenvector orients in the direction corresponding to the rank-one perturbation. we generalize these results to the wishart ensemble, and we extend them to the first $n$ eigenvalues and the associated eigenvectors.",,2019-04-03,,"['giulio biroli', 'alice guionnet']"
378,1904.01849,measurement error induced by locational uncertainty when estimating   discrete choice models with a distance as a regressor,stat.me,"spatial microeconometric studies typically suffer from various forms of inaccuracies that are not present when dealing with the classical regional spatial econometrics models. among those, missing data, locational errors, sampling without a formal sample design, measurement errors and misalignment are the typical sources of inaccuracy that can affects the results in a spatial microeconometric analysis. in this paper, we have examined the effects of measurement error introduced in a logistic model by random geo-masking, when distances are used as predictors. extending the classical results on the measurement error in a linear regression model, our mc experiment on hospital choices showed that the higher the distortion produced by the geo-masking, the higher is the downward bias in absolute value towards zero of the coefficient associated to the distance in a regression model.",,2019-04-03,,"['giuseppe arbia', 'paolo berta', 'carrie b. dolan']"
379,1904.01855,a stochastic interpretation of stochastic mirror descent: risk-sensitive   optimality,math.oc cs.lg cs.sy stat.ml,"stochastic mirror descent (smd) is a fairly new family of algorithms that has recently found a wide range of applications in optimization, machine learning, and control. it can be considered a generalization of the classical stochastic gradient algorithm (sgd), where instead of updating the weight vector along the negative direction of the stochastic gradient, the update is performed in a ""mirror domain"" defined by the gradient of a (strictly convex) potential function. this potential function, and the mirror domain it yields, provides considerable flexibility in the algorithm compared to sgd. while many properties of smd have already been obtained in the literature, in this paper we exhibit a new interpretation of smd, namely that it is a risk-sensitive optimal estimator when the unknown weight vector and additive noise are non-gaussian and belong to the exponential family of distributions. the analysis also suggests a modified version of smd, which we refer to as symmetric smd (ssmd). the proofs rely on some simple properties of bregman divergence, which allow us to extend results from quadratics and gaussians to certain convex functions and exponential families in a rather seamless way.",,2019-04-03,,"['navid azizan', 'babak hassibi']"
380,1904.01859,creating new distributions using integration and summation by parts,math.st stat.th,"methods for generating new distributions from old can be thought of as techniques for simplifying integrals used in reverse. hence integrating a probability density function (pdf) by parts provides a new way of modifying distributions; the resulting pdfs are integrals that sometimes require computation as special functions. summation by parts can be used similarly for discrete distributions. the general methodology is given, with some examples of distribution classes and of specific distributions, and fits to data.",,2019-04-03,,['rose baker']
381,1904.01932,modeling the causal effect of treatment initiation time on survival:   application to hiv/tb co-infection,stat.ap stat.me,"the timing of antiretroviral therapy (art) initiation for hiv and tuberculosis (tb) co-infected patients needs to be considered carefully. cd4 cell count can be used to guide decision making about when to initiate art. evidence from recent randomized trials and observational studies generally supports early initiation but does not provide information about effects of initiation time on a continuous scale. in this paper, we develop and apply a highly flexible structural proportional hazards model for characterizing the effect of treatment initiation time on a survival distribution. the model can be fitted using a weighted partial likelihood score function. construction of both the score function and the weights must accommodate censoring of the treatment initiation time, the outcome, or both. the methods are applied to data on 4903 individuals with hiv/tb co-infection, derived from electronic health records in a large hiv care program in kenya. we use a model formulation that flexibly captures the joint effects of art initiation time and art duration using natural cubic splines. the model is used to generate survival curves corresponding to specific treatment initiation times; and to identify optimal times for art initiation for subgroups defined by cd4 count at time of tb diagnosis. our findings potentially provide ""higher resolution"" information about the relationship between art timing and mortality, and about the differential effect of art timing within cd4 subgroups.",,2019-04-02,,"['liangyuan hu', 'joseph w. hogan', 'ann w. mwangi', 'abraham siika']"
382,1904.01948,simulation study of estimating between-study variance and overall effect   in meta-analyses of mean difference,stat.me,"methods for random-effects meta-analysis require an estimate of the between-study variance, $\tau^2$. the performance of estimators of $\tau^2$ (measured by bias and coverage) affects their usefulness in assessing heterogeneity of study-level effects, and also the performance of related estimators of the overall effect. for the effect measure mean difference (md), we review five point estimators of $\tau^2$ (the popular methods of dersimonian-laird, restricted maximum likelihood, and mandel and paule (mp); the less-familiar method of jackson; and a new method (wt) based on the improved approximation to the distribution of the $q$ statistic by \cite{kulinskaya2004welch}), five interval estimators for $\tau^2$ (profile likelihood, q-profile, biggerstaff and jackson, jackson, and the new wt method), six point estimators of the overall effect (the five related to the point estimators of $\tau^2$ and an estimator whose weights use only study-level sample sizes), and eight interval estimators for the overall effect (five based on the point estimators for $\tau^2$, the hartung-knapp-sidik-jonkman (hksj) interval, a modification of hksj, and an interval based on the sample-size-weighted estimator). we obtain empirical evidence from extensive simulations and an example.",,2019-04-01,,"['ilyas bakbergenuly', 'david c. hoaglin', 'elena kulinskaya']"
383,1904.02016,stochastic blockmodels with edge information,cs.si cs.lg stat.ml,"stochastic blockmodels allow us to represent networks in terms of a latent community structure, often yielding intuitions about the underlying social structure. typically, this structure is inferred based only on a binary network representing the presence or absence of interactions between nodes, which limits the amount of information that can be extracted from the data. in practice, many interaction networks contain much more information about the relationship between two nodes. for example, in an email network, the volume of communication between two users and the content of that communication can give us information about both the strength and the nature of their relationship.   in this paper, we propose the topic blockmodel, a stochastic blockmodel that uses a count-based topic model to capture the interaction modalities within and between latent communities. by explicitly incorporating information sent between nodes in our network representation, we are able to address questions of interest in real-world situations, such as predicting recipients for an email message or inferring the content of an unopened email. further, by considering topics associated with a pair of communities, we are better able to interpret the nature of each community and the manner in which it interacts with other communities.",,2019-04-03,,"['guy w. cole', 'sinead a. williamson']"
384,1904.02054,discretefdr: an r package for controlling the false discovery rate for   discrete test statistics,stat.co,"the simultaneous analysis of many statistical tests is ubiquitous in applications. perhaps the most popular error rate used for avoiding type one error inflation is the false discovery rate (fdr). however, most theoretical and software development for fdr control has focused on the case of continuous test statistics. for discrete data, methods that provide proven fdr control and good performance have been proposed only recently. the r package discretefdr provides an implementation of these methods. for particular commonly used discrete tests such as fisher's exact test, it can be applied as an off-the-shelf tool by taking only the raw data as input. it can also be used for any arbitrary discrete test statistics by using some additional information on the distribution of these statistics. the paper reviews the statistical methods in a non-technical way, provides a detailed description of the implementation in discretefdr and presents some sample code and analyses.",,2019-04-03,,"['guillermo durand', 'florian junge', 'sebastian döhler', 'etienne roquain']"
385,1904.02058,do hospital data breaches reduce patient care quality?,econ.gn q-fin.ec stat.ap,"objective: to estimate the relationship between a hospital data breach and hospital quality outcome   materials and methods: hospital data breaches reported to the u.s. department of health and human services breach portal and the privacy rights clearinghouse database were merged with the medicare hospital compare data to assemble a panel of non-federal acutecare inpatient hospitals for years 2011 to 2015. the study panel included 2,619 hospitals. changes in 30-day ami mortality rate following a hospital data breach were estimated using a multivariate regression model based on a difference-in-differences approach.   results: a data breach was associated with a 0.338[95% ci, 0.101-0.576] percentage point increase in the 30-day ami mortality rate in the year following the breach and a 0.446[95% ci, 0.164-0.729] percentage point increase two years after the breach. for comparison, the median 30-day ami mortality rate has been decreasing about 0.4 percentage points annually since 2011 due to progress in care. the magnitude of the breach impact on hospitals' ami mortality rates was comparable to a year's worth historical progress in reducing ami mortality rates.   conclusion: hospital data breaches significantly increased the 30-day mortality rate for ami. data breaches may disrupt the processes of care that rely on health information technology. financial costs to repair a breach may also divert resources away from patient care. thus breached hospitals should carefully focus investments in security procedures, processes, and health information technology that jointly lead to better data security and improved patient outcomes.",,2019-04-03,,"['sung j. choi', 'm. eric johnson']"
386,1904.02064,minimum volume topic modeling,stat.ml cs.ir cs.lg,"we propose a new topic modeling procedure that takes advantage of the fact that the latent dirichlet allocation (lda) log likelihood function is asymptotically equivalent to the logarithm of the volume of the topic simplex. this allows topic modeling to be reformulated as finding the probability simplex that minimizes its volume and encloses the documents that are represented as distributions over words. a convex relaxation of the minimum volume topic model optimization is proposed, and it is shown that the relaxed problem has the same global minimum as the original problem under the separability assumption and the sufficiently scattered assumption introduced by arora et al. (2013) and huang et al. (2016). a locally convergent alternating direction method of multipliers (admm) approach is introduced for solving the relaxed minimum volume problem. numerical experiments illustrate the benefits of our approach in terms of computation time and topic recovery performance.",,2019-04-03,,"['byoungwook jang', 'alfred hero']"
387,1904.02107,data-driven discovery of coordinates and governing equations,stat.ot,"the discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. the resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. this provides an algorithmic approach to occam's razor for model discovery. however, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. in this work, we design a custom autoencoder to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. thus, we simultaneously learn the governing equations and the associated coordinate system. we demonstrate this approach on several example high-dimensional dynamical systems with low-dimensional behavior. the resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (sindy) for parsimonious models. it is the first method of its kind to place the discovery of coordinates and models on an equal footing.",,2019-03-29,2019-04-04,"['kathleen champion', 'bethany lusch', 'j. nathan kutz', 'steven l. brunton']"
388,1904.02130,normal approximation for stochastic gradient descent via non-asymptotic   rates of martingale clt,math.st math.oc math.pr stat.ml stat.th,"we provide non-asymptotic convergence rates of the polyak-ruppert averaged stochastic gradient descent (sgd) to a normal random vector for a class of twice-differentiable test functions. a crucial intermediate step is proving a non-asymptotic martingale central limit theorem (clt), i.e., establishing the rates of convergence of a multivariate martingale difference sequence to a normal random vector, which might be of independent interest. we obtain the explicit rates for the multivariate martingale clt using a combination of stein's method and lindeberg's argument, which is then used in conjunction with a non-asymptotic analysis of averaged sgd proposed in [pj92]. our results have potentially interesting consequences for computing confidence intervals for parameter estimation with sgd and constructing hypothesis tests with sgd that are valid in a non-asymptotic sense.",,2019-04-03,,"['andreas anastasiou', 'krishnakumar balasubramanian', 'murat a. erdogdu']"
389,1904.02182,statistical analysis of some evolution equations driven by space-only   noise,math.st stat.th,"we study the statistical properties of stochastic evolution equations driven by space-only noise, either additive or multiplicative. while forward problems, such as existence, uniqueness, and regularity of the solution, for such equations have been studied, little is known about inverse problems for these equations. we exploit the somewhat unusual structure of the observations coming from these equations that leads to an interesting interplay between classical and non-traditional statistical models. we derive several types of estimators for the drift and/or diffusion coefficients of these equations, and prove their relevant properties.",,2019-04-03,,"['igor cialenco', 'hyun-jung kim', 'sergey v. lototsky']"
390,1904.02205,progressive stochastic binarization of deep networks,cs.lg stat.ml,"a plethora of recent research has focused on improving the memory footprint and inference speed of deep networks by reducing the complexity of (i) numerical representations (for example, by deterministic or stochastic quantization) and (ii) arithmetic operations (for example, by binarization of weights).   we propose a stochastic binarization scheme for deep networks that allows for efficient inference on hardware by restricting itself to additions of small integers and fixed shifts. unlike previous approaches, the underlying randomized approximation is progressive, thus permitting an adaptive control of the accuracy of each operation at run-time. in a low-precision setting, we match the accuracy of previous binarized approaches. our representation is unbiased - it approaches continuous computation with increasing sample size. in a high-precision regime, the computational costs are competitive with previous quantization schemes. progressive stochastic binarization also permits localized, dynamic accuracy control within a single network, thereby providing a new tool for adaptively focusing computational attention.   we evaluate our method on networks of various architectures, already pretrained on imagenet. with representational costs comparable to previous schemes, we obtain accuracies close to the original floating point implementation. this includes pruned networks, except the known special case of certain types of separated convolutions. by focusing computational attention using progressive sampling, we reduce inference costs on imagenet further by a factor of up to 33% (before network pruning).",,2019-04-03,,"['david hartmann', 'michael wand']"
391,1904.02206,"jointly pre-training with supervised, autoencoder, and value losses for   deep reinforcement learning",cs.lg stat.ml,"deep reinforcement learning (drl) algorithms are known to be data inefficient. one reason is that a drl agent learns both the feature and the policy tabula rasa. integrating prior knowledge into drl algorithms is one way to improve learning efficiency since it helps to build helpful representations. in this work, we consider incorporating human knowledge to accelerate the asynchronous advantage actor-critic (a3c) algorithm by pre-training a small amount of non-expert human demonstrations. we leverage the supervised autoencoder framework and propose a novel pre-training strategy that jointly trains a weighted supervised classification loss, an unsupervised reconstruction loss, and an expected return loss. the resulting pre-trained model learns more useful features compared to independently training in supervised or unsupervised fashion. our pre-training method drastically improved the learning performance of the a3c agent in atari games of pong and mspacman, exceeding the performance of the state-of-the-art algorithms at a much smaller number of game interactions. our method is light-weight and easy to implement in a single machine. for reproducibility, our code is available at github.com/gabrieledcjr/deeprl/tree/a3c-ala2019",,2019-04-03,,"['gabriel v. de la cruz', 'yunshu du', 'matthew e. taylor']"
392,1904.02217,decomposing temperature time series with non-negative matrix   factorization,cs.lg cs.cv physics.app-ph stat.ml,"during the fabrication of casting parts sensor data is typically automatically recorded and accumulated for process monitoring and defect diagnosis. as casting is a thermal process with many interacting process parameters, root cause analysis tends to be tedious and ineffective. we show how a decomposition based on non-negative matrix factorization (nmf), which is guided by a knowledge-based initialization strategy, is able to extract physical meaningful sources from temperature time series collected during a thermal manufacturing process. the approach assumes the time series to be generated by a superposition of several simultaneously acting component processes. nmf is able to reverse the superposition and to identify the hidden component processes. the latter can be linked to ongoing physical phenomena and process variables, which cannot be monitored directly. our approach provides new insights into the underlying physics and offers a tool, which can assist in diagnosing defect causes. we demonstrate our method by applying it to real world data, collected in a foundry during the series production of casting parts for the automobile industry.",,2019-04-03,,"['peter weiderer', 'ana maria tomé', 'elmar wolfgang lang']"
393,1904.02219,robust semiparametric inference for polytomous logistic regression with   complex survey design,stat.me math.st stat.ap stat.th,"analyzing polytomous response from a complex survey scheme, like stratified or cluster sampling is very crucial in several socio-economics applications. we present a class of minimum quasi weighted density power divergence estimators for the polytomous logistic regression model with such a complex survey. this family of semiparametric estimators is a robust generalization of the maximum quasi weighted likelihood estimator exploiting the advantages of the popular density power divergence measure. accordingly robust estimators for the design effects are also derived. robust testing of general linear hypotheses on the regression coefficients are proposed using the new estimators. their asymptotic distributions and robustness properties are theoretically studied and also empirically validated through a numerical example and an extensive monte carlo study.",,2019-04-03,,"['elena castilla', 'abhik ghosh', 'nirian martin', 'leandro pardo']"
394,1904.02235,robust multi-agent counterfactual prediction,cs.gt cs.ai cs.ma stat.ap,"we consider the problem of using logged data to make predictions about what would happen if we changed the `rules of the game' in a multi-agent system. this task is difficult because in many cases we observe actions individuals take but not their private information or their full reward functions. in addition, agents are strategic, so when the rules change, they will also change their actions. existing methods (e.g. structural estimation, inverse reinforcement learning) make counterfactual predictions by constructing a model of the game, adding the assumption that agents' behavior comes from optimizing given some goals, and then inverting observed actions to learn agent's underlying utility function (a.k.a. type). once the agent types are known, making counterfactual predictions amounts to solving for the equilibrium of the counterfactual environment. this approach imposes heavy assumptions such as rationality of the agents being observed, correctness of the analyst's model of the environment/parametric form of the agents' utility functions, and various other conditions to make point identification possible. we propose a method for analyzing the sensitivity of counterfactual conclusions to violations of these assumptions. we refer to this method as robust multi-agent counterfactual prediction (rmac). we apply our technique to investigating the robustness of counterfactual claims for classic environments in market design: auctions, school choice, and social choice. importantly, we show rmac can be used in regimes where point identification is impossible (e.g. those which have multiple equilibria or non-injective maps from type distributions to outcomes).",,2019-04-03,,"['alexander peysakhovich', 'christian kroer', 'adam lerer']"
395,1904.02243,optimized preprocessing and machine learning for quantitative raman   spectroscopy in biology,eess.sp cs.lg q-bio.qm stat.ml,"raman spectroscopy's capability to provide meaningful composition predictions is heavily reliant on a pre-processing step to remove insignificant spectral variation. this is crucial in biofluid analysis. widespread adoption of diagnostics using raman requires a robust model which can withstand routine spectra discrepancies due to unavoidable variations such as age, diet, and medical background. a wealth of pre-processing methods are available, and it is often up to trial-and-error or user experience to select the method which gives the best results. this process can be incredibly time consuming and inconsistent for multiple operators.   in this study we detail a method to analyze the statistical variability within a set of training spectra and determine suitability to form a robust model. this allows us to selectively qualify or exclude a pre-processing method, predetermine robustness, and simultaneously identify the number of components which will form the best predictive model. we demonstrate the ability of this technique to improve predictive models of two artificial biological fluids.   raman spectroscopy is ideal for noninvasive, nondestructive analysis. routine health monitoring which maximizes comfort is increasingly crucial, particularly in epidemic-level diabetes diagnoses. high variability in spectra of biological samples can hinder raman's adoption for these methods. our technique allows the decision of optimal pre-treatment method to be determined for the operator; model performance is no longer a function of user experience. we foresee this statistical technique being an instrumental element to widening the adoption of raman as a monitoring tool in a field of biofluid analysis.",,2019-04-03,,"['emily e storey', 'amr s. helmy']"
396,1904.02250,a new class of change point test statistics of r\'enyi type,math.st math.pr stat.th,"a new class of change point test statistics is proposed that utilizes a weighting and trimming scheme for the cumulative sum (cusum) process inspired by r\'enyi (1953). a thorough asymptotic analysis and simulations both demonstrate that this new class of statistics possess superior power compared to traditional change point statistics based on the cusum process when the change point is near the beginning or end of the sample. generalizations of these ""r\'enyi"" statistics are also developed to test for changes in the parameters in linear and non-linear regression models, and in generalized method of moments estimation. in these contexts we applied the proposed statistics, as well as several others, to test for changes in the coefficients of fama-french factor models. we observed that the r\'enyi statistic was the most effective in terms of retrospectively detecting change points that occur near the endpoints of the sample.",,2019-04-03,,"['lajos horváth', 'curtis miller', 'gregory rice']"
397,1904.02278,dagcn: dual attention graph convolutional networks,cs.lg stat.ml,"graph convolutional networks (gcns) have recently become one of the most powerful tools for graph analytics tasks in numerous applications, ranging from social networks and natural language processing to bioinformatics and chemoinformatics, thanks to their ability to capture the complex relationships between concepts. at present, the vast majority of gcns use a neighborhood aggregation framework to learn a continuous and compact vector, then performing a pooling operation to generalize graph embedding for the classification task. these approaches have two disadvantages in the graph classification task: (1)when only the largest sub-graph structure ($k$-hop neighbor) is used for neighborhood aggregation, a large amount of early-stage information is lost during the graph convolution step; (2) simple average/sum pooling or max pooling utilized, which loses the characteristics of each node and the topology between nodes. in this paper, we propose a novel framework called, dual attention graph convolutional networks (dagcn) to address these problems. dagcn automatically learns the importance of neighbors at different hops using a novel attention graph convolution layer, and then employs a second attention component, a self-attention pooling layer, to generalize the graph representation from the various aspects of a matrix graph embedding. the dual attention network is trained in an end-to-end manner for the graph classification task. we compare our model with state-of-the-art graph kernels and other deep learning methods. the experimental results show that our framework not only outperforms other baselines but also achieves a better rate of convergence.",,2019-04-03,,"['fengwen chen', 'shirui pan', 'jing jiang', 'huan huo', 'guodong long']"
398,1904.02383,artificial neural network modeling for path loss prediction in urban   environments,cs.lg stat.ml,"although various linear log-distance path loss models have been developed, advanced models are requiring to more accurately and flexibly represent the path loss for complex environments such as the urban area. this letter proposes an artificial neural network (ann) based multi-dimensional regression framework for path loss modeling in urban environments at 3 to 6 ghz frequency band. ann is used to learn the path loss structure from the measured path loss data which is a function of distance and frequency. the effect of the network architecture parameter (activation function, the number of hidden layers and nodes) on the prediction accuracy are analyzed. we observe that the proposed model is more accurate and flexible compared to the conventional linear model.",,2019-04-04,,"['chanshin park', 'daniel k. tettey', 'han-shin jo']"
399,1904.02405,white-to-black: efficient distillation of black-box adversarial attacks,cs.lg cs.cr stat.ml,"adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adversarial training. recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (ebrahimi et al., 2018). in this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network. we train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples. moreover, it reduces adversarial example generation time by 19x-39x. we also show that our approach transfers to a black-box setting, by attacking the google perspective api and exposing its vulnerability. our attack flips the api-predicted label in 42\% of the generated examples, while humans maintain high-accuracy in predicting the gold label.",,2019-04-04,,"['yotam gil', 'yoav chai', 'or gorodissky', 'jonathan berant']"
400,1904.02436,few-shot brain segmentation from weakly labeled data with deep   heteroscedastic multi-task networks,cs.lg eess.iv stat.ml,"in applications of supervised learning applied to medical image segmentation, the need for large amounts of labeled data typically goes unquestioned. in particular, in the case of brain anatomy segmentation, hundreds or thousands of weakly-labeled volumes are often used as training data. in this paper, we first observe that for many brain structures, a small number of training examples, (n=9), weakly labeled using freesurfer 6.0, plus simple data augmentation, suffice as training data to achieve high performance, achieving an overall mean dice coefficient of $0.84 \pm 0.12$ compared to freesurfer over 28 brain structures in t1-weighted images of $\approx 4000$ 9-10 year-olds from the adolescent brain cognitive development study. we then examine two varieties of heteroscedastic network as a method for improving classification results. an existing proposal by kendall and gal, which uses monte-carlo inference to learn to predict the variance of each prediction, yields an overall mean dice of $0.85 \pm 0.14$ and showed statistically significant improvements over 25 brain structures. meanwhile a novel heteroscedastic network which directly learns the probability that an example has been mislabeled yielded an overall mean dice of $0.87 \pm 0.11$ and showed statistically significant improvements over all but one of the brain structures considered. the loss function associated to this network can be interpreted as performing a form of learned label smoothing, where labels are only smoothed where they are judged to be uncertain.",,2019-04-04,,"['richard mckinley', 'michael rebsamen', 'raphael meier', 'mauricio reyes', 'christian rummel', 'roland wiest']"
401,1904.02526,constrained generative adversarial networks for interactive image   generation,cs.gr cs.cv cs.lg stat.ml,"generative adversarial networks (gans) have received a great deal of attention due in part to recent success in generating original, high-quality samples from visual domains. however, most current methods only allow for users to guide this image generation process through limited interactions. in this work we develop a novel gan framework that allows humans to be ""in-the-loop"" of the image generation process. our technique iteratively accepts relative constraints of the form ""generate an image more like image a than image b"". after each constraint is given, the user is presented with new outputs from the gan, informing the next round of feedback. this feedback is used to constrain the output of the gan with respect to an underlying semantic space that can be designed to model a variety of different notions of similarity (e.g. classes, attributes, object relationships, color, etc.). in our experiments, we show that our gan framework is able to generate images that are of comparable quality to equivalent unsupervised gans while satisfying a large number of the constraints provided by users, effectively changing a gan into one that allows users interactive control over image generation without sacrificing image quality.",,2019-04-03,,['eric heim']
402,1904.02624,efficient estimation of accelerated lifetime models under length-biased   sampling,math.st stat.th,"in prevalent cohort studies where subjects are recruited at a cross-section, the time to an event may be subject to length-biased sampling, with the observed data being either the forward recurrence time, or the backward recurrence time, or their sum. in the regression setting, it has been shown that the accelerated failure time model for the underlying event time is invariant under these observed data set-ups and can be fitted using standard methodology for accelerated failure time model estimation, ignoring the length-bias. however, the efficiency of these estimators is unclear, owing to the fact that the observed covariate distribution, which is also length-biased, may contain information about the regression parameter in the accelerated life model. we demonstrate that if the true covariate distribution is completely unspecified, then the naive estimator based on the conditional likelihood given the covariates is fully efficient.",,2019-04-04,,"['pourab roy', 'jason p. fine', 'michael r. kosorok']"
403,1904.02632,a learned representation for scalable vector graphics,cs.cv cs.lg stat.ml,"dramatic advances in generative models have resulted in near photographic quality for artificially rendered faces, animals and other objects in the natural world. in spite of such advances, a higher level understanding of vision and imagery does not arise from exhaustively modeling an object, but instead identifying higher-level attributes that best summarize the aspects of an object. in this work we attempt to model the drawing process of fonts by building sequential generative models of vector graphics. this model has the benefit of providing a scale-invariant representation for imagery whose latent representation may be systematically manipulated and exploited to perform style propagation. we demonstrate these results on a large dataset of fonts and highlight how such a model captures the statistical dependencies and richness of this dataset. we envision that our model can find use as a tool for graphic designers to facilitate font design.",,2019-04-04,,"['raphael gontijo lopes', 'david ha', 'douglas eck', 'jonathon shlens']"
404,1904.02655,determining input variable ranges in industry 4.0: a heuristic for   estimating the domain of a real-valued function or trained regression model   given an output range,cs.lg stat.ml,"industrial process control systems try to keep an output variable within a given tolerance around a target value. pid control systems have been widely used in industry to control input variables in order to reach this goal. however, this kind of transfer function based approach cannot be extended to complex processes where input data might be non-numeric, high dimensional, sparse, etc. in such cases, there is still a need for determining the subspace of input data that produces an output within a given range. this paper presents a non-stochastic heuristic to determine input values for a mathematical function or trained regression model given an output range. the proposed method creates a synthetic training data set of input combinations with a class label that indicates whether the output is within the given target range or not. then, a decision tree classifier is used to determine the subspace of input data of interest. this method is more general than a traditional controller as the target range for the output does not have to be centered around a reference value and it can be applied given a regression model of the output variable, which may have categorical variables as inputs and may be high dimensional, sparse... the proposed heuristic is validated with a proof of concept on a real use case where the quality of a lamination factory is established to identify the suitable subspace of production variable values.",,2019-04-03,,"['noelia oses', 'aritz legarretaetxebarria', 'marco quartulli', 'igor garcía', 'mikel serrano']"
405,1904.02773,adaptive sequential machine learning,cs.lg stat.ml,"a framework previously introduced in [3] for solving a sequence of stochastic optimization problems with bounded changes in the minimizers is extended and applied to machine learning problems such as regression and classification. the stochastic optimization problems arising in these machine learning problems is solved using algorithms such as stochastic gradient descent (sgd). a method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples at each time step to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. a bound is developed to show that the estimate of the change in the minimizers is non-trivial provided that the excess risk is small enough. extensions relevant to the machine learning setting are considered, including a cost-based approach to select the number of samples with a cost budget over a fixed horizon, and an approach to applying cross-validation for model selection. finally, experiments with synthetic and real data are used to validate the algorithms.",,2019-04-04,,"['craig wilson', 'yuheng bu', 'venugopal veeravalli']"
406,1904.02792,unifying human and statistical evaluation for natural language   generation,cs.cl cs.ai stat.ml,"how can we measure whether a natural language generation system produces both high quality and diverse outputs? human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. on the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. in this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or machine-generated. we demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call huse. on summarization and chit-chat dialogue, we show that (i) huse detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease huse due to decreased diversity.",,2019-04-04,,"['tatsunori b. hashimoto', 'hugh zhang', 'percy liang']"
407,1904.02818,neural networks for modeling source code edits,cs.lg cs.cl cs.se stat.ml,"programming languages are emerging as a challenging and interesting domain for machine learning. a core task, which has received significant attention in recent years, is building generative models of source code. however, to our knowledge, previous generative models have always been framed in terms of generating static snapshots of code. in this work, we instead treat source code as a dynamic object and tackle the problem of modeling the edits that software developers make to source code files. this requires extracting intent from previous edits and leveraging it to generate subsequent edits. we develop several neural networks and use synthetic data to test their ability to learn challenging edit patterns that require strong generalization. we then collect and train our models on a large-scale dataset of google source code, consisting of millions of fine-grained edits from thousands of python developers. from the modeling perspective, our main conclusion is that a new composition of attentional and pointer network components provides the best overall performance and scalability. from the application perspective, our results provide preliminary evidence of the feasibility of developing tools that learn to predict future edits.",,2019-04-04,,"['rui zhao', 'david bieber', 'kevin swersky', 'daniel tarlow']"
408,1904.02855,probabilistic recalibration of forecasts,stat.me,"we present a scheme by which a probabilistic forecasting system whose predictions have poor probabilistic calibration may be recalibrated by incorporating past performance information to produce a new forecasting system that is demonstrably superior to the original, in that one may use it to consistently win wagers against someone using the original system. the scheme utilizes gaussian process (gp) modeling to estimate a probability distribution over the probability integral transform (pit) of a scalar predictand. the gp density estimate gives closed-form access to information entropy measures associated with the estimated distribution, which allows prediction of winnings in wagers against the base forecasting system. a separate consequence of the procedure is that the recalibrated forecast has a uniform expected pit distribution. a distinguishing feature of the procedure is that it is appropriate even if the pit values are not i.i.d. the recalibration scheme is formulated in a framework that exploits the deep connections between information theory, forecasting, and betting. we demonstrate the effectiveness of the scheme in two case studies: a laboratory experiment with a nonlinear circuit and seasonal forecasts of the intensity of the el ni\~no-southern oscillation phenomenon.",,2019-04-04,,"['carlo graziani', 'robert rosner', 'jennifer m. adams', 'reason l. machete']"
409,1904.02877,single-path nas: designing hardware-efficient convnets in less than 4   hours,cs.lg cs.cv stat.ml,"can we automatically design a convolutional network (convnet) with the highest image classification accuracy under the runtime constraint of a mobile device? neural architecture search (nas) has revolutionized the design of hardware-efficient convnets by automating this process. however, the nas problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 gpu-hours). to alleviate this complexity, we propose single-path nas, a novel differentiable nas method for designing hardware-efficient convnets in less than 4 hours. our contributions are as follows: 1. single-path search space: compared to previous differentiable nas methods, single-path nas uses one single-path over-parameterized convnet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. hardware-efficient imagenet classification: single-path nas achieves 74.96% top-1 accuracy on imagenet with 79ms latency on a pixel 1 phone, which is state-of-the-art accuracy compared to nas methods with similar constraints (<80ms). 3. nas efficiency: single-path nas search cost is only 8 epochs (30 tpu-hours), which is up to 5,000x faster compared to prior work. 4. reproducibility: unlike all recent mobile-efficient nas methods which only release pretrained models, we open-source our entire codebase at: https://github.com/dstamoulis/single-path-nas.",,2019-04-05,,"['dimitrios stamoulis', 'ruizhou ding', 'di wang', 'dimitrios lymberopoulos', 'bodhi priyantha', 'jie liu', 'diana marculescu']"
410,1904.02880,predictive density estimation under the wasserstein loss,math.st stat.th,we investigate predictive density estimation under the $l^2$ wasserstein loss for location families and location-scale families. we show that plug-in densities form a complete class and that the bayesian predictive density is given by the plug-in density with the posterior mean of the location and scale parameters. we provide bayesian predictive densities that dominate the best equivariant one in normal models.,,2019-04-05,,"['takeru matsuda', 'william e. strawderman']"
411,1904.02883,on missing label patterns in semi-supervised learning,stat.me math.st stat.th,"we investigate model based classification with partially labelled training data. in many biostatistical applications, labels are manually assigned by experts, who may leave some observations unlabelled due to class uncertainty. we analyse semi-supervised learning as a missing data problem and identify situations where the missing label pattern is non-ignorable for the purposes of maximum likelihood estimation. in particular, we find that a relationship between classification difficulty and the missing label pattern implies a non-ignorable missingness mechanism. we examine a number of real datasets and conclude the pattern of missing labels is related to the difficulty of classification. we propose a joint modelling strategy involving the observed data and the missing label mechanism to account for the systematic missing labels. full likelihood inference including the missing label mechanism can improve the efficiency of parameter estimation, and increase classification accuracy.",,2019-04-05,,"['daniel ahfock', 'geoffrey j. mclachlan']"
412,1904.02910,blind deconvolution microscopy using cycle consistent cnn with explicit   psf layer,cs.lg cs.cv stat.ml,"deconvolution microscopy has been extensively used to improve the resolution of the widefield fluorescent microscopy. conventional approaches, which usually require the point spread function (psf) measurement or blind estimation, are however computationally expensive. recently, cnn based approaches have been explored as a fast and high performance alternative. in this paper, we present a novel unsupervised deep neural network for blind deconvolution based on cycle consistency and psf modeling layers. in contrast to the recent cnn approaches for similar problem, the explicit psf modeling layers improve the robustness of the algorithm. experimental results confirm the efficacy of the algorithm.",,2019-04-05,,"['sungjun lim', 'sang-eun lee', 'sunghoe chang', 'jong chul ye']"
413,1904.02921,simulation of virtual cohorts increases predictive accuracy of cognitive   decline in mci subjects,stat.me cs.lg stat.ap,"the ability to predict the progression of biomarkers, notably in ndd, is limited by the size of the longitudinal data sets, in terms of number of patients, number of visits per patients and total follow-up time. to this end, we introduce a data augmentation technique that is able to reproduce the variability seen in a longitudinal training data set and simulate continuous biomarkers trajectories for any number of virtual patients. thanks to this simulation framework, we propose to transform the training set into a simulated data set with more patients, more time-points per patient and longer follow-up duration. we illustrate this approach on the prediction of the mmse of mci subjects of the adni data set. we show that it allows to reach predictions with errors comparable to the noise in the data, estimated in test/retest studies, achieving a improvement of 37% of the mean absolute error compared to the same non-augmented model.",,2019-04-05,,"['igor koval', 'stéphanie allassonnière', 'stanley durrleman']"
414,1904.02958,logitron: perceptron-augmented classification model based on an extended   logistic loss function,cs.lg cs.it math.it stat.ml,"classification is the most important process in data analysis. however, due to the inherent non-convex and non-smooth structure of the zero-one loss function of the classification model, various convex surrogate loss functions such as hinge loss, squared hinge loss, logistic loss, and exponential loss are introduced. these loss functions have been used for decades in diverse classification models, such as svm (support vector machine) with hinge loss, logistic regression with logistic loss, and adaboost with exponential loss and so on. in this work, we present a perceptron-augmented convex classification framework, {\it logitron}. the loss function of it is a smoothly stitched function of the extended logistic loss with the famous perceptron loss function. the extended logistic loss function is a parameterized function established based on the extended logarithmic function and the extended exponential function. the main advantage of the proposed logitron classification model is that it shows the connection between svm and logistic regression via polynomial parameterization of the loss function. in more details, depending on the choice of parameters, we have the hinge-logitron which has the generalized $k$-th order hinge-loss with an additional $k$-th root stabilization function and the logistic-logitron which has a logistic-like loss function with relatively large $|k|$. interestingly, even $k=-1$, hinge-logitron satisfies the classification-calibration condition and shows reasonable classification performance with low computational cost. the numerical experiment in the linear classifier framework demonstrates that hinge-logitron with $k=4$ (the fourth-order svm with the fourth root stabilization function) outperforms logistic regression, svm, and other logitron models in terms of classification accuracy.",,2019-04-05,,['hyenkyun woo']
415,1904.02961,analytic evaluation of the fractional moments for the quasi-stationary   distribution of the shiryaev martingale on an interval,stat.co math.pr stat.ot,"we consider the quasi-stationary distribution of the classical shiryaev diffusion restricted to the interval $[0,a]$ with absorption at a fixed $a>0$. we derive analytically a closed-form formula for the distribution's fractional moment of an {\em arbitrary} given order $s\in\mathbb{r}$; the formula is consistent with that previously found by polunchenko and pepelyshev (2018) for the case of $s\in\mathbb{n}$. we also show by virtue of the formula that, if $s<1$, then the $s$-th fractional moment of the quasi-stationary distribution becomes that of the exponential distribution (with mean $1/2$) in the limit as $a\to+\infty$; the limiting exponential distribution is the stationary distribution of the reciprocal of the shiryaev diffusion.",,2019-04-05,,"['kexuan li', 'aleksey s. polunchenko', 'andrey pepelyshev']"
416,1904.02965,aggregated kernel based tests for signal detection in a regression model,math.st stat.th,"considering a regression model, we address the question of testing the nullity of the regression function. the testing procedure is available when the variance of the observations is unknown and does not depend on any prior information on the alternative. we first propose a single testing procedure based on a general symmetrickernel and an estimation of the variance of the observations. the corresponding critical values are constructed to obtain non asymptotic level-? tests. we then introduce an aggregation procedure to avoid the difficult choice of the kernel and of the parameters of the kernel. the multiple tests satisfy non-asymptotic properties and are adaptive in the minimax sense over several classes of regular alternatives.",10.13140/rg.2.2.22895.28325,2019-04-05,,['thi thien trang bui']
417,1904.02966,rare event simulation for steady-state probabilities via recurrency   cycles,math.pr stat.co,"we develop a new algorithm for the estimation of rare event probabilities associated with the steady-state of a markov stochastic process with continuous state space $\mathbb r^d$ and discrete time steps (i.e. a discrete-time $\mathbb r^d$-valued markov chain). the algorithm, which we coin recurrent multilevel splitting (rms), relies on the markov chain's underlying recurrent structure, in combination with the multilevel splitting method. extensive simulation experiments are performed, including experiments with a nonlinear stochastic model that has some characteristics of complex climate models. the numerical experiments show that rms can boost the computational efficiency by several orders of magnitude compared to the monte carlo method.",10.1063/1.5080296,2019-04-05,,"['krzysztof bisewski', 'daan crommelin', 'michel mandjes']"
418,1904.02971,a topological data analysis based classification method for multiple   measurements,cs.lg math.at stat.ml,"machine learning models for repeated measurements are limited. using topological data analysis (tda), we present a classifier for repeated measurements which samples from the data space and builds a network graph based on the data topology. when applying this to two case studies, accuracy exceeds alternative models with additional benefits such as reporting data subsets with high purity along with feature values. for 300 examples of 3 tree species, the accuracy reached 80% after 30 datapoints, which was improved to 90% after increased sampling to 400 datapoints. using data from 100 examples of each of 6 point processes, the classifier achieved 96.8% accuracy. in both datasets, the tda classifier outperformed an alternative model. this algorithm and software can be beneficial for repeated measurement data common in biological sciences, as both an accurate classifier and a feature selection tool.",,2019-04-05,,"['henri riihimäki', 'wojciech chachólski', 'jakob theorell', 'jan hillert', 'ryan ramanujam']"
419,1904.03012,quantitative system risk assessment from incomplete data with belief   networks and pairwise comparison elicitation,stat.me,"a method for conducting bayesian elicitation and learning in risk assessment is presented. it assumes that the risk process can be described as a fault tree. this is viewed as a belief network, for which prior distributions on primary event probabilities are elicited by means of a pairwise comparison approach. a bayesian updating procedure, following observation of some or all of the events in the fault tree, is described. the application is illustrated through the motivating example of risk assessment of spacecraft explosion during controlled re-entry.",,2019-04-05,,"['cristina de persis', 'jose luis bosque', 'irene huertas', 'simon paul wilson']"
420,1904.03028,optimal rate-exponent region for a class of hypothesis testing against   conditional independence problems,cs.it math.it math.st stat.th,"we study a class of distributed hypothesis testing against conditional independence problems. under the criterion that stipulates minimization of the type ii error rate subject to a (constant) upper bound $\epsilon$ on the type i error rate, we characterize the set of encoding rates and exponent for both discrete memoryless and memoryless vector gaussian settings.",,2019-04-04,,"['abdellatif zaidi', 'inaki estella aguerri']"
421,1904.03061,a literature study of embeddings on source code,cs.lg cs.pl cs.se stat.ml,"natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. recently, the same idea has been applied on source code with encouraging results. in this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. the articles in this survey have been collected by asking authors of related work and with an extensive search on google scholar. each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. we also provide links to experimental data and show some remarkable visualization of code embeddings. in summary, word embedding has been successfully applied on different granularities of source code. with access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.",,2019-04-05,,"['zimin chen', 'martin monperrus']"
422,1904.03063,bayesian heatmaps: probabilistic classification with multiple unreliable   information sources,cs.lg stat.ml,"unstructured data from diverse sources, such as social media and aerial imagery, can provide valuable up-to-date information for intelligent situation assessment. mining these different information sources could bring major benefits to applications such as situation awareness in disaster zones and mapping the spread of diseases. such applications depend on classifying the situation across a region of interest, which can be depicted as a spatial ""heatmap"". annotating unstructured data using crowdsourcing or automated classifiers produces individual classifications at sparse locations that typically contain many errors. we propose a novel bayesian approach that models the relevance, error rates and bias of each information source, enabling us to learn a spatial gaussian process classifier by aggregating data from multiple sources with varying reliability and relevance. our method does not require gold-labelled data and can make predictions at any location in an area of interest given only sparse observations. we show empirically that our approach can handle noisy and biased data sources, and that simultaneously inferring reliability and transferring information between neighbouring reports leads to more accurate predictions. we demonstrate our method on two real-world problems from disaster response, showing how our approach reduces the amount of crowdsourced data required and can be used to generate valuable heatmap visualisations from sms messages and satellite images.",,2019-04-05,,"['edwin simpson', 'steven reece', 'stephen j. roberts']"
423,1904.03099,diverse personalized recommendations with uncertainty from implicit   preference data with the bayesian mallows model,stat.me,"clicking data, which exists in abundance and contains objective user preference information, is widely used to produce personalized recommendations in web-based applications. current popular recommendation algorithms, typically based on matrix factorizations, often have high accuracy and achieve good clickthrough rates. however, diversity of the recommended items, which can greatly enhance user experiences, is often overlooked. moreover, most algorithms do not produce interpretable uncertainty quantifications of the recommendations. in this work, we propose the bayesian mallows for clicking data (bmcd) method, which augments clicking data into compatible full ranking vectors by enforcing all the clicked items to be top-ranked. user preferences are learned using a mallows ranking model. bayesian inference leads to interpretable uncertainties of each individual recommendation, and we also propose a method to make personalized recommendations based on such uncertainties. with a simulation study and a real life data example, we demonstrate that compared to state-of-the-art matrix factorization, bmcd makes personalized recommendations with similar accuracy, while achieving much higher level of diversity, and producing interpretable and actionable uncertainty estimation.",,2019-04-05,,"['qinghua liu', 'andrew henry reiner', 'arnoldo frigessi', 'ida scheel']"
424,1904.03136,estimation of monge matrices,math.st cs.it cs.lg math.it stat.ml stat.th,"monge matrices and their permuted versions known as pre-monge matrices naturally appear in many domains across science and engineering. while the rich structural properties of such matrices have long been leveraged for algorithmic purposes, little is known about their impact on statistical estimation. in this work, we propose to view this structure as a shape constraint and study the problem of estimating a monge matrix subject to additive random noise. more specifically, we establish the minimax rates of estimation of monge and pre-monge matrices. in the case of pre-monge matrices, the minimax-optimal least-squares estimator is not efficiently computable, and we propose two efficient estimators and establish their rates of convergence. our theoretical findings are supported by numerical experiments.",,2019-04-05,,"['jan-christian hütter', 'cheng mao', 'philippe rigollet', 'elina robeva']"
425,1904.03170,diversified hidden markov models for sequential labeling,cs.lg stat.ml,"labeling of sequential data is a prevalent meta-problem for a wide range of real world applications. while the first-order hidden markov models (hmm) provides a fundamental approach for unsupervised sequential labeling, the basic model does not show satisfying performance when it is directly applied to real world problems, such as part-of-speech tagging (pos tagging) and optical character recognition (ocr). aiming at improving performance, important extensions of hmm have been proposed in the literatures. one of the common key features in these extensions is the incorporation of proper prior information. in this paper, we propose a new extension of hmm, termed diversified hidden markov models (dhmm), which utilizes a diversity-encouraging prior over the state-transition probabilities and thus facilitates more dynamic sequential labellings. specifically, the diversity is modeled by a continuous determinantal point process prior, which we apply to both unsupervised and supervised scenarios. learning and inference algorithms for dhmm are derived. empirical evaluations on benchmark datasets for unsupervised pos tagging and supervised ocr confirmed the effectiveness of dhmm, with competitive performance to the state-of-the-art.",10.1109/tkde.2015.2433262,2019-04-05,,"['maoying qiao', 'wei bian', 'richard yida xu', 'dacheng tao']"
426,1904.03171,on shrinkage estimation for balanced loss functions,math.st stat.th,"the estimation of a multivariate mean $\theta$ is considered under natural modifications of balanced loss function of the form: (i) $\omega \, \rho(\|\delta-\delta_0\|^2) + (1-\omega) \, \rho(\|\delta-\theta\|^2) $, and (ii) $\ell \left( \omega \, \|\delta-\delta_0\|^2 + (1-\omega) \, \|\delta-\theta\|^2 \right)\,$, where $\delta_0$ is a target estimator of $\gamma(\theta)$. after briefly reviewing known results for original balanced loss with identity $\rho$ or $\ell$, we provide, for increasing and concave $\rho$ and $\ell$ which also satisfy a completely monotone property, baranchik-type estimators of $\theta$ which dominate the benchmark $\delta_0(x)=x$ for $x$ either distributed as multivariate normal or as a scale mixture of normals. implications are given with respect to model robustness and simultaneous dominance with respect to either $\rho$ or $\ell",,2019-04-05,,"['éric marchand', 'william e. strawderman']"
427,1904.03246,spatial cusum for signal region detection,stat.me cs.lg stat.ml,"detecting weak clustered signal in spatial data is important but challenging in applications such as medical image and epidemiology. a more efficient detection algorithm can provide more precise early warning, and effectively reduce the decision risk and cost. to date, many methods have been developed to detect signals with spatial structures. however, most of the existing methods are either too conservative for weak signals or computationally too intensive. in this paper, we consider a novel method named spatial cusum (scusum), which employs the idea of the cusum procedure and false discovery rate controlling. we develop theoretical properties of the method which indicates that asymptotically scusum can reach high classification accuracy. in the simulation study, we demonstrate that scusum is sensitive to weak spatial signals. this new method is applied to a real fmri dataset as illustration, and more irregular weak spatial signals are detected in the images compared to some existing methods, including the conventional fdr, fdr$_l$ and scan statistics.",,2019-04-05,,"['xin zhang', 'zhengyuan zhu']"
428,1904.03259,is 'unsupervised learning' a misconceived term?,cs.lg cs.ai cs.cv stat.ml,"is all of machine learning supervised to some degree? the field of machine learning has traditionally been categorized pedagogically into $supervised~vs~unsupervised~learning$; where supervised learning has typically referred to learning from labeled data, while unsupervised learning has typically referred to learning from unlabeled data. in this paper, we assert that all machine learning is in fact supervised to some degree, and that the scope of supervision is necessarily commensurate to the scope of learning potential. in particular, we argue that clustering algorithms such as k-means, and dimensionality reduction algorithms such as principal component analysis, variational autoencoders, and deep belief networks are each internally supervised by the data itself to learn their respective representations of its features. furthermore, these algorithms are not capable of external inference until their respective outputs (clusters, principal components, or representation codes) have been identified and externally labeled in effect. as such, they do not suffice as examples of unsupervised learning. we propose that the categorization `supervised vs unsupervised learning' be dispensed with, and instead, learning algorithms be categorized as either $internally~or~externally~supervised$ (or both). we believe this change in perspective will yield new fundamental insights into the structure and character of data and of learning algorithms.",,2019-04-05,,['stephen g. odaibo']
429,1904.03272,convex optimization for the densest subgraph and densest submatrix   problems,math.oc cs.lg stat.ml,"we consider the densest $k$-subgraph problem, which seeks to identify the $k$-node subgraph of a given input graph with maximum number of edges. this problem is well-known to be np-hard, by reduction to the maximum clique problem. we propose a new convex relaxation for the densest $k$-subgraph problem, based on a nuclear norm relaxation of a low-rank plus sparse decomposition of the adjacency matrices of $k$-node subgraphs to partially address this intractability. we establish that the densest $k$-subgraph can be recovered with high probability from the optimal solution of this convex relaxation if the input graph is randomly sampled from a distribution of random graphs constructed to contain an especially dense $k$-node subgraph with high probability. specifically, the relaxation is exact when the edges of the input graph are added independently at random, with edges within a particular $k$-node subgraph added with higher probability than other edges in the graph. we provide a sufficient condition on the size of this subgraph $k$ and the expected density under which the optimal solution of the proposed relaxation recovers this $k$-node subgraph with high probability. further, we propose a first-order method for solving this relaxation based on the alternating direction method of multipliers, and empirically confirm our predicted recovery thresholds using simulations involving randomly generated graphs, as well as graphs drawn from social and collaborative networks.",,2019-04-05,,"['polina bombina', 'brendan ames']"
430,1904.03275,robust subspace recovery with adversarial outliers,cs.lg math.oc stat.ml,"we study the problem of robust subspace recovery (rsr) in the presence of adversarial outliers. that is, we seek a subspace that contains a large portion of a dataset when some fraction of the data points are arbitrarily corrupted. we first examine a theoretical estimator that is intractable to calculate and use it to derive information-theoretic bounds of exact recovery. we then propose two tractable estimators: a variant of ransac and a simple relaxation of the theoretical estimator. the two estimators are fast to compute and achieve state-of-the-art theoretical performance in a noiseless rsr setting with adversarial outliers. the former estimator achieves better theoretical guarantees in the noiseless case, while the latter estimator is robust to small noise, and its guarantees significantly improve with non-adversarial models of outliers. we give a complete comparison of guarantees for the adversarial rsr problem, as well as a short discussion on the estimation of affine subspaces.",,2019-04-05,,"['tyler maunu', 'gilad lerman']"
431,1904.03276,synthesized policies for transfer and adaptation across tasks and   environments,cs.lg stat.ml,"the ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. in this paper, we consider the problem of learning to simultaneously transfer across both environments (env) and tasks (task), probably more importantly, by learning from only sparse (env, task) pairs out of all the possible combinations. we propose a novel compositional neural network architecture which depicts a meta rule for composing policies from the environment and task embeddings. notably, one of the main challenges is to learn the embeddings jointly with the meta rule. we further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. experiments on gridworld and thor, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (env, task) pairs after learning from only 40\% of them.",,2019-04-05,,"['hexiang hu', 'liyu chen', 'boqing gong', 'fei sha']"
432,1904.03295,multi-preference actor critic,cs.lg cs.ai stat.ml,"policy gradient algorithms typically combine discounted future rewards with an estimated value function, to compute the direction and magnitude of parameter updates. however, for most reinforcement learning tasks, humans can provide additional insight to constrain the policy learning. we introduce a general method to incorporate multiple different feedback channels into a single policy gradient loss. in our formulation, the multi-preference actor critic (m-pac), these different types of feedback are implemented as constraints on the policy. we use a lagrangian relaxation to satisfy these constraints using gradient descent while learning a policy that maximizes rewards. experiments in atari and pendulum verify that constraints are being respected and can accelerate the learning process.",,2019-04-05,,"['ishan durugkar', 'matthew hausknecht', 'adith swaminathan', 'patrick macalpine']"
433,1904.03335,local regularization of noisy point clouds: improved global geometric   estimates and data analysis,stat.ml cs.lg,"several data analysis techniques employ similarity relationships between data points to uncover the intrinsic dimension and geometric structure of the underlying data-generating mechanism. in this paper we work under the model assumption that the data is made of random perturbations of feature vectors lying on a low-dimensional manifold. we study two questions: how to define the similarity relationship over noisy data points, and what is the resulting impact of the choice of similarity in the extraction of global geometric information from the underlying manifold. we provide concrete mathematical evidence that using a local regularization of the noisy data to define the similarity improves the approximation of the hidden euclidean distance between unperturbed points. furthermore, graph-based objects constructed with the locally regularized similarity function satisfy better error bounds in their recovery of global geometric ones. our theory is supported by numerical experiments that demonstrate that the gain in geometric understanding facilitated by local regularization translates into a gain in classification accuracy in simulated and real data.",,2019-04-05,,"['nicolas garcia trillos', 'daniel sanz-alonso', 'ruiyi yang']"
434,1904.03367,reinforcement learning with attention that works: a self-supervised   approach,cs.lg stat.ml,"attention models have had a significant positive impact on deep learning across a range of tasks. however previous attempts at integrating attention with reinforcement learning have failed to produce significant improvements. we propose the first combination of self attention and reinforcement learning that is capable of producing significant improvements, including new state of the art results in the arcade learning environment. unlike the selective attention models used in previous attempts, which constrain the attention via preconceived notions of importance, our implementation utilises the markovian properties inherent in the state input. our method produces a faithful visualisation of the policy, focusing on the behaviour of the agent. our experiments demonstrate that the trained policies use multiple simultaneous foci of attention, and are able to modulate attention over time to deal with situations of partial observability.",,2019-04-06,,"['anthony manchin', 'ehsan abbasnejad', 'anton van den hengel']"
435,1904.03401,idealize - a notion of idea strength,cs.ir stat.ap,"business entrepreneurs frequently thrive on looking for ways to test business ideas, without giving too much information. recent techniques in startup development promote the use of surveys to measure the potential client's interest. in this preliminary report, we describe the concept behind idealize, a shiny r application to measure the local trend strength of a potential idea. additionally, the system might provide a relative distance to the capital city of the country. the tests were made for the united states of america, i.e., made available regarding native english language. this report shows some of the tests results with this system.",,2019-04-06,,['rui portocarrero sarmento']
436,1904.03416,learning problem-agnostic speech representations from multiple   self-supervised tasks,cs.lg cs.sd eess.as stat.ml,"learning good representations without supervision is still an open issue in machine learning, and is particularly challenging for speech signals, which are often characterized by long sequences with a complex hierarchical structure. some recent works, however, have shown that it is possible to derive useful speech representations by employing a self-supervised encoder-discriminator approach. this paper proposes an improved self-supervised method, where a single neural encoder is followed by multiple workers that jointly solve different self-supervised tasks. the needed consensus across different tasks naturally imposes meaningful constraints to the encoder, contributing to discover general representations and to minimize the risk of learning superficial ones. experiments show that the proposed approach can learn transferable, robust, and problem-agnostic features that carry on relevant information from the speech signal, such as speaker identity, phonemes, and even higher-level features such as emotional cues. in addition, a number of design choices make the encoder easily exportable, facilitating its direct usage or adaptation to different problems.",,2019-04-06,,"['santiago pascual', 'mirco ravanelli', 'joan serrà', 'antonio bonafonte', 'yoshua bengio']"
437,1904.03444,bus travel time prediction: a lognormal auto-regressive (ar) modeling   approach,stat.ap,"providing real time information about the arrival time of the transit buses has become inevitable in urban areas to make the system more user-friendly and advantageous over various other transportation modes. however, accurate prediction of arrival time of buses is still a challenging problem in dynamically varying traffic conditions especially under heterogeneous traffic condition without lane discipline. one broad approach researchers have adopted over the years is to segment the entire bus route into segments and work with these segment travel times as the data input (from gps traces) for prediction. this paper adopts this approach and proposes predictive modelling approaches which fully exploit the temporal correlations in the bus gps data. specifically, we propose two approaches: (a) classical time-series approach employing a seasonal ar model (b)unconventional linear, non-stationary ar approach. the second approach is a novel technique and exploits the notion of partial correlation for learning from data. a detailed analysis of the marginal distributions of the data from indian conditions (used here), revealed a predominantly log-normal behavior. this aspect was incorporated into the above proposed predictive models and statistically optimal prediction schemes in the lognormal sense are utilized for all predictions. both the above temporal predictive modeling approaches predict ahead in time at each segment independently. for real-time bus travel time prediction, one however needs to predict across multiple segments ahead in space. towards a complete solution, the study also proposes an intelligent procedure to perform (real-time) multi-section ahead travel-time predictions based on either of the above proposed temporal models. results showed a clear improvement in prediction accuracy using the proposed methods,",,2019-04-06,,"['b. dhivyabharathi', 'b. anil kumar', 'avinash achar', 'lelitha vanajakshi']"
438,1904.03469,tulip: a toolbox for linear discriminant analysis with penalties,stat.co,"linear discriminant analysis (lda) is a powerful tool in building classifiers with easy computation and interpretation. recent advancements in science technology have led to the popularity of datasets with high dimensions, high orders and complicated structure. such datasetes motivate the generalization of lda in various research directions. the r package tulip integrates several popular high-dimensional lda-based methods and provides a comprehensive and user-friendly toolbox for linear, semi-parametric and tensor-variate classification. functions are included for model fitting, cross validation and prediction. in addition, motivated by datasets with diverse sources of predictors, we further include functions for covariate adjustment. our package is carefully tailored for low storage and high computation efficiency. moreover, our package is the first r package for many of these methods, providing great convenience to researchers in this area.",,2019-04-06,,"['yuqing pan', 'qing mai', 'xin zhang']"
439,1904.03491,a compendium on network and host based intrusion detection systems,cs.lg cs.cr cs.ne cs.ni stat.ml,"the techniques of deep learning have become the state of the art methodology for executing complicated tasks from various domains of computer vision, natural language processing, and several other areas. due to its rapid development and promising benchmarks in those fields, researchers started experimenting with this technique to perform in the area of, especially in intrusion detection related tasks. deep learning is a subset and a natural extension of classical machine learning and an evolved model of neural networks. this paper contemplates and discusses all the methodologies related to the leading edge deep learning and neural network models purposing to the arena of intrusion detection systems.",,2019-04-06,,"['rahul-vigneswaran k', 'prabaharan poornachandran', 'soman kp']"
440,1904.03513,team qcri-mit at semeval-2019 task 4: propaganda analysis meets   hyperpartisan news detection,cs.ir cs.cl cs.lg stat.ml,"in this paper, we describe our submission to semeval-2019 task 4 on hyperpartisan news detection. our system relies on a variety of engineered features originally used to detect propaganda. this is based on the assumption that biased messages are propagandistic in the sense that they promote a particular political cause or viewpoint. we trained a logistic regression model with features ranging from simple bag-of-words to vocabulary richness and text readability features. our system achieved 72.9% accuracy on the test data that is annotated manually and 60.8% on the test data that is annotated with distant supervision. additional experiments showed that significant performance improvements can be achieved with better feature pre-processing.",,2019-04-06,,"['abdelrhman saleh', 'ramy baly', 'alberto barrón-cedeño', 'giovanni da san martino', 'mitra mohtarami', 'preslav nakov', 'james glass']"
441,1904.03516,instance-level meta normalization,cs.lg cs.cv stat.ml,"this paper presents a normalization mechanism called instance-level meta normalization (ilm~norm) to address a learning-to-normalize problem. ilm~norm learns to predict the normalization parameters via both the feature feed-forward and the gradient back-propagation paths. ilm~norm provides a meta normalization mechanism and has several good properties. it can be easily plugged into existing instance-level normalization schemes such as instance normalization, layer normalization, or group normalization. ilm~norm normalizes each instance individually and therefore maintains high performance even when small mini-batch is used. the experimental results show that ilm~norm well adapts to different network architectures and tasks, and it consistently improves the performance of the original models. the code is available at url{https://github.com/gasoonjia/ilm-norm.",,2019-04-06,,"['songhao jia', 'ding-jie chen', 'hwann-tzong chen']"
442,1904.03530,a bayesian theory of change detection in statistically periodic random   processes,eess.sp cs.it math.it math.st stat.th,"a new class of stochastic processes called independent and periodically identically distributed (i.p.i.d.) processes is defined to capture periodically varying statistical behavior. a novel bayesian theory is developed for detecting a change in the distribution of an i.p.i.d. process. it is shown that the bayesian change point problem can be expressed as a problem of optimal control of a markov decision process (mdp) with periodic transition and cost structures. optimal control theory is developed for periodic mdps for discounted and undiscounted total cost criteria. a fixed-point equation is obtained that is satisfied by the optimal cost function. it is shown that the optimal policy for the mdp is nonstationary but periodic in nature. a value iteration algorithm is obtained to compute the optimal cost function. the results from the mdp theory are then applied to detect changes in i.p.i.d. processes. it is shown that while the optimal change point algorithm is a stopping rule based on a periodic sequence of thresholds, a single-threshold policy is asymptotically optimal, as the probability of false alarm goes to zero. numerical results are provided to demonstrate that the asymptotically optimal policy is not strictly optimal.",,2019-04-06,,"['taposh banerjee', 'prudhvi gurram', 'gene whipps']"
443,1904.03535,randomised bayesian least-squares policy iteration,cs.lg cs.ai stat.ml,"we introduce bayesian least-squares policy iteration (blspi), an off-policy, model-free, policy iteration algorithm that uses the bayesian least-squares temporal-difference (blstd) learning algorithm to evaluate policies. an online variant of blspi has been also proposed, called randomised blspi (rblspi), that improves its policy based on an incomplete policy evaluation step. in online setting, the exploration-exploitation dilemma should be addressed as we try to discover the optimal policy by using samples collected by ourselves. rblspi exploits the advantage of blstd to quantify our uncertainty about the value function. inspired by thompson sampling, rblspi first samples a value function from a posterior distribution over value functions, and then selects actions based on the sampled value function. the effectiveness and the exploration abilities of rblspi are demonstrated experimentally in several environments.",,2019-04-06,,"['nikolaos tziortziotis', 'christos dimitrakakis', 'michalis vazirgiannis']"
444,1904.03546,robustness of urban road networks based on spatial topological patterns,stat.ap physics.soc-ph,"during the last decade, road network vulnerability assessment has received an increasing attention. on one hand, it is due to the significant advances in network science and the potentialities that its tools offer. on the other hand, it is due to its utility for urban planning and emergency response. despite these facts and the increasingly available data, related work is still sparse in latin america, even more so in ecuador. due to its geographical, historical, and social characteristics, the city of quito is considered as a case study. at first, the spatial distributions of several topological centrality measures are analyzed. as expected, there are hotspots where high values of these measures concentrate. these results serve to further simulate several strategies for disconnecting the urban road network. finally, we observe that centrality-based strategies are more effective than randomly-based strategies in disconnecting the network.",,2019-04-06,,['felipe vaca-ramírez']
445,1904.03548,precision matrix estimation with noisy and missing data,stat.ml cs.lg stat.me,"estimating conditional dependence graphs and precision matrices are some of the most common problems in modern statistics and machine learning. when data are fully observed, penalized maximum likelihood-type estimators have become standard tools for estimating graphical models under sparsity conditions. extensions of these methods to more complex settings where data are contaminated with additive or multiplicative noise have been developed in recent years. in these settings, however, the relative performance of different methods is not well understood and algorithmic gaps still exist. in particular, in high-dimensional settings these methods require using non-positive semidefinite matrices as inputs, presenting novel optimization challenges. we develop an alternating direction method of multipliers (admm) algorithm for these problems, providing a feasible algorithm to estimate precision matrices with indefinite input and potentially nonconvex penalties. we compare this method with existing alternative solutions and empirically characterize the tradeoffs between them. finally, we use this method to explore the networks among us senators estimated from voting records data.",,2019-04-06,,"['roger fan', 'byoungwook jang', 'yuekai sun', 'shuheng zhou']"
446,1904.03549,supervised discrete hashing with relaxation,cs.lg stat.ml,"data-dependent hashing has recently attracted attention due to being able to support efficient retrieval and storage of high-dimensional data such as documents, images, and videos. in this paper, we propose a novel learning-based hashing method called ""supervised discrete hashing with relaxation"" (sdhr) based on ""supervised discrete hashing"" (sdh). sdh uses ordinary least squares regression and traditional zero-one matrix encoding of class label information as the regression target (code words), thus fixing the regression target. in sdhr, the regression target is instead optimized. the optimized regression target matrix satisfies a large margin constraint for correct classification of each example. compared with sdh, which uses the traditional zero-one matrix, sdhr utilizes the learned regression target matrix and, therefore, more accurately measures the classification error of the regression model and is more flexible. as expected, sdhr generally outperforms sdh. experimental results on two large-scale image datasets (cifar-10 and mnist) and a large-scale and challenging face dataset (frgc) demonstrate the effectiveness and efficiency of sdhr.",,2019-04-06,,"['jie gui', 'tongliang liu', 'zhenan sun', 'dacheng tao', 'tieniu tan']"
447,1904.03556,fast supervised discrete hashing,cs.lg stat.ml,"learning-based hashing algorithms are ``hot topics"" because they can greatly increase the scale at which existing methods operate. in this paper, we propose a new learning-based hashing method called ``fast supervised discrete hashing"" (fsdh) based on ``supervised discrete hashing"" (sdh). regressing the training examples (or hash code) to the corresponding class labels is widely used in ordinary least squares regression. rather than adopting this method, fsdh uses a very simple yet effective regression of the class labels of training examples to the corresponding hash code to accelerate the algorithm. to the best of our knowledge, this strategy has not previously been used for hashing. traditional sdh decomposes the optimization into three sub-problems, with the most critical sub-problem - discrete optimization for binary hash codes - solved using iterative discrete cyclic coordinate descent (dcc), which is time-consuming. however, fsdh has a closed-form solution and only requires a single rather than iterative hash code-solving step, which is highly efficient. furthermore, fsdh is usually faster than sdh for solving the projection matrix for least squares regression, making fsdh generally faster than sdh. for example, our results show that fsdh is about 12-times faster than sdh when the number of hashing bits is 128 on the cifar-10 data base, and fsdh is about 151-times faster than fasthash when the number of hashing bits is 64 on the mnist data-base. our experimental results show that fsdh is not only fast, but also outperforms other comparative methods.",,2019-04-06,,"['jie gui', 'tongliang liu', 'zhenan sun', 'dacheng tao', 'tieniu tan']"
448,1904.03559,statistical meaning of mean functions,math.st stat.th,"the basic properties of the fisher information allow to reveal the statistical meaning of classical inequalities between mean functions. the properties applied to scale mixtures of gaussian distributions lead to a new mean function of purely statistical origin, unrelated to the classical arithmetic, geometric, and harmonic means. we call it the informational mean and show that when the arguments of the mean functions are hermitian positive definite matrices, not necessarily commuting, the informational mean lies between the arithmetic and harmonic means, playing, in a sense, the role of the geometric mean that cannot be correctly defined in case of non-commuting matrices.\\ surprisingly the monotonicity and additivity properties of the fisher information lead to a new generalization of the classical inequality between the arithmetic and harmonic means.",,2019-04-06,,"['abram m. kagan', 'paul j. smith']"
449,1904.03579,adaptively connected neural networks,cs.cv stat.ml,"this paper presents a novel adaptively connected neural network (acnet) to improve the traditional convolutional neural networks (cnns) {in} two aspects. first, acnet employs a flexible way to switch global and local inference in processing the internal feature representations by adaptively determining the connection status among the feature nodes (e.g., pixels of the feature maps) \footnote{in a computer vision domain, a node refers to a pixel of a feature map{, while} in {the} graph domain, a node denotes a graph node.}. we can show that existing cnns, the classical multilayer perceptron (mlp), and the recently proposed non-local network (nln) \cite{nonlocalnn17} are all special cases of acnet. second, acnet is also capable of handling non-euclidean data. extensive experimental analyses on {a variety of benchmarks (i.e.,} imagenet-1k classification, coco 2017 detection and segmentation, cuhk03 person re-identification, cifar analysis, and cora document categorization) demonstrate that {acnet} cannot only achieve state-of-the-art performance but also overcome the limitation of the conventional mlp and cnn \footnote{corresponding author: liang lin (linliang@ieee.org)}. the code is available at \url{https://github.com/wanggrun/adaptively-connected-neural-networks}.",,2019-04-07,,"['guangrun wang', 'keze wang', 'liang lin']"
450,1904.03595,joint learning of pre-trained and random units for domain adaptation in   part-of-speech tagging,cs.cl cs.lg stat.ml,"fine-tuning neural networks is widely used to transfer valuable knowledge from high-resource to low-resource domains. in a standard fine-tuning scheme, source and target problems are trained using the same architecture. although capable of adapting to new domains, pre-trained units struggle with learning uncommon target-specific patterns. in this paper, we propose to augment the target-network with normalised, weighted and randomly initialised units that beget a better adaptation while maintaining the valuable source knowledge. our experiments on pos tagging of social media texts (tweets domain) demonstrate that our method achieves state-of-the-art performances on 3 commonly used datasets.",,2019-04-07,,"['sara meftah', 'youssef tamaazousti', 'nasredine semmar', 'hassane essafi', 'fatiha sadat']"
451,1904.03602,competitive ratio versus regret minimization: achieving the best of both   worlds,cs.lg cs.ds stat.ml,"we consider online algorithms under both the competitive ratio criteria and the regret minimization one. our main goal is to build a unified methodology that would be able to guarantee both criteria simultaneously.   for a general class of online algorithms, namely any metrical task system (mts), we show that one can simultaneously guarantee the best known competitive ratio and a natural regret bound. for the paging problem we further show an efficient online algorithm (polynomial in the number of pages) with this guarantee.   to this end, we extend an existing regret minimization algorithm (specifically, kapralov and panigrahy) to handle movement cost (the cost of switching between states of the online system). we then show how to use the extended regret minimization algorithm to combine multiple online algorithms. our end result is an online algorithm that can combine a ""base"" online algorithm, having a guaranteed competitive ratio, with a range of online algorithms that guarantee a small regret over any interval of time. the combined algorithm guarantees both that the competitive ratio matches that of the base algorithm and a low regret over any time interval.   as a by product, we obtain an expert algorithm with close to optimal regret bound on every time interval, even in the presence of switching costs. this result is of independent interest.",,2019-04-07,,"['amit daniely', 'yishay mansour']"
452,1904.03620,teaching gans to sketch in vector format,cs.gr cs.lg stat.ml,"sketching is more fundamental to human cognition than speech. deep neural networks (dnns) have achieved the state-of-the-art in speech-related tasks but have not made significant development in generating stroke-based sketches a.k.a sketches in vector format. though there are variational auto encoders (vaes) for generating sketches in vector format, there is no generative adversarial network (gan) architecture for the same. in this paper, we propose a standalone gan architecture skegan and a vae-gan architecture vaskegan, for sketch generation in vector format. skegan is a stochastic policy in reinforcement learning (rl), capable of generating both multidimensional continuous and discrete outputs. vaskegan hybridizes a vae and a gan, in order to couple the efficient representation of data by vae with the powerful generating capabilities of a gan, to produce visually appealing sketches. we also propose a new metric called the ske-score which quantifies the quality of vector sketches. we have validated that skegan and vaskegan generate visually appealing sketches by using human turing test and ske-score.",,2019-04-07,,"['varshaneya v', 's balasubramanian', 'vineeth n balasubramanian']"
453,1904.03643,ensemble patch transformation: a new tool for signal decomposition,eess.sp stat.me,"this paper considers the problem of signal decomposition and data visualization. for this purpose, we introduce a new multiscale transform, termed `ensemble patch transformation' that enhances identification of local characteristics embedded in a signal and provides multiscale visualization according to different levels; hence, it is useful for data analysis and signal decomposition. in literature, there are data-adaptive decomposition methods such as empirical mode decomposition (emd) by huang et al. (1998). along the same line of emd, we propose a new decomposition algorithm that extracts meaningful components from a signal that belongs to a large class of signals, compared to the previous methods. some theoretical properties of the proposed algorithm are investigated. to evaluate the proposed method, we analyze several synthetic examples and a real-world signal.",,2019-04-07,,"['donghoh kim', 'guebin choi', 'hee-seok oh']"
454,1904.03646,policy gradient search: online planning and expert iteration without   search trees,cs.lg stat.ml,"monte carlo tree search (mcts) algorithms perform simulation-based search to improve policies online. during search, the simulation policy is adapted to explore the most promising lines of play. mcts has been used by state-of-the-art programs for many problems, however a disadvantage to mcts is that it estimates the values of states with monte carlo averages, stored in a search tree; this does not scale to games with very high branching factors. we propose an alternative simulation-based search method, policy gradient search (pgs), which adapts a neural network simulation policy online via policy gradient updates, avoiding the need for a search tree. in hex, pgs achieves comparable performance to mcts, and an agent trained using expert iteration with pgs was able defeat mohex 2.0, the strongest open-source hex agent, in 9x9 hex.",,2019-04-07,,"['thomas anthony', 'robert nishihara', 'philipp moritz', 'tim salimans', 'john schulman']"
455,1904.03688,proposing a localized relevance vector machine for pattern   classification,cs.lg stat.ml,"relevance vector machine (rvm) can be seen as a probabilistic version of support vector machines which is able to produce sparse solutions by linearly weighting a small number of basis functions instead using all of them. regardless of a few merits of rvm such as giving probabilistic predictions and relax of parameter tuning, it has poor prediction for test instances that are far away from the relevance vectors. as a solution, we propose a new combination of rvm and k-nearest neighbor (k-nn) rule which resolves this issue with regionally dealing with every test instance. in our settings, we obtain the relevance vectors for each test instance in the local area given by k-nn rule. in this way, relevance vectors are closer and more relevant to the test instance which results in a more accurate model. this can be seen as a piece-wise learner which locally classifies test instances. the model is hence called localized relevance vector machine (lrvm). the lrvm is examined on several datasets of the university of california, irvine (uci) repository. results supported by statistical tests indicate that the performance of lrvm is competitive as compared with a few state-of-the-art classifiers.",,2019-04-07,,"['farhood rismanchian', 'karim rahimian']"
456,1904.03717,bayesian influence diagnostics using normalizing functional bregman   divergence,stat.ap,"ideally, any statistical inference should be robust to local influences. although there are simple ways to check about leverage points in independent and linear problems, more complex models require more sophisticated methods. kullback-leiber and bregman divergences were already applied in bayesian inference to measure the isolated impact of each observation in a model. we extend these ideas to models for dependent data and with non-normal probability distributions such as time series, spatial models and generalized linear models. we also propose a strategy to rescale the functional bregman divergence to lie in the (0,1) interval thus facilitating interpretation and comparison. this is accomplished with a minimal computational effort and maintaining all theoretical properties. for computational efficiency, we take advantage of hamiltonian monte carlo methods to draw samples from the posterior distribution of model parameters. the resulting markov chains are then directly connected with bregman calculus, which results in fast computation. we check the propositions in both simulated and empirical studies.",,2019-04-07,,"['ian m danilevicz', 'ricardo s ehlers']"
457,1904.03720,an unsupervised transfer learning algorithm for sleep monitoring,stat.ap,"objective: to develop multisensor-wearable-device sleep monitoring algorithms that are robust to health disruptions affecting sleep patterns. methods: we develop an unsupervised transfer learning algorithm based on a multivariate hidden markov model and fisher's linear discriminant analysis, adaptively adjusting to sleep pattern shift by training on dynamics of sleep/wake states. the proposed algorithm operates, without requiring a priori information about true sleep/wake states, by establishing an initial training set with hidden markov model and leveraging a taper window mechanism to learn the sleep pattern in an incremental fashion. our domain-adaptation algorithm is applied to a dataset collected in a human viral challenge study to identify sleep/wake periods of both uninfected and infected participants. results: the algorithm successfully detects sleep/wake sessions in subjects whose sleep patterns are disrupted by respiratory infection (h3n2 flu virus). pre-symptomatic features based on the detected periods are found to be strongly predictive of both infection status (auc = 0.844) and infection onset time (auc = 0.885), indicating the effectiveness and usefulness of the algorithm. conclusion: our method can effectively detect sleep/wake states in the presence of sleep pattern shift. significance: utilizing integrated multisensor signal processing and adaptive training schemes, our algorithm is able to capture key sleep patterns in ambulatory monitoring, leading to better automated sleep assessment and prediction.",,2019-04-07,,"['xichen she', 'yaya zhai', 'ricardo henao', 'christopher w. woods', 'geoffrey s. ginsburg', 'peter x. k. song', 'alfred o. hero']"
458,1904.03743,information bottleneck and its applications in deep learning,cs.lg cs.it math.it stat.ml,"information theory (it) has been used in machine learning (ml) from early days of this field. in the last decade, advances in deep neural networks (dnns) have led to surprising improvements in many applications of ml. the result has been a paradigm shift in the community toward revisiting previous ideas and applications in this new framework. ideas from it are no exception. one of the ideas which is being revisited by many researchers in this new era, is information bottleneck (ib); a formulation of information extraction based on it. the ib is promising in both analyzing and improving dnns. the goal of this survey is to review the ib concept and demonstrate its applications in deep learning. the information theoretic nature of ib, makes it also a good candidate in showing the more general concept of how it can be used in ml. two important concepts are highlighted in this narrative on the subject, i) the concise and universal view that it provides on seemingly unrelated methods of ml, demonstrated by explaining how ib relates to minimal sufficient statistics, stochastic gradient descent, and variational auto-encoders, and ii) the common technical mistakes and problems caused by applying ideas from it, which is discussed by a careful study of some recent methods suffering from them.",,2019-04-07,,"['hassan hafez-kolahi', 'shohreh kasaei']"
459,1904.03767,an irt-based model for omitted and not-reached items,stat.me,"missingness is a common occurrence in educational assessment and psychological measurement. it could not be casually ignored as it may threaten the validity of the test if not handled properly. considering the difference between omitted and not-reached items, we developed an irt-based model to handle these missingness. in the proposed method, not-reached responses are captured by the cumulative missingness. moreover, the nonignorability is attributed to the correlation between ability and person missing trait. we proved that its item parameters estimate under maximum marginal likelihood (mml) estimation is consistent. we further proposed a bayesian estimation procedure using mcmc methods to estimate all the parameters. the simulation results indicate that the model parameters under the proposed method are better recovered than that under listwise deletion, and the nonignorable model fits the simulated nonignorable nonresponses better than ignorable model in terms of bayesian model selection. furthermore, the program for international student assessment (pisa) data set was analyzed to further illustrate the usage of the proposed method.",,2019-04-07,,['jinxin guo']
460,1904.03779,cluster developing 1-bit matrix completion,cs.lg stat.ml,"matrix completion has a long-time history of usage as the core technique of recommender systems. in particular, 1-bit matrix completion, which considers the prediction as a ``recommended'' or ``not recommended'' question, has proved its significance and validity in the field. however, while customers and products aggregate into interacted clusters, state-of-the-art model-based 1-bit recommender systems do not take the consideration of grouping bias. to tackle the gap, this paper introduced group-specific 1-bit matrix completion (gs1mc) by first-time consolidating group-specific effects into 1-bit recommender systems under the low-rank latent variable framework. additionally, to empower gs1mc even when grouping information is unobtainable, cluster developing matrix completion (cdmc) was proposed by integrating the sparse subspace clustering technique into gs1mc. namely, cdmc allows clustering users/items and to leverage their group effects into matrix completion at the same time. experiments on synthetic and real-world data show that gs1mc outperforms the current 1-bit matrix completion methods. meanwhile, it is compelling that cdmc can successfully capture items' genre features only based on sparse binary user-item interactive data. notably, gs1mc provides a new insight to incorporate and evaluate the efficacy of clustering methods while cdmc can be served as a new tool to explore unrevealed social behavior or market phenomenon.",,2019-04-07,,"['chengkun zhang. junbin gao', 'stephen lu']"
461,1904.03807,binary matrix completion with nonconvex regularizers,cs.lg stat.ml,"many practical problems involve the recovery of a binary matrix from partial information, which makes the binary matrix completion (bmc) technique received increasing attention in machine learning. in particular, we consider a special case of bmc problem, in which only a subset of positive elements can be observed. in recent years, convex regularization based methods are the mainstream approaches for this task. however, the applications of nonconvex surrogates in standard matrix completion have demonstrated better empirical performance. accordingly, we propose a novel bmc model with nonconvex regularizers and provide the recovery guarantee for the model. furthermore, for solving the resultant nonconvex optimization problem, we improve the popular proximal algorithm with acceleration strategies. it can be guaranteed that the convergence rate of the algorithm is in the order of ${1/t}$, where $t$ is the number of iterations. extensive experiments conducted on both synthetic and real-world data sets demonstrate the superiority of the proposed approach over other competing methods.",,2019-04-07,,['chunsheng liu']
462,1904.03817,unbiased variance reduction in randomized experiments,math.st stat.th,"this paper develops a flexible method for decreasing the variance of estimators for complex experiment effect metrics (e.g. ratio metrics) while retaining asymptotic unbiasedness. this method uses the auxiliary information about the experiment units to decrease the variance. the method can incorporate almost any arbitrary predictive model (e.g. linear regression, regularization, neural networks) to adjust the estimators. the adjustment involves some free parameters which can be optimized to achieve the smallest variance reduction given the predictive model performance. also we approximate the achievable reduction in variance in fairly general settings mathematically. finally, we use simulations to show the method works.",,2019-04-07,,"['reza hosseini', 'amir najmi']"
463,1904.03825,application of data compression techniques to time series forecasting,cs.it math.it stat.ap,"in this study we show that standard well-known file compression programs (zlib, bzip2, etc.) are able to forecast real-world time series data well. the strength of our approach is its ability to use a set of data compression algorithms and ""automatically"" choose the best one of them during the process of forecasting. besides, modern data-compressors are able to find many kinds of latent regularities using some methods of artificial intelligence (for example, some data-compressors are based on finding the smallest formal grammar that describes the time series). thus, our approach makes it possible to apply some particular methods of artificial intelligence for time-series forecasting.   as examples of the application of the proposed method, we made forecasts for the monthly t-index and the kp-index time series using standard compressors. in both cases, we used the mean absolute error (mae) as an accuracy measure.   for the monthly t-index time series, we made 18 forecasts beyond the available data for each month since january 2011 to july 2017. we show that, in comparison with the forecasts made by the australian bureau of meteorology, our method more accurately predicts one value ahead.   the kp-index time series consists of 3-hour values ranging from 0 to 9. for each day from february 4, 2018 to march 28, 2018, we made forecasts for 24 values ahead. we compared our forecasts with the forecasts made by the space weather prediction center (swpc). the results showed that the accuracy of our method is similar to the accuracy of the swpc's method. as in the previous case, we also obtained more accurate one-step forecasts.",,2019-04-07,,"['k. s. chirikhin', 'b. ya. ryabko']"
464,1904.03837,centripetal sgd for pruning very deep convolutional networks with   complicated structure,cs.lg cs.cv stat.ml,"the redundancy is widely recognized in convolutional neural networks (cnns), which enables to remove unimportant filters from convolutional layers so as to slim the network with acceptable performance drop. inspired by the linear and combinational properties of convolution, we seek to make some filters increasingly close and eventually identical for network slimming. to this end, we propose centripetal sgd (c-sgd), a novel optimization method, which can train several filters to collapse into a single point in the parameter hyperspace. when the training is completed, the removal of the identical filters can trim the network with no performance loss, thus no finetuning is needed. by doing so, we have partly solved an open problem of constrained filter pruning on cnns with complicated structure, where some layers must be pruned following others. our experimental results on cifar-10 and imagenet have justified the effectiveness of c-sgd-based filter pruning. moreover, we have provided empirical evidences for the assumption that the redundancy in deep neural networks helps the convergence of training by showing that a redundant cnn trained using c-sgd outperforms a normally trained counterpart with the equivalent width.",,2019-04-08,,"['xiaohan ding', 'guiguang ding', 'yuchen guo', 'jungong han']"
465,1904.03866,on the learnability of deep random networks,cs.lg stat.ml,"in this paper we study the learnability of deep random networks from both theoretical and practical points of view. on the theoretical front, we show that the learnability of random deep networks with sign activation drops exponentially with its depth. on the practical front, we find that the learnability drops sharply with depth even with the state-of-the-art training methods, suggesting that our stylized theoretical results are closer to reality.",,2019-04-08,,"['abhimanyu das', 'sreenivas gollapudi', 'ravi kumar', 'rina panigrahy']"
466,1904.03901,multi-view matrix completion for multi-label image classification,stat.ml cs.cv cs.lg,"there is growing interest in multi-label image classification due to its critical role in web-based image analytics-based applications, such as large-scale image retrieval and browsing. matrix completion has recently been introduced as a method for transductive (semi-supervised) multi-label classification, and has several distinct advantages, including robustness to missing data and background noise in both feature and label space. however, it is limited by only considering data represented by a single-view feature, which cannot precisely characterize images containing several semantic concepts. to utilize multiple features taken from different views, we have to concatenate the different features as a long vector. but this concatenation is prone to over-fitting and often leads to very high time complexity in mc based image classification. therefore, we propose to weightedly combine the mc outputs of different views, and present the multi-view matrix completion (mvmc) framework for transductive multi-label image classification. to learn the view combination weights effectively, we apply a cross validation strategy on the labeled set. in the learning process, we adopt the average precision (ap) loss, which is particular suitable for multi-label image classification. a least squares loss formulation is also presented for the sake of efficiency, and the robustness of the algorithm based on the ap loss compared with the other losses is investigated. experimental evaluation on two real world datasets (pascal voc' 07 and mir flickr) demonstrate the effectiveness of mvmc for transductive (semi-supervised) multi-label image classification, and show that mvmc can exploit complementary properties of different features and output-consistent labels for improved multi-label image classification.",10.1109/tip.2015.2421309,2019-04-08,,"['yong luo', 'tongliang liu', 'dacheng tao', 'chao xu']"
467,1904.03909,generalized active learning and design of statistical experiments for   manifold-valued data,stat.ml cs.lg,"characterizing the appearance of real-world surfaces is a fundamental problem in multidimensional reflectometry, computer vision and computer graphics. for many applications, appearance is sufficiently well characterized by the bidirectional reflectance distribution function (brdf). we treat brdf measurements as samples of points from high-dimensional non-linear non-convex manifolds. brdf manifolds form an infinite-dimensional space, but typically the available measurements are very scarce for complicated problems such as brdf estimation. therefore, an efficient learning strategy is crucial when performing the measurements.   in this paper, we build the foundation of a mathematical framework that allows to develop and apply new techniques within statistical design of experiments and generalized proactive learning, in order to establish more efficient sampling and measurement strategies for brdf data manifolds.",,2019-04-08,,['mikhail a. langovoy']
468,1904.03911,on learning density aware embeddings,cs.lg cs.cv stat.ml,"deep metric learning algorithms have been utilized to learn discriminative and generalizable models which are effective for classifying unseen classes. in this paper, a novel noise tolerant deep metric learning algorithm is proposed. the proposed method, termed as density aware metric learning, enforces the model to learn embeddings that are pulled towards the most dense region of the clusters for each class. it is achieved by iteratively shifting the estimate of the center towards the dense region of the cluster thereby leading to faster convergence and higher generalizability. in addition to this, the approach is robust to noisy samples in the training data, often present as outliers. detailed experiments and analysis on two challenging cross-modal face recognition databases and two popular object recognition databases exhibit the efficacy of the proposed approach. it has superior convergence, requires lesser training time, and yields better accuracies than several popular deep metric learning methods.",,2019-04-08,,"['soumyadeep ghosh', 'richa singh', 'mayank vatsa']"
469,1904.03921,multi-view vector-valued manifold regularization for multi-label image   classification,stat.ml cs.cv cs.lg,"in computer vision, image datasets used for classification are naturally associated with multiple labels and comprised of multiple views, because each image may contain several objects (e.g. pedestrian, bicycle and tree) and is properly characterized by multiple visual features (e.g. color, texture and shape). currently available tools ignore either the label relationship or the view complementary. motivated by the success of the vector-valued function that constructs matrix-valued kernels to explore the multi-label structure in the output space, we introduce multi-view vector-valued manifold regularization (mv$\mathbf{^3}$mr) to integrate multiple features. mv$\mathbf{^3}$mr exploits the complementary property of different features and discovers the intrinsic local geometry of the compact support shared by different features under the theme of manifold regularization. we conducted extensive experiments on two challenging, but popular datasets, pascal voc' 07 (voc) and mir flickr (mir), and validated the effectiveness of the proposed mv$\mathbf{^3}$mr for image classification.",10.1109/tnnls.2013.2238682,2019-04-08,,"['yong luo', 'dacheng tao', 'chang xu', 'chao xu', 'hong liu', 'yonggang wen']"
470,1904.03943,component-wise boosting of targets for multi-output prediction,stat.ml cs.lg,"multi-output prediction deals with the prediction of several targets of possibly diverse types. one way to address this problem is the so called problem transformation method. this method is often used in multi-label learning, but can also be used for multi-output prediction due to its generality and simplicity. in this paper, we introduce an algorithm that uses the problem transformation method for multi-output prediction, while simultaneously learning the dependencies between target variables in a sparse and interpretable manner. in a first step, predictions are obtained for each target individually. target dependencies are then learned via a component-wise boosting approach. we compare our new method with similar approaches in a benchmark using multi-label, multivariate regression and mixed-type datasets.",,2019-04-08,,"['quay au', 'daniel schalk', 'giuseppe casalicchio', 'ramona schoedel', 'clemens stachl', 'bernd bischl']"
471,1904.03987,early warning in egg production curves from commercial hens: a svm   approach,stat.ap cs.lg,"artificial intelligence allows the improvement of our daily life, for instance, speech and handwritten text recognition, real time translation and weather forecasting are common used applications. in the livestock sector, machine learning algorithms have the potential for early detection and warning of problems, which represents a significant milestone in the poultry industry. production problems generate economic loss that could be avoided by acting in a timely manner. in the current study, training and testing of support vector machines are addressed, for an early detection of problems in the production curve of commercial eggs, using farm's egg production data of 478,919 laying hens grouped in 24 flocks. experiments using support vector machines with a 5 k-fold cross-validation were performed at different previous time intervals, to alert with up to 5 days of forecasting interval, whether a flock will experience a problem in production curve. performance metrics such as accuracy, specificity, sensitivity, and positive predictive value were evaluated, reaching 0-day values of 0.9874, 0.9876, 0.9783 and 0.6518 respectively on unseen data (test-set). the optimal forecasting interval was from zero to three days, performance metrics decreases as the forecasting interval is increased. it should be emphasized that this technique was able to issue an alert a day in advance, achieving an accuracy of 0.9854, a specificity of 0.9865, a sensitivity of 0.9333 and a positive predictive value of 0.6135. this novel application embedded in a computer system of poultry management is able to provide significant improvements in early detection and warning of problems related to the production curve.",10.1016/j.compag.2015.12.009,2019-04-08,,"['iván ramírez morales', 'daniel rivero cebrián', 'enrique fernández blanco', 'alejandro pazos sierra']"
472,1904.03990,import2vec - learning embeddings for software libraries,cs.se cs.ir cs.lg stat.ml,"we consider the problem of developing suitable learning representations (embeddings) for library packages that capture semantic similarity among libraries. such representations are known to improve the performance of downstream learning tasks (e.g. classification) or applications such as contextual search and analogical reasoning.   we apply word embedding techniques from natural language processing (nlp) to train embeddings for library packages (""library vectors""). library vectors represent libraries by similar context of use as determined by import statements present in source code. experimental results obtained from training such embeddings on three large open source software corpora reveals that library vectors capture semantically meaningful relationships among software libraries, such as the relationship between frameworks and their plug-ins and libraries commonly used together within ecosystems such as big data infrastructure projects (in java), front-end and back-end web development frameworks (in javascript) and data science toolkits (in python).",,2019-03-27,,"['bart theeten', 'frederik vandeputte', 'tom van cutsem']"
473,1904.04018,modelling air pollution crises using multi-agent simulation,stat.ap cs.ma,"this paper describes an agent based approach for simulating the control of an air pollution crisis. a gaussian plum air pollution dispersion model (gpd) is combined with an artificial neural network (ann) to predict the concentration levels of three different air pollutants. the two models (gpm and ann) are integrated with a mas (multi-agent system). the mas models pollutant sources controllers and air pollution monitoring agencies as software agents. the population of agents cooperates with each other in order to reduce their emissions and control the air pollution. leaks or natural sources of pollution are modelled as uncontrolled sources. a cooperation strategy is simulated and its impact on air pollution evolution is assessed and compared. the simulation scenario is built using data about annaba (a city in northeast algeria). the simulation helps to compare and assess the efficiency of policies to control air pollution during crises, and takes in to account uncontrolled sources.",,2019-04-08,,"['sabri ghazi', 'julie dugdale', 'tarek khadir']"
474,1904.04020,crad: clustering with robust autocuts and depth,stat.co stat.ml,"we develop a new density-based clustering algorithm named crad which is based on a new neighbor searching function with a robust data depth as the dissimilarity measure. our experiments prove that the new crad is highly competitive at detecting clusters with varying densities, compared with the existing algorithms such as dbscan, optics and dbca. furthermore, a new effective parameter selection procedure is developed to select the optimal underlying parameter in the real-world clustering, when the ground truth is unknown. lastly, we suggest a new clustering framework that extends crad from spatial data clustering to time series clustering without a-priori knowledge of the true number of clusters. the performance of crad is evaluated through extensive experimental studies.",10.1109/icdm.2017.116,2019-04-08,,"['xin huang', 'yulia r. gel']"
475,1904.04021,adaptation of hierarchical structured models for speech act recognition   in asynchronous conversation,cs.cl cs.lg stat.ml,"we address the problem of speech act recognition (sar) in asynchronous conversations (forums, emails). unlike synchronous conversations (e.g., meetings, phone), asynchronous domains lack large labeled datasets to train an effective sar model. in this paper, we propose methods to effectively leverage abundant unlabeled conversational data and the available labeled data from synchronous domains. we carry out our research in three main steps. first, we introduce a neural architecture based on hierarchical lstms and conditional random fields (crf) for sar, and show that our method outperforms existing methods when trained on in-domain data only. second, we improve our initial sar models by semi-supervised learning in the form of pretrained word embeddings learned from a large unlabeled conversational corpus. finally, we employ adversarial training to improve the results further by leveraging the labeled data from synchronous domains and by explicitly modeling the distributional shift in two domains.",,2019-04-01,,"['tasnim mohiuddin', 'thanh-tung nguyen', 'shafiq joty']"
476,1904.04055,evaluating kgr10 polish word embeddings in the recognition of temporal   expressions using bilstm-crf,cs.cl cs.lg stat.ml,"the article introduces a new set of polish word embeddings, built using kgr10 corpus, which contains more than 4 billion words. these embeddings are evaluated in the problem of recognition of temporal expressions (timexes) for the polish language. we described the process of kgr10 corpus creation and a new approach to the recognition problem using bidirectional long-short term memory (bilstm) network with additional crf layer, where specific embeddings are essential. we presented experiments and conclusions drawn from them.",,2019-04-03,,"['jan kocoń', 'michał gawor']"
477,1904.04057,task oriented channel state information quantization,cs.it cs.lg math.it stat.ml,"in this paper, we propose a new perspective for quantizing a signal and more specifically the channel state information (csi). the proposed point of view is fully relevant for a receiver which has to send a quantized version of the channel state to the transmitter. roughly, the key idea is that the receiver sends the right amount of information to the transmitter so that the latter be able to take its (resource allocation) decision. more formally, the decision task of the transmitter is to maximize an utility function u(x;g) with respect to x (e.g., a power allocation vector) given the knowledge of a quantized version of the function parameters g. we exhibit a special case of an energy-efficient power control (pc) problem for which the optimal task oriented csi quantizer (tocq) can be found analytically. for more general utility functions, we propose to use neural networks (nn) based learning. simulations show that the compression rate obtained by adapting the feedback information rate to the function to be optimized may be significantly increased.",10.1109/pimrc.2018.8580826,2019-04-02,,"['hang zou', 'chao zhang', 'samson lasaulce']"
478,1904.04061,transferring knowledge fragments for learning distance metric from a   heterogeneous domain,stat.ml cs.cv cs.lg,"the goal of transfer learning is to improve the performance of target learning task by leveraging information (or transferring knowledge) from other related tasks. in this paper, we examine the problem of transfer distance metric learning (dml), which usually aims to mitigate the label information deficiency issue in the target dml. most of the current transfer dml (tdml) methods are not applicable to the scenario where data are drawn from heterogeneous domains. some existing heterogeneous transfer learning (htl) approaches can learn target distance metric by usually transforming the samples of source and target domain into a common subspace. however, these approaches lack flexibility in real-world applications, and the learned transformations are often restricted to be linear. this motivates us to develop a general flexible heterogeneous tdml (htdml) framework. in particular, any (linear/nonlinear) dml algorithms can be employed to learn the source metric beforehand. then the pre-learned source metric is represented as a set of knowledge fragments to help target metric learning. we show how generalization error in the target domain could be reduced using the proposed transfer strategy, and develop novel algorithm to learn either linear or nonlinear target metric. extensive experiments on various applications demonstrate the effectiveness of the proposed method.",10.1109/tpami.2018.2824309,2019-04-08,,"['yong luo', 'yonggang wen', 'tongliang liu', 'dacheng tao']"
479,1904.04063,analyzing and interpreting neural networks for nlp: a report on the   first blackboxnlp workshop,cs.cl stat.ml,"the emnlp 2018 workshop blackboxnlp was dedicated to resources and techniques specifically developed for analyzing and understanding the inner-workings and representations acquired by neural models of language. approaches included: systematic manipulation of input to neural networks and investigating the impact on their performance, testing whether interpretable knowledge can be decoded from intermediate representations acquired by neural networks, proposing modifications to neural network architectures to make their knowledge state or generated output more explainable, and examining the performance of networks on simplified or formal languages. here we review a number of representative studies in each category.",,2019-04-05,,"['afra alishahi', 'grzegorz chrupała', 'tal linzen']"
480,1904.04081,heterogeneous multi-task metric learning across multiple domains,stat.ml cs.lg,"distance metric learning (dml) plays a crucial role in diverse machine learning algorithms and applications. when the labeled information in target domain is limited, transfer metric learning (tml) helps to learn the metric by leveraging the sufficient information from other related domains. multi-task metric learning (mtml), which can be regarded as a special case of tml, performs transfer across all related domains. current tml tools usually assume that the same feature representation is exploited for different domains. however, in real-world applications, data may be drawn from heterogeneous domains. heterogeneous transfer learning approaches can be adopted to remedy this drawback by deriving a metric from the learned transformation across different domains. but they are often limited in that only two domains can be handled. to appropriately handle multiple domains, we develop a novel heterogeneous multi-task metric learning (hmtml) framework. in hmtml, the metrics of all different domains are learned together. the transformations derived from the metrics are utilized to induce a common subspace, and the high-order covariance among the predictive structures of these domains is maximized in this subspace. there do exist a few heterogeneous transfer learning approaches that deal with multiple domains, but the high-order statistics (correlation information), which can only be exploited by simultaneously examining all domains, is ignored in these approaches. compared with them, the proposed hmtml can effectively explore such high-order information, thus obtaining more reliable feature transformations and metrics. effectiveness of our method is validated by the extensive and intensive experiments on text categorization, scene classification, and social image annotation.",10.1109/tnnls.2017.2750321,2019-04-08,,"['yong luo', 'yonggang wen', 'dacheng tao']"
481,1904.04083,convolutive blind source separation on surface emg signals for   respiratory diagnostics and medical ventilation control,eess.sp cs.sy stat.ap,"the electromyogram (emg) is an important tool for assessing the activity of a muscle and thus also a valuable measure for the diagnosis and control of respiratory support. in this article we propose convolutive blind source separation (bss) as an effective tool to pre-process surface electromyogram (semg) data of the human respiratory muscles. specifically, the problem of discriminating between inspiratory, expiratory and cardiac muscle activity is addressed, which currently poses a major obstacle for the clinical use of semg for adaptive ventilation control. it is shown that using the investigated broadband algorithm, a clear separation of these components can be achieved. the algorithm is based on a generic framework for bss that utilizes multiple statistical signal characteristics. apart from a four-channel fir structure, there are no further restrictive assumptions on the demixing system.",10.1109/embc.2016.7591513,2019-04-08,,"['herbert buchner', 'eike petersen', 'marcus eger', 'philipp rostalski']"
482,1904.04088,large margin multi-modal multi-task feature extraction for image   classification,stat.ml cs.cv cs.lg,"the features used in many image analysis-based applications are frequently of very high dimension. feature extraction offers several advantages in high-dimensional cases, and many recent studies have used multi-task feature extraction approaches, which often outperform single-task feature extraction approaches. however, most of these methods are limited in that they only consider data represented by a single type of feature, even though features usually represent images from multiple modalities. we therefore propose a novel large margin multi-modal multi-task feature extraction (lm3fe) framework for handling multi-modal features for image classification. in particular, lm3fe simultaneously learns the feature extraction matrix for each modality and the modality combination coefficients. in this way, lm3fe not only handles correlated and noisy features, but also utilizes the complementarity of different modalities to further help reduce feature redundancy in each modality. the large margin principle employed also helps to extract strongly predictive features so that they are more suitable for prediction (e.g., classification). an alternating algorithm is developed for problem optimization and each sub-problem can be efficiently solved. experiments on two challenging real-world image datasets demonstrate the effectiveness and superiority of the proposed method.",10.1109/tip.2015.2495116,2019-04-08,,"['yong luo', 'yonggang wen', 'dacheng tao', 'jie gui', 'chao xu']"
483,1904.04096,deep learning sentiment analysis of amazon.com reviews and ratings,cs.ir cs.cl cs.lg stat.ml,"our study employs sentiment analysis to evaluate the compatibility of amazon.com reviews with their corresponding ratings. sentiment analysis is the task of identifying and classifying the sentiment expressed in a piece of text as being positive or negative. on e-commerce websites such as amazon.com, consumers can submit their reviews along with a specific polarity rating. in some instances, there is a mismatch between the review and the rating. to identify the reviews with mismatched ratings we performed sentiment analysis using deep learning on amazon.com product review data. product reviews were converted to vectors using paragraph vector, which then was used to train a recurrent neural network with gated recurrent unit. our model incorporated both semantic relationship of review text and product information. we also developed a web service application that predicts the rating score for a submitted review using the trained model and if there is a mismatch between predicted rating score and submitted rating score, it provides feedback to the reviewer.",10.5121/ijscai.2019.8101,2019-04-04,,"['nishit shrestha', 'fatma nasoz']"
484,1904.04116,revisiting adversarial autoencoder for unsupervised word translation   with cycle consistency and improved training,cs.cl cs.lg stat.ml,"adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. however, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. in this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. extensive experimentations with european, non-european and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.",,2019-04-04,,"['tasnim mohiuddin', 'shafiq joty']"
485,1904.04137,"diabetes mellitus forecasting using population health data in ontario,   canada",stat.ap cs.lg,"leveraging health administrative data (had) datasets for predicting the risk of chronic diseases including diabetes has gained a lot of attention in the machine learning community recently. in this paper, we use the largest health records datasets of patients in ontario,canada. provided by the institute of clinical evaluative sciences (ices), this database is age, gender and ethnicity-diverse. the datasets include demographics, lab measurements,drug benefits, healthcare system interactions, ambulatory and hospitalizations records. we perform one of the first large-scale machine learning studies with this data to study the task of predicting diabetes in a range of 1-10 years ahead, which requires no additional screening of individuals.in the best setup, we reach a test auc of 80.3 with a single-model trained on an observation window of 5 years with a one-year buffer using all datasets. a subset of top 15 features alone (out of a total of 963) could provide a test auc of 79.1. in this paper, we provide extensive machine learning model performance and feature contribution analysis, which enables us to narrow down to the most important features useful for diabetes forecasting. examples include chronic conditions such as asthma and hypertension, lab results, diagnostic codes in insurance claims, age and geographical information.",,2019-04-08,,"['mathieu ravaut', 'hamed sadeghi', 'kin kwan leung', 'maksims volkovs', 'laura c. rosella']"
486,1904.04148,common statistical patterns in urban terrorism,stat.ap cs.si physics.soc-ph,"the underlying reasons behind modern terrorism are seemingly complex and intangible. despite diverse causal mechanisms, research has shown that there exists general statistical patterns at the global scale that can shed light on human confrontation behaviour. whilst many policing and counter-terrorism operations are conducted at a city level, there has been a lack of research in building city-level resolution prediction engines based on statistical patterns. for the first time, the paper shows that there exists general commonalities between global cities under terrorist attacks. by examining over 30,000 geo-tagged terrorism acts over 7000 cities worldwide from 2002 to today, the results shows the following. all cities experience attacks $a$ that are uncorrelated to the population and separated by a time interval $t$ that is negative exponentially distributed $\sim \exp(-a^{-1})$, with a death-toll per attack that follows a power law distribution. the prediction parameters yield a high confidence of explaining up to 87\% of the variations in frequency and 89\% in the death-toll data. these findings show that the aggregate statistical behaviour of terror attacks are seemingly random and memoryless for all global cities. the enabled the author to develop a data-driven city-specific prediction system and we quantify its information theoretic uncertainty and information loss. further analysis show that there appears to be an increase in the uncertainty over the predictability of attacks, challenging our ability to develop effective counter-terrorism capabilities.",,2019-04-08,,['weisi guo']
487,1904.04153,autosem: automatic task selection and mixing in multi-task learning,cs.cl cs.lg stat.ml,"multi-task learning (mtl) has achieved success over a wide range of problems, where the goal is to improve the performance of a primary task using a set of relevant auxiliary tasks. however, when the usefulness of the auxiliary tasks w.r.t. the primary task is not known a priori, the success of mtl models depends on the correct choice of these auxiliary tasks and also a balanced mixing ratio of these tasks during alternate training. these two problems could be resolved via manual intuition or hyper-parameter tuning over all combinatorial task choices, but this introduces inductive bias or is not scalable when the number of candidate auxiliary tasks is very large. to address these issues, we present autosem, a two-stage mtl pipeline, where the first stage automatically selects the most useful auxiliary tasks via a beta-bernoulli multi-armed bandit with thompson sampling, and the second stage learns the training mixing ratio of these selected auxiliary tasks via a gaussian process based bayesian optimization framework. we conduct several mtl experiments on the glue language understanding tasks, and show that our autosem framework can successfully find relevant auxiliary tasks and automatically learn their mixing ratio, achieving significant performance boosts on several primary tasks. finally, we present ablations for each stage of autosem and analyze the learned auxiliary task choices.",,2019-04-08,,"['han guo', 'ramakanth pasunuru', 'mohit bansal']"
488,1904.04154,bayesian neural networks at finite temperature,stat.ml cond-mat.dis-nn cs.lg,"we recapitulate the bayesian formulation of neural network based classifiers and show that, while sampling from the posterior does indeed lead to better generalisation than is obtained by standard optimisation of the cost function, even better performance can in general be achieved by sampling finite temperature ($t$) distributions derived from the posterior. taking the example of two different deep (3 hidden layers) classifiers for mnist data, we find quite different $t$ values to be appropriate in each case. in particular, for a typical neural network classifier a clear minimum of the test error is observed at $t>0$. this suggests an early stopping criterion for full batch simulated annealing: cool until the average validation error starts to increase, then revert to the parameters with the lowest validation error. as $t$ is increased classifiers transition from accurate classifiers to classifiers that have higher training error than assigning equal probability to each class. efficient studies of these temperature-induced effects are enabled using a replica-exchange hamiltonian monte carlo simulation technique. finally, we show how thermodynamic integration can be used to perform model selection for deep neural networks. similar to the laplace approximation, this approach assumes that the posterior is dominated by a single mode. crucially, however, no assumption is made about the shape of that mode and it is not required to precisely compute and invert the hessian.",,2019-04-08,,"['robert j. n. baldock', 'nicola marzari']"
489,1904.04161,audio source separation via multi-scale learning with dilated dense   u-nets,cs.lg cs.sd eess.as stat.ml,"modern audio source separation techniques rely on optimizing sequence model architectures such as, 1d-cnns, on mixture recordings to generalize well to unseen mixtures. specifically, recent focus is on time-domain based architectures such as wave-u-net which exploit temporal context by extracting multi-scale features. however, the optimality of the feature extraction process in these architectures has not been well investigated. in this paper, we examine and recommend critical architectural changes that forge an optimal multi-scale feature extraction process. to this end, we replace regular $1-$d convolutions with adaptive dilated convolutions that have innate capability of capturing increased context by using large temporal receptive fields. we also investigate the impact of dense connections on the extraction process that encourage feature reuse and better gradient flow. the dense connections between the downsampling and upsampling paths of a u-net architecture capture multi-resolution information leading to improved temporal modelling. we evaluate the proposed approaches on the musdb test dataset. in addition to providing an improved performance over the state-of-the-art, we also provide insights on the impact of different architectural choices on complex data-driven solutions for source separation.",,2019-04-08,,"['vivek sivaraman narayanaswamy', 'sameeksha katoch', 'jayaraman j. thiagarajan', 'huan song', 'andreas spanias']"
490,1904.04185,multiple imputation in data that grow over time: a comparison of three   strategies,stat.me,"multiple imputation is a highly recommended technique to deal with missing data, but the application to longitudinal datasets can be done in multiple ways. when a new wave of longitudinal data arrives, we can treat the combined data of multiple waves as a new missing data problem and overwrite existing imputations with new values (re-imputation). alternatively, we may keep the existing imputations, and impute only the new data. we may do either a full multiple imputation (nested) or a single imputation (appended) on the new data per imputed set. this study compares these three strategies by means of simulation. all techniques resulted in valid inference under a monotone missingness pattern. a non-monotone missingness pattern led to biased and non-confidence valid regression coefficients after nested and appended imputation, depending on the correlation structure of the data. correlations within timepoints must be stronger than correlations between timepoints to obtain valid inference. in an empirical example, the three strategies performed similarly.we conclude that appended imputation is especially beneficial in longitudinal datasets that suffer from dropout.",,2019-04-08,,"['x. m. kavelaars', 's. van buuren', 'j. r. van ginkel']"
491,1904.04203,characterizing the social interactions in the artificial bee colony   algorithm,cs.ne cs.si stat.ml,"computational swarm intelligence consists of multiple artificial simple agents exchanging information while exploring a search space. despite a rich literature in the field, with works improving old approaches and proposing new ones, the mechanism by which complex behavior emerges in these systems is still not well understood. this literature gap hinders the researchers' ability to deal with known problems in swarms intelligence such as premature convergence, and the balance of coordination and diversity among agents. recent advances in the literature, however, have proposed to study these systems via the network that emerges from the social interactions within the swarm (i.e., the interaction network). in our work, we propose a definition of the interaction network for the artificial bee colony (abc) algorithm. with our approach, we captured striking idiosyncrasies of the algorithm. we uncovered the different patterns of social interactions that emerge from each type of bee, revealing the importance of the bees variations throughout the iterations of the algorithm. we found that abc exhibits a dynamic information flow through the use of different bees but lacks continuous coordination between the agents.",,2019-04-08,,"['lydia taw', 'nishant gurrapadi', 'mariana macedo', 'marcos oliveira', 'diego pinheiro', 'carmelo bastos-filho', 'ronaldo menezes']"
